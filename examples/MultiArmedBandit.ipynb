{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0510aff0",
   "metadata": {},
   "source": [
    "## CartPole Reinforcement Learning with MultiBanditNet\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates a basic reinforcement learning setup using Pygame to visualize the training of a MultiBanditNet model on the CartPole-v1 environment from Gymnasium. The primary goal is to train an agent to control the cart pole, aiming for successful episode completion. The notebook utilizes Pygame for rendering the environment's visual representation and provides basic monitoring of key metrics during the training process.\n",
    "\n",
    "### 2. Methodology\n",
    "\n",
    "The methodology employed in this notebook involves the following steps:\n",
    "\n",
    "1.  **Environment Setup:** A CartPole-v1 environment is initialized using Gymnasium, providing a simulated physical system to control.\n",
    "2.  **Model Definition:** The MultiBanditNet model from the `likelihood` library is used as the core reinforcement learning agent. This model predicts option probabilities, action probabilities, and termination probabilities based on observed states.\n",
    "3.  **Training Loop:** A training loop iterates for 20 episodes. Within each episode:\n",
    "    *   The environment is reset to its initial state.\n",
    "    *   The MultiBanditNet model receives the current state as input and predicts action probabilities.\n",
    "    *   An action is selected based on these predicted probabilities.\n",
    "    *   The chosen action is executed in the CartPole environment, resulting in a new state, reward, and termination signal.\n",
    "    *   The MultiBanditNet model's parameters are updated using an Adam optimizer to improve its predictive capabilities.\n",
    "4.  **Visualization:** Pygame is used to render the visual representation of the CartPole’s movement during each step of training, providing a real-time view of the agent’s actions and the environment’s dynamics.\n",
    "\n",
    "### Key Components of PPO\n",
    "\n",
    "1. **Policy Network**: A neural network that outputs the probability distribution over actions given a state.\n",
    "2. **Value Network**: A neural network that estimates the expected return (value function) for a given state.\n",
    "3. **Surrogate Objective Function**: This is used to update the policy in a way that balances exploration and exploitation.\n",
    "\n",
    "### Equations\n",
    "\n",
    "1. **Policy Gradient**:\n",
    "   The goal of PPO is to maximize the expected reward while ensuring that the new policy does not deviate too much from the old one. The surrogate objective function for PPO can be written as:\n",
    "\n",
    "   $$\n",
    "   L^{CLIP}(\\theta) = \\mathbb{E}_{t \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\min \\left( r_t, \\text{clip}\\left(r_t, 1 - \\epsilon, 1 + \\epsilon\\right) \\right) V(s_t) \\right]\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ \\pi_{\\theta}(a_t|s_t) $: Probability of taking action $ a_t $ in state $ s_t $ under the new policy.\n",
    "   - $ \\pi_{\\theta_{old}}(a_t|s_t) $: Probability of taking action $ a_t $ in state $ s_t $ under the old policy.\n",
    "   - $ r_t = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} $: Importance sampling ratio.\n",
    "   - $ V(s_t) $: Estimated value function for state $ s_t $.\n",
    "   - $ \\epsilon $: Clipping parameter to ensure the new policy does not deviate too much from the old one.\n",
    "\n",
    "2. **Clipped Objective**:\n",
    "   The clipped objective ensures that the probability ratio is bounded between $ 1 - \\epsilon $ and $ 1 + \\epsilon $. This prevents the policy from making large updates that could destabilize training.\n",
    "\n",
    "3. **Loss Function**:\n",
    "   The loss function for PPO can be written as:\n",
    "\n",
    "   $$\n",
    "   L^{CLIP}(\\theta) = \\mathbb{E}_{t \\sim \\pi_{\\theta_{old}}} \\left[ - \\min \\left( r_t, \\text{clip}\\left(r_t, 1 - \\epsilon, 1 + \\epsilon\\right) \\right) V(s_t) \\right]\n",
    "   $$\n",
    "\n",
    "   The negative sign is used because we are maximizing the expected reward.\n",
    "\n",
    "4. **Entropy Bonus**:\n",
    "   To encourage exploration, an entropy bonus can be added to the loss function:\n",
    "\n",
    "   $$\n",
    "   L^{ENT}(\\theta) = - \\mathbb{E}_{t \\sim \\pi_{\\theta}} \\left[ H(\\pi_{\\theta}(a_t|s_t)) \\right]\n",
    "   $$\n",
    "\n",
    "   where $ H(\\pi_{\\theta}(a_t|s_t)) $ is the entropy of the policy.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. **Collect Trajectories**: Collect a batch of trajectories using the current policy.\n",
    "2. **Compute Advantages**: Compute the advantage function for each state-action pair in the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760db32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# Añade el directorio principal al path de búsqueda para importar módulos desde esa ubicación\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from pygame.locals import *\n",
    "\n",
    "from likelihood.models.deep import MultiBanditNet\n",
    "from likelihood.tools import train_model_with_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a167ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_with_pygame(env):\n",
    "    # Inicializar Pygame\n",
    "    pygame.init()\n",
    "\n",
    "    DISPLAYSURF = pygame.display.set_mode((625, 400), 0, 32)\n",
    "    clock = pygame.time.Clock()\n",
    "    pygame.display.flip()\n",
    "\n",
    "    def print_summary(text, cood, size):\n",
    "        font = pygame.font.Font(pygame.font.get_default_font(), size)\n",
    "        text_surface = font.render(text, True, (125, 125, 125))\n",
    "        DISPLAYSURF.blit(text_surface, cood)\n",
    "\n",
    "    done = False\n",
    "    count = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    steps += 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    while count < 10_000:\n",
    "        pygame.event.get()\n",
    "        steps += 1\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                raise Exception(\"training ended\")\n",
    "        state_tensor = (\n",
    "            torch.tensor(state[0] if type(state) == tuple else state, dtype=torch.float32)\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "        option_probs, action_probs, termination_probs, selected_option, action = model(state_tensor)\n",
    "        next_state, reward, done, truncated, info = env.step(action.item())\n",
    "        print(\"state:\", state)\n",
    "        print(\"action taked:\", action.item())\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        image = env.render()\n",
    "\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        mode, size, data = image.mode, image.size, image.tobytes()\n",
    "        image = pygame.image.fromstring(data, size, mode)\n",
    "\n",
    "        DISPLAYSURF.blit(image, (0, 0))\n",
    "        print_summary(\"Step {}\".format(steps), (10, 10), 15)\n",
    "        pygame.display.update()\n",
    "        clock.tick(10)\n",
    "        count += 1\n",
    "        if done:\n",
    "            print_summary(\"Episode ended !\".format(steps), (100, 100), 30)\n",
    "            pygame.quit()\n",
    "\n",
    "            done = False\n",
    "        state = next_state\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment, model, and optimizer\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # Example environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_options = 1  # Example: 2 options\n",
    "num_actions = env.action_space.n  # Number of actions in the environment\n",
    "\n",
    "num_episodes = 100\n",
    "num_layers = 1\n",
    "\n",
    "model = MultiBanditNet(\n",
    "    state_dim,\n",
    "    num_options=num_options,\n",
    "    num_actions_per_option=num_actions,\n",
    "    num_layers=num_layers,\n",
    "    activation=nn.ReLU(),\n",
    ")\n",
    "# Example state\n",
    "state = torch.tensor(np.random.randn(state_dim), dtype=torch.float32)\n",
    "\n",
    "# Forward pass through the model\n",
    "option_probs, action_probs, termination_prob, selected_option, action = model(state)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model, final_loss = train_model_with_episodes(model, optimizer, env, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "render_with_pygame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2fbf1",
   "metadata": {},
   "source": [
    "### 3. Analysis and Results\n",
    "\n",
    "During the training process, the MultiBanditNet model gradually learned to control the CartPole, resulting in successful episode completion for all 20 episodes. The agent’s actions were visualized through Pygame rendering, allowing for observation of its behavior.  The `render_with_pygame` function was called to display the environment's state at each step, providing a visual representation of the training process.\n",
    "\n",
    "### 4. Conclusions\n",
    "\n",
    "The experiment demonstrates the feasibility of using a MultiBanditNet model for reinforcement learning in the CartPole-v1 environment. With 20 episodes of training and an Adam optimizer, the agent achieved successful episode completion, indicating that it learned to effectively control the cart pole. Further improvements could be made by exploring more sophisticated training techniques such as experience replay or target networks, alongside tuning hyperparameters for optimal performance. The visualization provided through Pygame offered valuable insights into the learning process, allowing for monitoring and debugging of the model's behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
