{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# Añade el directorio principal al path de búsqueda para importar módulos desde esa ubicación\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import json\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from likelihood.models.deep import AutoClassifier\n",
    "from likelihood.tools import OneHotEncoder\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "is_updated = False\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(tf.__version__) > version.parse(\"2.15.0\"):\n",
    "    is_updated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de cáncer de mama desde sklearn\n",
    "df = datasets.load_breast_cancer()\n",
    "\n",
    "# Convertir los datos a un DataFrame de pandas para facilitar la manipulación\n",
    "df_cancer = pd.DataFrame(data=df.data, columns=df.feature_names)\n",
    "df_cancer[\"target\"] = df.target  # Añadir la columna de etiquetas 'target'\n",
    "\n",
    "# OneHotEncoder convierte las etiquetas a formato one-hot encoding\n",
    "y_encoder = OneHotEncoder()\n",
    "y = y_encoder.encode(df_cancer[\"target\"].to_list())  # Codificar las etiquetas de la clase (target)\n",
    "X = df_cancer.drop(\n",
    "    columns=\"target\"\n",
    ").to_numpy()  # Extraer las características (sin la columna 'target')\n",
    "X = np.asarray(X).astype(np.float32)  # Convertir X a tipo float32 para la entrada del modelo\n",
    "y = np.asarray(y).astype(np.float32)  # Convertir y a tipo float32\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de clasificación automática con las especificaciones dadas\n",
    "model = AutoClassifier(\n",
    "    input_shape_parm=X.shape[1],  # El número de características de entrada (columnas de X)\n",
    "    num_classes=y.shape[1],  # El número de clases (salidas) del modelo\n",
    "    units=17,  # Número de unidades en las capas ocultas\n",
    "    activation=\"selu\",  # Función de activación de las capas ocultas\n",
    "    l2_reg=0.0,\n",
    ")\n",
    "\n",
    "if is_updated:\n",
    "    model = model._main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tf.convert_to_tensor(tf.random.normal([1, X.shape[1]]))\n",
    "# Guardamos dummy_input para generar el sample-request.json más adelante\n",
    "sample_request = {\n",
    "    \"input_data\": {\n",
    "        \"columns\": list(range(X.shape[1])),\n",
    "        \"index\": list(range(2)),\n",
    "        \"data\": X_train[:2].tolist(),\n",
    "    }\n",
    "}\n",
    "with open(\"sample-request.json\", \"w\") as f:\n",
    "    json.dump(sample_request, f)\n",
    "model(dummy_input)\n",
    "\n",
    "# Conda environment\n",
    "custom_env = _mlflow_conda_env(\n",
    "    additional_conda_deps=None,\n",
    "    additional_pip_deps=[\"likelihood[full]\", \"azureml-inference-server-http\"],\n",
    "    additional_conda_channels=None,\n",
    ")\n",
    "\n",
    "# Sample\n",
    "input_sample = X_train[0].reshape(1, -1)\n",
    "output_sample = model.predict(input_sample)\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer=\"adam\",  # Optimizador Adam\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),  # Función de pérdida para clasificación multiclase\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(),  # Añadir la métrica AUC para un análisis más completo\n",
    "        tf.keras.metrics.CategoricalAccuracy(),  # Métrica de precisión categórica\n",
    "        tf.keras.metrics.F1Score(threshold=0.5),  # Métrica F1 con threshold\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Definir callback para registrar las métricas en cada época\n",
    "class LogMetricsCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Loggear las métricas como valores escalares\n",
    "        mlflow.log_metric(\"loss\", float(logs[\"loss\"]), step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", float(logs[\"val_loss\"]), step=epoch)\n",
    "        mlflow.log_metric(\"accuracy\", float(logs[\"categorical_accuracy\"]), step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", float(logs[\"val_categorical_accuracy\"]), step=epoch)\n",
    "        mlflow.log_metric(\"auc\", float(logs[\"auc\"]), step=epoch)\n",
    "        mlflow.log_metric(\"val_auc\", float(logs[\"val_auc\"]), step=epoch)\n",
    "        mlflow.log_metric(\"f1_score\", float(logs[\"f1_score\"][0]), step=epoch)\n",
    "        mlflow.log_metric(\"val_f1_score\", float(logs[\"val_f1_score\"][0]), step=epoch)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "if is_updated:\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"best_model.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"best_model\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        save_format=\"tf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # Entrenar el modelo con los datos, usando 15 épocas y 20% de los datos para validación\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,  # Para obtener información durante el entrenamiento\n",
    "        callbacks=[\n",
    "            LogMetricsCallback(),\n",
    "            early_stopping,\n",
    "            checkpoint,\n",
    "        ],  # Usar el callback para registrar las métricas\n",
    "    )\n",
    "    print(\"Modelo entrenado\")\n",
    "\n",
    "    # Inferir la firma del modelo\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Loggear el modelo con la firma personalizada\n",
    "    mlflow.tensorflow.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model_clf\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[:1],  # Tomar un ejemplo de entrada\n",
    "        conda_env=custom_env,  # Asegúrate de tener un entorno Conda adecuado\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
