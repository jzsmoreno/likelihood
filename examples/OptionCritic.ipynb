{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f082674",
   "metadata": {},
   "source": [
    "## CartPole-v1 Environment with MultiBanditNet Model and OptionCriticEnv\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "This Jupyter Notebook simulates interactions with the CartPole-v1 environment using a MultiBanditNet model to learn optimal actions for controlling the pole. The goal is to train a model that can effectively navigate the environment and achieve episode completion. The code generates simulated episodes and trains the model based on rewards received from the environment.\n",
    "\n",
    "### 2. Methodology\n",
    "\n",
    "The notebook follows these key steps:\n",
    "\n",
    "1.  **Environment Setup:** A Gymnasium environment (`CartPole-v1`) is created, defining a state dimension of 6 and an action space of 2.\n",
    "2.  **Model Initialization:** A `MultiBanditNet` model is instantiated with specified parameters (state dimension, number of options, number of actions).\n",
    "3.  **Episode Simulation:** The code simulates a series of episodes to train the model. Each episode consists of a fixed number of steps.\n",
    "4.  **Action Selection:** Within each step, an action is randomly selected from the available action space (0 or 1).\n",
    "5.  **Environment Interaction:** The selected action is applied to the environment, resulting in a new state, reward, and done flag.\n",
    "6.  **Model Training:** The `MultiBanditNet` model's parameters are updated based on the received reward using an Adam optimizer.\n",
    "7. **Visualization (Pygame):** Uses Pygame to render the environment and display the current state of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# Añade el directorio principal al path de búsqueda para importar módulos desde esa ubicación\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from pygame.locals import *\n",
    "\n",
    "from likelihood.models.deep import MultiBanditNet\n",
    "from likelihood.models.environments import OptionCriticEnv\n",
    "from likelihood.tools import train_model_with_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21634e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data is generated in a tabular format\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # Environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n  # Number of actions in the simulated environment\n",
    "\n",
    "episodes = {}\n",
    "num_simulated_steps = 20_000\n",
    "num_simulated_episodes = 100\n",
    "\n",
    "for i in range(num_simulated_episodes):\n",
    "    state = env.reset()\n",
    "    reward, done, truncated, info = 0.0, False, False, {}\n",
    "    episodes[i] = {\n",
    "        \"state\": [],\n",
    "        \"selected_option\": [],\n",
    "        \"action\": [],\n",
    "        \"next_state\": [],\n",
    "        \"reward\": [],\n",
    "        \"done\": [],\n",
    "    }\n",
    "    for j in range(num_simulated_steps):\n",
    "        action = np.random.randint(num_actions)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        if done:\n",
    "            reward = -1.0\n",
    "        state = state[0] if type(state) == tuple else state\n",
    "        episodes[i][\"state\"].append(state)\n",
    "        episodes[i][\"selected_option\"].append(0)\n",
    "        episodes[i][\"action\"].append(action)\n",
    "        episodes[i][\"next_state\"].append(next_state)\n",
    "        episodes[i][\"reward\"].append(reward)\n",
    "        episodes[i][\"done\"].append(done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_with_pygame(env):\n",
    "    # Inicializar Pygame\n",
    "    pygame.init()\n",
    "\n",
    "    DISPLAYSURF = pygame.display.set_mode((625, 400), 0, 32)\n",
    "    clock = pygame.time.Clock()\n",
    "    pygame.display.flip()\n",
    "\n",
    "    def print_summary(text, cood, size):\n",
    "        font = pygame.font.Font(pygame.font.get_default_font(), size)\n",
    "        text_surface = font.render(text, True, (125, 125, 125))\n",
    "        DISPLAYSURF.blit(text_surface, cood)\n",
    "\n",
    "    done = False\n",
    "    count = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    steps += 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    while count < 10_000:\n",
    "        pygame.event.get()\n",
    "        steps += 1\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                raise Exception(\"training ended\")\n",
    "        state_tensor = (\n",
    "            torch.tensor(state[0] if type(state) == tuple else state, dtype=torch.float32)\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "        option_probs, action_probs, termination_probs, selected_option, action = model(state_tensor)\n",
    "        next_state, reward, done, truncated, info = env.step(action.item())\n",
    "        print(\"state:\", state)\n",
    "        print(\"action taked:\", action.item())\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        image = env.render()\n",
    "\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        mode, size, data = image.mode, image.size, image.tobytes()\n",
    "        image = pygame.image.fromstring(data, size, mode)\n",
    "\n",
    "        DISPLAYSURF.blit(image, (0, 0))\n",
    "        print_summary(\"Step {}\".format(steps), (10, 10), 15)\n",
    "        pygame.display.update()\n",
    "        clock.tick(10)\n",
    "        count += 1\n",
    "        if done:\n",
    "            print_summary(\"Episode ended !\".format(steps), (100, 100), 30)\n",
    "            pygame.quit()\n",
    "\n",
    "            done = False\n",
    "        state = next_state\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OptionCriticEnv(episodes)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n  # Number of actions in the environment\n",
    "num_options = len(num_actions)\n",
    "\n",
    "print(\"state_dim :\", state_dim)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "model = MultiBanditNet(state_dim, num_options, num_actions)\n",
    "# Example state\n",
    "state = torch.tensor(np.random.randn(state_dim), dtype=torch.float32)\n",
    "\n",
    "# Forward pass through the model\n",
    "option_probs, action_probs, termination_prob, selected_option, action = model(state)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model, final_loss = train_model_with_episodes(model, optimizer, env, num_episodes, tolerance=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "render_with_pygame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce753752",
   "metadata": {},
   "source": [
    "### 3. Analysis and Results\n",
    "\n",
    "The notebook simulates 100 simulated episodes using Gymnasium's CartPole-v1 environment. The `MultiBanditNet` model is trained within each episode, iteratively adjusting its parameters based on rewards received from the environment. The simulation generates data for each step of each episode, including the state, selected action, next state, reward, and done flag.\n",
    "\n",
    "**Table 1: Metric - Reward**\n",
    "\n",
    "| Metric          | Threshold | Outcome                               | Notes                                                     |\n",
    "|-----------------|-----------|---------------------------------------|----------------------------------------------------------|\n",
    "| Reward          | -1.0      | Episode terminates with reward of -1.0 when `done` is True. | Reward set to -1.0 upon episode termination.           |\n",
    "\n",
    "The primary metric being tracked is the \"Reward\". Upon completion of an episode (indicated by `done` becoming `True`), the reward is consistently set to -1.0. This signifies that the episode has terminated, and a negative reward is assigned as a default outcome for reaching this state. The threshold of -1.0 indicates the minimum acceptable reward value for successful episode termination.\n",
    "\n",
    "### 4. Conclusions\n",
    "\n",
    "The simulation demonstrates the basic framework for training a reinforcement learning agent using a `MultiBanditNet` model within the CartPole-v1 environment. While the code utilizes random action selection, it successfully trains the model to complete simulated episodes. The consistent termination of episodes with a reward of -1.0 highlights the importance of defining appropriate reward signals and episode termination criteria for effective training. Further improvements could be achieved by implementing more sophisticated action selection strategies (e.g., Q-learning or policy gradients) and refining the reward function to better guide the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e7931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
