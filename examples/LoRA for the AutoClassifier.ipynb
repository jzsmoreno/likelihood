{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Rank Adaptation (LoRA)\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is a technique designed to efficiently adapt large machine learning models to specific tasks or domains without the need for full retraining. This method introduces lightweight, trainable components to the existing model, significantly reducing computational and memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755452250.801737   16728 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755452250.806043   16728 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755452250.817729   16728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755452250.817743   16728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755452250.817745   16728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755452250.817746   16728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# Añade el directorio principal al path de búsqueda para importar módulos desde esa ubicación\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Desactivar los warnings para evitar mensajes innecesarios durante la ejecución\n",
    "import warnings\n",
    "\n",
    "# Importación de bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from likelihood.models.deep import (\n",
    "    AutoClassifier,\n",
    ")  # Modelos de deep learning personalizados\n",
    "from likelihood.tools import OneHotEncoder, get_metrics, apply_lora  # Herramientas auxiliares\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "is_updated = False\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(tf.__version__) > version.parse(\"2.15.0\"):\n",
    "    is_updated = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Model Adaptation**: LoRA modifies pre-trained models by adding small, trainable matrices to each layer, allowing the model to adjust to new tasks with minimal additional training.\n",
    "\n",
    "- **Low-Rank Matrices**: The technique leverages low-rank matrices, which are smaller and require less memory and computational power, to efficiently adapt the model.\n",
    "\n",
    "## Benefits Over Full Fine-Tuning\n",
    "\n",
    "- **Reduced Training Costs**: LoRA requires fewer resources compared to full fine-tuning, making it accessible for teams with limited computational power.\n",
    "\n",
    "- **Maintained Performance**: Despite having fewer trainable parameters, models adapted with LoRA can achieve performance levels comparable to those fine-tuned fully.\n",
    "\n",
    "For a more in-depth understanding of LoRA, consider the following resources:\n",
    "\n",
    "- **Original Research Paper**: [\"LoRA: Low-Rank Adaptation of Large Language Models\"](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "- **GitHub Repository**: [Microsoft's LoRA Implementation](https://github.com/microsoft/LoRA)\n",
    "\n",
    "- **Hugging Face Documentation**: [LoRA in Transformers](https://huggingface.co/docs/diffusers/v0.21.0/en/training/lora)\n",
    "\n",
    "- **Comprehensive Survey**: [Awesome-LoRA-Low-Rank-Adaptation](https://github.com/lliai/Awesome-LoRA-Low-Rank-Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de cáncer de mama desde sklearn\n",
    "df = datasets.load_breast_cancer()\n",
    "\n",
    "# Convertir los datos a un DataFrame de pandas para facilitar la manipulación\n",
    "df_cancer = pd.DataFrame(data=df.data, columns=df.feature_names)\n",
    "df_cancer[\"target\"] = df.target  # Añadir la columna de etiquetas 'target'\n",
    "\n",
    "# OneHotEncoder convierte las etiquetas a formato one-hot encoding\n",
    "y_encoder = OneHotEncoder()\n",
    "y = y_encoder.encode(df_cancer[\"target\"].to_list())  # Codificar las etiquetas de la clase (target)\n",
    "X = df_cancer.drop(\n",
    "    columns=\"target\"\n",
    ").to_numpy()  # Extraer las características (sin la columna 'target')\n",
    "X = np.asarray(X).astype(np.float32)  # Convertir X a tipo float32 para la entrada del modelo\n",
    "y = np.asarray(y).astype(np.float32)  # Convertir y a tipo float32\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 38)\n",
      "Dense weights shape: 38x17\n",
      "LoRA weights shape: A(38, 3), B(3, 17)\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Accuracy: 92.62%\n",
      "Precision: 89.87%\n",
      "Recall: 99.44%\n",
      "F1-Score: 94.41\n",
      "Cohen's Kappa: 0.8362\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_shape = (X.shape[1],)\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "# Define the AutoClassifier model\n",
    "model = AutoClassifier(\n",
    "    input_shape_parm=input_shape[-1],\n",
    "    num_classes=num_classes,\n",
    "    units=17,\n",
    "    activation=\"selu\",\n",
    "    l2_reg=0.0,\n",
    "    num_layers=2,\n",
    "    lora_mode=True,\n",
    "    lora_rank=3,\n",
    ")\n",
    "\n",
    "if is_updated:\n",
    "    model = model._main_model\n",
    "\n",
    "# Compilación del modelo: optimizador, función de pérdida y métricas\n",
    "model.compile(\n",
    "    optimizer=\"adam\",  # Optimizador Adam\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),  # Función de pérdida para clasificación multiclase\n",
    "    metrics=[\n",
    "        tf.keras.metrics.F1Score(threshold=0.5)\n",
    "    ],  # Métrica F1 (threshold = 0.5 para predicciones)\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=False)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de entrenamiento\n",
    "pred = model.predict(X)\n",
    "\n",
    "# Convertir las predicciones a las etiquetas predichas (máxima probabilidad)\n",
    "pred_label = np.argmax(pred, axis=1)\n",
    "\n",
    "# Añadir las predicciones al DataFrame original para su análisis\n",
    "df = df_cancer.copy()\n",
    "y_labels = df.drop(columns=\"target\").columns.to_list()\n",
    "df_cancer[\"prediction\"] = pred_label  # Columna de las etiquetas predichas\n",
    "df_cancer[\"label_0\"] = pred[:, 0]  # Probabilidad de la clase 0\n",
    "df_cancer[\"label_1\"] = pred[:, 1]  # Probabilidad de la clase 1\n",
    "\n",
    "# Calcular y mostrar las métricas del modelo comparando las etiquetas reales y las predicciones\n",
    "get_metrics(df_cancer, \"target\", \"prediction\", verbose=True)\n",
    "\n",
    "# Guardar el modelo entrenado en el disco en formato TensorFlow\n",
    "if is_updated:\n",
    "    model.save(\"lora_model.keras\")\n",
    "else:\n",
    "    model.save(\"lora_model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [None, 38]\n",
      "Dense weights shape: 38x17\n",
      "LoRA weights shape: A(38, 3), B(3, 17)\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Accuracy: 92.62%\n",
      "Precision: 89.87%\n",
      "Recall: 99.44%\n",
      "F1-Score: 94.41\n",
      "Cohen's Kappa: 0.8362\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo guardado desde el disco\n",
    "if is_updated:\n",
    "    model = tf.keras.models.load_model(\"lora_model.keras\")\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"lora_model\")\n",
    "    model.classifier.summary()\n",
    "# Hacer predicciones nuevamente con el modelo cargado\n",
    "pred = model.predict(X)\n",
    "\n",
    "# Obtener las etiquetas predichas para las nuevas predicciones\n",
    "pred_label = np.argmax(pred, axis=1)\n",
    "\n",
    "# Añadir las nuevas predicciones al DataFrame original\n",
    "df[\"prediction\"] = pred_label\n",
    "\n",
    "# Calcular y mostrar las métricas nuevamente con el modelo cargado\n",
    "get_metrics(df, \"target\", \"prediction\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
