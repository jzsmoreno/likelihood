{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm in Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns how to make decisions by interacting with an environment. The objective of the agent is to maximize cumulative reward over time through trial and error.\n",
    "\n",
    "Q-Learning, a model-free, value-based method within RL, allows the agent to estimate the action-value function (Q-function) and ultimately learn the optimal policy by iteratively improving its action-value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Añade el directorio principal al path de búsqueda para importar módulos desde esa ubicación\n",
    "sys.path.insert(0, \"..\")\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from pygame.locals import *\n",
    "from tensorflow import keras\n",
    "\n",
    "from likelihood.models.deep import AutoClassifier\n",
    "\n",
    "is_updated = False\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(tf.__version__) > version.parse(\"2.15.0\"):\n",
    "    is_updated = True\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = fig_id + \".\" + fig_extension\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Foundations\n",
    "\n",
    "### 1. **Action-Value Function (Q-function)**\n",
    "\n",
    "The action-value function, denoted as $Q(s, a)$, evaluates the quality of a given action $a$ in a state $s$. It measures the expected cumulative reward an agent can obtain by taking action $a$ in state $s$ and thereafter following an optimal policy. The function $Q(s, a)$ is updated during the learning process to reflect the agent’s evolving understanding of its environment.\n",
    "\n",
    "### 2. **Optimal Policy**\n",
    "\n",
    "An optimal policy, $\\pi^*$, is a strategy where the agent always selects actions that maximize its expected cumulative reward. Formally, the optimal policy is defined as:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "$$\n",
    "\n",
    "This means that for each state $s$, the agent will choose the action $a$ that maximizes the Q-value, thereby ensuring the highest long-term reward.\n",
    "\n",
    "### 3. **Bellman Optimality Equation**\n",
    "\n",
    "The Bellman optimality equation defines the relationship between the Q-value of a state-action pair and the rewards that can be obtained from it. It is central to Q-Learning:\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\max_{a'} Q^*(s', a')\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R(s, a) $ is the immediate reward for taking action $a$ in state $s$.\n",
    "- $ \\gamma $ is the discount factor ($ 0 < \\gamma \\leq 1$), which determines the weight of future rewards compared to immediate rewards. A higher $\\gamma$ values future rewards more heavily.\n",
    "\n",
    "### 4. **Q-Learning Algorithm**\n",
    "\n",
    "Q-Learning iteratively updates the action-value function based on the agent's experiences. The update rule is as follows:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha $ is the learning rate ($ 0 < \\alpha  \\leq 1$), controlling how much new information overrides the previous Q-value.\n",
    "- $ R(s, a) $ is the immediate reward obtained by taking action $a$ in state $s$.\n",
    "- $ \\gamma $ is the discount factor, which balances immediate and future rewards.\n",
    "- $ \\max_{a'} Q(s', a') $ represents the maximum Q-value over all possible actions from the next state $s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s define our gym environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# Declaring a few constants\n",
    "num_episodes = 50\n",
    "num_steps = 100\n",
    "gamma = 0.7  # discount_factor\n",
    "batch_size = 32\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "input_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "model = AutoClassifier(\n",
    "    input_shape_parm=input_shape,\n",
    "    num_classes=num_actions,\n",
    "    units=16,\n",
    "    activation=\"elu\",\n",
    "    classifier_activation=\"linear\",\n",
    ")\n",
    "\n",
    "if is_updated:\n",
    "    model = model._main_model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(name=\"mean_squared_error\")\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() > epsilon:\n",
    "        if isinstance(state, tuple):\n",
    "            value_function = model.predict(state[0].reshape(1, -1), verbose=0)[0]\n",
    "        elif isinstance(state, np.ndarray):\n",
    "            value_function = model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        action = np.argmax(value_function)\n",
    "    else:\n",
    "        action = np.random.choice(num_actions)\n",
    "    return action\n",
    "\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info, _ = env.step(action)\n",
    "    done = 1 if done else 0\n",
    "    state = state[0].reshape(1, -1) if isinstance(state, tuple) else state.reshape(1, -1)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "\n",
    "def training_step(batch_size):\n",
    "    batch_ = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch_)\n",
    "    states = np.array(states).reshape(batch_size, -1)\n",
    "    actions = np.array(actions).reshape(\n",
    "        batch_size,\n",
    "    )\n",
    "    rewards = np.array(rewards).reshape(\n",
    "        batch_size,\n",
    "    )\n",
    "    next_states = np.array(next_states).reshape(batch_size, -1)\n",
    "    dones = np.array(dones).reshape(\n",
    "        batch_size,\n",
    "    )\n",
    "    next_Q_values = model(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = rewards + (1 - dones) * gamma * max_next_Q_values\n",
    "\n",
    "    # mask = tf.one_hot(actions, num_actions)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.gather(all_Q_values, actions, axis=1)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    sum_rewards = 0\n",
    "    epsilon = max(1 - episode / int(num_episodes * 0.8), 0.01)  # Epsilon decay\n",
    "    for step in range(num_steps):\n",
    "        next_state, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        sum_rewards += reward\n",
    "\n",
    "        # If the buffer has enough samples, begin training\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = training_step(batch_size)\n",
    "\n",
    "        obs = next_state  # Update current state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Keep track of the best model based on performance\n",
    "    if loss < best_loss:\n",
    "        best_weights = model.get_weights()\n",
    "        best_loss = loss\n",
    "        sum_rewards += 1\n",
    "\n",
    "    rewards.append(sum_rewards)\n",
    "\n",
    "    # Logging every 10% of episodes\n",
    "    if episode % (num_episodes // 10) == 0:\n",
    "        print(\n",
    "            f\"Episode: {episode}, Steps: {step+1}, Epsilon: {epsilon:.3f}, Loss: {loss:.6f}, Reward: {sum_rewards}\"\n",
    "        )\n",
    "\n",
    "\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "if is_updated:\n",
    "    model.save(\"keras_model.keras\")\n",
    "else:\n",
    "    model.save(\"keras_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "save_fig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pygame essentials\n",
    "pygame.init()\n",
    "DISPLAYSURF = pygame.display.set_mode((625, 400), 0, 32)\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.flip()\n",
    "\n",
    "# openai gym env\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "input_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "count = 0\n",
    "steps = 0\n",
    "\n",
    "# loading trained model\n",
    "if is_updated:\n",
    "    model = tf.keras.models.load_model(\"keras_model.keras\")\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"keras_model\")\n",
    "\n",
    "\n",
    "def print_summary(text, cood, size):\n",
    "    font = pygame.font.Font(pygame.font.get_default_font(), size)\n",
    "    text_surface = font.render(text, True, (125, 125, 125))\n",
    "    DISPLAYSURF.blit(text_surface, cood)\n",
    "\n",
    "\n",
    "while count < 10000:\n",
    "    pygame.event.get()\n",
    "    steps += 1\n",
    "\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pygame.quit()\n",
    "            raise Exception(\"training ended\")\n",
    "    # Get the action probabilities from the policy network\n",
    "    action = epsilon_greedy_policy(state, epsilon=epsilon)\n",
    "    print(\"state:\", state)\n",
    "    print(\"action taked:\", action)\n",
    "    clear_output(wait=True)\n",
    "    next_state, reward, done, info, _ = env.step(action)  # take a step in the environment\n",
    "    image = env.render()  # render the environment to the screen\n",
    "\n",
    "    # convert image to pygame surface object\n",
    "    image = Image.fromarray(image, \"RGB\")\n",
    "    mode, size, data = image.mode, image.size, image.tobytes()\n",
    "    image = pygame.image.fromstring(data, size, mode)\n",
    "\n",
    "    DISPLAYSURF.blit(image, (0, 0))\n",
    "    print_summary(\"Step {}\".format(steps), (10, 10), 15)\n",
    "    pygame.display.update()\n",
    "    clock.tick(100)\n",
    "    count += 1\n",
    "    if done:\n",
    "        print_summary(\"Episode ended !\".format(steps), (100, 100), 30)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "    state = next_state\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
