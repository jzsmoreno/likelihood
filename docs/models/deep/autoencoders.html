<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>likelihood.models.deep.autoencoders API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.models.deep.autoencoders</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.models.deep.autoencoders.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>hp, input_shape_parm: None | int, num_classes: None | int, **kwargs) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(
    hp, input_shape_parm: None | int, num_classes: None | int, **kwargs
) -&gt; AutoClassifier:
    &#34;&#34;&#34;Builds a neural network model using Keras Tuner&#39;s search algorithm.

    Parameters
    ----------
    hp : `keras_tuner.HyperParameters`
        The hyperparameters to tune.
    input_shape_parm : `None` | `int`
        The shape of the input data.
    num_classes : `int`
        The number of classes in the dataset.

    Keyword Arguments
    -----------------
    Additional keyword arguments to pass to the model.

    hyperparameters : `dict`
        The hyperparameters to set.

    Returns
    -------
    `keras.Model`
        The neural network model.
    &#34;&#34;&#34;
    hyperparameters = kwargs.get(&#34;hyperparameters&#34;, None)
    hyperparameters_keys = hyperparameters.keys() if hyperparameters is not None else []

    units = (
        hp.Int(
            &#34;units&#34;,
            min_value=int(input_shape_parm * 0.2),
            max_value=int(input_shape_parm * 1.5),
            step=2,
        )
        if &#34;units&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;units&#34;, hyperparameters[&#34;units&#34;])
            if isinstance(hyperparameters[&#34;units&#34;], list)
            else hyperparameters[&#34;units&#34;]
        )
    )
    activation = (
        hp.Choice(&#34;activation&#34;, [&#34;sigmoid&#34;, &#34;relu&#34;, &#34;tanh&#34;, &#34;selu&#34;, &#34;softplus&#34;, &#34;softsign&#34;])
        if &#34;activation&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;activation&#34;, hyperparameters[&#34;activation&#34;])
            if isinstance(hyperparameters[&#34;activation&#34;], list)
            else hyperparameters[&#34;activation&#34;]
        )
    )
    optimizer = (
        hp.Choice(&#34;optimizer&#34;, [&#34;sgd&#34;, &#34;adam&#34;, &#34;adadelta&#34;, &#34;rmsprop&#34;, &#34;adamax&#34;, &#34;adagrad&#34;])
        if &#34;optimizer&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;optimizer&#34;, hyperparameters[&#34;optimizer&#34;])
            if isinstance(hyperparameters[&#34;optimizer&#34;], list)
            else hyperparameters[&#34;optimizer&#34;]
        )
    )
    threshold = (
        hp.Float(&#34;threshold&#34;, min_value=0.1, max_value=0.9, sampling=&#34;log&#34;)
        if &#34;threshold&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;threshold&#34;, hyperparameters[&#34;threshold&#34;])
            if isinstance(hyperparameters[&#34;threshold&#34;], list)
            else hyperparameters[&#34;threshold&#34;]
        )
    )
    num_layers = (
        hp.Int(&#34;num_layers&#34;, min_value=1, max_value=10, step=1)
        if &#34;num_layers&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;num_layers&#34;, hyperparameters[&#34;num_layers&#34;])
            if isinstance(hyperparameters[&#34;num_layers&#34;], list)
            else hyperparameters[&#34;num_layers&#34;]
        )
    )
    dropout = (
        hp.Float(&#34;dropout&#34;, min_value=0.1, max_value=0.9, sampling=&#34;log&#34;)
        if &#34;dropout&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;dropout&#34;, hyperparameters[&#34;dropout&#34;])
            if isinstance(hyperparameters[&#34;dropout&#34;], list)
            else hyperparameters[&#34;dropout&#34;]
        )
    )
    l2_reg = (
        hp.Float(&#34;l2_reg&#34;, min_value=1e-6, max_value=0.1, sampling=&#34;log&#34;)
        if &#34;l2_reg&#34; not in hyperparameters_keys
        else (
            hp.Choice(&#34;l2_reg&#34;, hyperparameters[&#34;l2_reg&#34;])
            if isinstance(hyperparameters[&#34;l2_reg&#34;], list)
            else hyperparameters[&#34;l2_reg&#34;]
        )
    )
    vae_mode = (
        hp.Choice(&#34;vae_mode&#34;, [True, False])
        if &#34;vae_mode&#34; not in hyperparameters_keys
        else hyperparameters[&#34;vae_mode&#34;]
    )

    try:
        vae_units = (
            hp.Int(&#34;vae_units&#34;, min_value=2, max_value=10, step=1)
            if (&#34;vae_units&#34; not in hyperparameters_keys) and vae_mode
            else (
                hp.Choice(&#34;vae_units&#34;, hyperparameters[&#34;vae_units&#34;])
                if isinstance(hyperparameters[&#34;vae_units&#34;], list)
                else hyperparameters[&#34;vae_units&#34;]
            )
        )
    except KeyError:
        vae_units = None

    model = call_existing_code(
        units=units,
        activation=activation,
        threshold=threshold,
        optimizer=optimizer,
        input_shape_parm=input_shape_parm,
        num_classes=num_classes,
        num_layers=num_layers,
        dropout=dropout,
        l2_reg=l2_reg,
        vae_mode=vae_mode,
        vae_units=vae_units,
    )
    return model</code></pre>
</details>
<div class="desc"><p>Builds a neural network model using Keras Tuner's search algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hp</code></strong> :&ensp;<code>keras_tuner.HyperParameters</code></dt>
<dd>The hyperparameters to tune.</dd>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>None` | `int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>hyperparameters : <code>dict</code>
The hyperparameters to set.</p>
<h2 id="returns">Returns</h2>
<p><code>keras.Model</code>
The neural network model.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.cal_loss_step"><code class="name flex">
<span>def <span class="ident">cal_loss_step</span></span>(<span>batch, encoder, decoder, vae_mode=False, training=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cal_loss_step(batch, encoder, decoder, vae_mode=False, training=True):
    &#34;&#34;&#34;
    Calculates the loss value on a batch of data.

    Parameters
    ----------
    batch : `tf.Tensor`
        The batch of data.
    encoder : `tf.keras.Model`
        The encoder model.
    decoder : `tf.keras.Model`
        The decoder model.
    optimizer : `tf.keras.optimizers.Optimizer`
        The optimizer to use.
    vae_mode : `bool`
        Whether to use variational autoencoder mode. Default is False.
    training : `bool`
        Whether the model is in training mode. Default is True.

    Returns
    -------
    `tf.Tensor`
        The loss value.
    &#34;&#34;&#34;
    if vae_mode:
        mean, log_var = encoder(batch, training=training)
        log_var = tf.clip_by_value(log_var, clip_value_min=1e-8, clip_value_max=tf.float32.max)
        decoded = decoder(sampling(mean, log_var), training=training)
        loss = vae_loss(batch, decoded, mean, log_var)
    else:
        encoded = encoder(batch, training=training)
        decoded = decoder(encoded, training=training)
        loss = mse_loss(batch, decoded)

    return loss</code></pre>
</details>
<div class="desc"><p>Calculates the loss value on a batch of data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The batch of data.</dd>
<dt><strong><code>encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>The encoder model.</dd>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>The decoder model.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>tf.keras.optimizers.Optimizer</code></dt>
<dd>The optimizer to use.</dd>
<dt><strong><code>vae_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use variational autoencoder mode. Default is False.</dd>
<dt><strong><code>training</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the model is in training mode. Default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code>
The loss value.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.call_existing_code"><code class="name flex">
<span>def <span class="ident">call_existing_code</span></span>(<span>units: int,<br>activation: str,<br>threshold: float,<br>optimizer: str,<br>input_shape_parm: None | int = None,<br>num_classes: None | int = None,<br>num_layers: int = 1,<br>**kwargs) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call_existing_code(
    units: int,
    activation: str,
    threshold: float,
    optimizer: str,
    input_shape_parm: None | int = None,
    num_classes: None | int = None,
    num_layers: int = 1,
    **kwargs,
) -&gt; AutoClassifier:
    &#34;&#34;&#34;
    Calls an existing AutoClassifier instance.

    Parameters
    ----------
    units : `int`
        The number of neurons in each hidden layer.
    activation : `str`
        The type of activation function to use for the neural network layers.
    threshold : `float`
        The threshold for the classifier.
    optimizer : `str`
        The type of optimizer to use for the neural network layers.
    input_shape_parm : `None` | `int`
        The shape of the input data.
    num_classes : `int`
        The number of classes in the dataset.
    num_layers : `int`
        The number of hidden layers in the classifier. Default is 1.

    Keyword Arguments
    -----------------
    vae_mode : `bool`
        Whether to use variational autoencoder mode. Default is False.
    vae_units : `int`
        The number of units in the variational autoencoder. Default is 2.

    Returns
    -------
    `AutoClassifier`
        The AutoClassifier instance.
    &#34;&#34;&#34;
    dropout = kwargs.get(&#34;dropout&#34;, None)
    l2_reg = kwargs.get(&#34;l2_reg&#34;, 0.0)
    vae_mode = kwargs.get(&#34;vae_mode&#34;, False)
    vae_units = kwargs.get(&#34;vae_units&#34;, 2)
    model = AutoClassifier(
        input_shape_parm=input_shape_parm,
        num_classes=num_classes,
        units=units,
        activation=activation,
        num_layers=num_layers,
        dropout=dropout,
        l2_reg=l2_reg,
        vae_mode=vae_mode,
        vae_units=vae_units,
    )
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.CategoricalCrossentropy(),
        metrics=[tf.keras.metrics.F1Score(threshold=threshold)],
    )
    return model</code></pre>
</details>
<div class="desc"><p>Calls an existing AutoClassifier instance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>units</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of neurons in each hidden layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of activation function to use for the neural network layers.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>The threshold for the classifier.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of optimizer to use for the neural network layers.</dd>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>None` | `int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
<dt><strong><code>num_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of hidden layers in the classifier. Default is 1.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>vae_mode : <code>bool</code>
Whether to use variational autoencoder mode. Default is False.
vae_units : <code>int</code>
The number of units in the variational autoencoder. Default is 2.</p>
<h2 id="returns">Returns</h2>
<p><code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code>
The AutoClassifier instance.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.check_for_nans"><code class="name flex">
<span>def <span class="ident">check_for_nans</span></span>(<span>tensors, name='Tensor')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_for_nans(tensors, name=&#34;Tensor&#34;):
    for t in tensors:
        if tf.reduce_any(tf.math.is_nan(t)) or tf.reduce_any(tf.math.is_inf(t)):
            print(f&#34;Warning: {name} contains NaNs or Infs&#34;)
            return True
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.kl_loss"><code class="name flex">
<span>def <span class="ident">kl_loss</span></span>(<span>mean, log_var)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl_loss(mean, log_var):
    &#34;&#34;&#34;
    Kullback-Leibler divergence loss function.

    Parameters
    ----------
    mean : `tf.Tensor`
        The mean of the distribution.
    log_var : `tf.Tensor`
        The log variance of the distribution.

    Returns
    -------
    `tf.Tensor`
    &#34;&#34;&#34;
    return -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))</code></pre>
</details>
<div class="desc"><p>Kullback-Leibler divergence loss function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The mean of the distribution.</dd>
<dt><strong><code>log_var</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The log variance of the distribution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code></p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.mse_loss"><code class="name flex">
<span>def <span class="ident">mse_loss</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mse_loss(y_true, y_pred):
    &#34;&#34;&#34;
    Mean squared error loss function.

    Parameters
    ----------
    y_true : `tf.Tensor`
        The true values.
    y_pred : `tf.Tensor`
        The predicted values.

    Returns
    -------
    `tf.Tensor`
    &#34;&#34;&#34;
    return tf.reduce_mean(tf.square(y_true - y_pred))</code></pre>
</details>
<div class="desc"><p>Mean squared error loss function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_true</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The true values.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The predicted values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code></p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.sampling"><code class="name flex">
<span>def <span class="ident">sampling</span></span>(<span>mean, log_var, epsilon_value=1e-08)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sampling(mean, log_var, epsilon_value=1e-8):
    &#34;&#34;&#34;
    Samples from the distribution.

    Parameters
    ----------
    mean : `tf.Tensor`
        The mean of the distribution.
    log_var : `tf.Tensor`
        The log variance of the distribution.
    epsilon_value : float
        A small value to avoid numerical instability.

    Returns
    -------
    `tf.Tensor`
    &#34;&#34;&#34;
    epsilon = tf.random.normal(shape=tf.shape(mean), mean=0.0, stddev=1.0)
    stddev = tf.exp(0.5 * log_var) + epsilon_value
    epsilon = tf.random.normal(shape=tf.shape(mean), mean=0.0, stddev=1.0)
    return mean + stddev * epsilon</code></pre>
</details>
<div class="desc"><p>Samples from the distribution.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The mean of the distribution.</dd>
<dt><strong><code>log_var</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The log variance of the distribution.</dd>
<dt><strong><code>epsilon_value</code></strong> :&ensp;<code>float</code></dt>
<dd>A small value to avoid numerical instability.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code></p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.setup_model"><code class="name flex">
<span>def <span class="ident">setup_model</span></span>(<span>data: pandas.core.frame.DataFrame,<br>target: str,<br>epochs: int,<br>train_size: float = 0.7,<br>seed=None,<br>train_mode: bool = True,<br>filepath: str = './my_dir/best_model',<br>method: str = 'Hyperband',<br>**kwargs) ‑> <a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@suppress_warnings
def setup_model(
    data: pd.DataFrame,
    target: str,
    epochs: int,
    train_size: float = 0.7,
    seed=None,
    train_mode: bool = True,
    filepath: str = &#34;./my_dir/best_model&#34;,
    method: str = &#34;Hyperband&#34;,
    **kwargs,
) -&gt; AutoClassifier:
    &#34;&#34;&#34;Setup model for training and tuning.

    Parameters
    ----------
    data : `pd.DataFrame`
        The dataset to train the model on.
    target : `str`
        The name of the target column.
    epochs : `int`
        The number of epochs to train the model for.
    train_size : `float`
        The proportion of the dataset to use for training.
    seed : `Any` | `int`
        The random seed to use for reproducibility.
    train_mode : `bool`
        Whether to train the model or not.
    filepath : `str`
        The path to save the best model to.
    method : `str`
        The method to use for hyperparameter tuning. Options are &#34;Hyperband&#34; and &#34;RandomSearch&#34;.

    Keyword Arguments
    -----------------
    Additional keyword arguments to pass to the model.

    max_trials : `int`
        The maximum number of trials to perform.
    directory : `str`
        The directory to save the model to.
    project_name : `str`
        The name of the project.
    objective : `str`
        The objective to optimize.
    verbose : `bool`
        Whether to print verbose output.
    hyperparameters : `dict`
        The hyperparameters to set.

    Returns
    -------
    model : `AutoClassifier`
        The trained model.
    &#34;&#34;&#34;
    max_trials = kwargs.get(&#34;max_trials&#34;, 10)
    directory = kwargs.get(&#34;directory&#34;, &#34;./my_dir&#34;)
    project_name = kwargs.get(&#34;project_name&#34;, &#34;get_best&#34;)
    objective = kwargs.get(&#34;objective&#34;, &#34;val_loss&#34;)
    verbose = kwargs.get(&#34;verbose&#34;, True)
    hyperparameters = kwargs.get(&#34;hyperparameters&#34;, None)

    X = data.drop(columns=target)
    input_sample = X.sample(1)
    y = data[target]
    assert (
        X.select_dtypes(include=[&#34;object&#34;]).empty == True
    ), &#34;Categorical variables within the DataFrame must be encoded, this is done by using the DataFrameEncoder from likelihood.&#34;
    validation_split = 1.0 - train_size

    if train_mode:
        try:
            if (not os.path.exists(directory)) and directory != &#34;./&#34;:
                os.makedirs(directory)
            elif directory != &#34;./&#34;:
                print(f&#34;Directory {directory} already exists, it will be deleted.&#34;)
                rmtree(directory)
                os.makedirs(directory)
        except:
            print(&#34;Warning: unable to create directory&#34;)

        y_encoder = OneHotEncoder()
        y = y_encoder.encode(y.to_list())
        X = X.to_numpy()
        input_sample.to_numpy()
        X = np.asarray(X).astype(np.float32)
        input_sample = np.asarray(input_sample).astype(np.float32)
        y = np.asarray(y).astype(np.float32)

        input_shape_parm = X.shape[1]
        num_classes = y.shape[1]
        global build_model
        build_model = partial(
            build_model,
            input_shape_parm=input_shape_parm,
            num_classes=num_classes,
            hyperparameters=hyperparameters,
        )

        if method == &#34;Hyperband&#34;:
            tuner = keras_tuner.Hyperband(
                hypermodel=build_model,
                objective=objective,
                max_epochs=epochs,
                factor=3,
                directory=directory,
                project_name=project_name,
                seed=seed,
            )
        elif method == &#34;RandomSearch&#34;:
            tuner = keras_tuner.RandomSearch(
                hypermodel=build_model,
                objective=objective,
                max_trials=max_trials,
                directory=directory,
                project_name=project_name,
                seed=seed,
            )

        tuner.search(X, y, epochs=epochs, validation_split=validation_split, verbose=verbose)
        models = tuner.get_best_models(num_models=2)
        best_model = models[0]
        best_model(input_sample)

        best_model.save(filepath, save_format=&#34;tf&#34;)

        if verbose:
            tuner.results_summary()
    else:
        best_model = tf.keras.models.load_model(filepath)
    best_hps = tuner.get_best_hyperparameters(1)[0].values
    vae_mode = best_hps.get(&#34;vae_mode&#34;, hyperparameters.get(&#34;vae_mode&#34;, False))
    best_hps[&#34;vae_units&#34;] = None if not vae_mode else best_hps[&#34;vae_units&#34;]

    return best_model, pd.DataFrame(best_hps, index=[&#34;Value&#34;]).dropna(axis=1)</code></pre>
</details>
<div class="desc"><p>Setup model for training and tuning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dataset to train the model on.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the target column.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to train the model for.</dd>
<dt><strong><code>train_size</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of the dataset to use for training.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>Any` | `int</code></dt>
<dd>The random seed to use for reproducibility.</dd>
<dt><strong><code>train_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to train the model or not.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to save the best model to.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>The method to use for hyperparameter tuning. Options are "Hyperband" and "RandomSearch".</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>max_trials : <code>int</code>
The maximum number of trials to perform.
directory : <code>str</code>
The directory to save the model to.
project_name : <code>str</code>
The name of the project.
objective : <code>str</code>
The objective to optimize.
verbose : <code>bool</code>
Whether to print verbose output.
hyperparameters : <code>dict</code>
The hyperparameters to set.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code></dt>
<dd>The trained model.</dd>
</dl></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>batch, encoder, decoder, optimizer, vae_mode=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.function
def train_step(batch, encoder, decoder, optimizer, vae_mode=False):
    &#34;&#34;&#34;
    Trains the model on a batch of data.

    Parameters
    ----------
    mean : `tf.Tensor`
        The mean of the distribution.
    log_var : `tf.Tensor`
        The log variance of the distribution.
    batch : `tf.Tensor`
        The batch of data.
    encoder : `tf.keras.Model`
        The encoder model.
    decoder : `tf.keras.Model`
        The decoder model.
    optimizer : `tf.keras.optimizers.Optimizer`
        The optimizer to use.
    vae_mode : `bool`
        Whether to use variational autoencoder mode. Default is False.

    Returns
    -------
    `tf.Tensor`
        The loss value.
    &#34;&#34;&#34;
    optimizer.build(encoder.trainable_variables + decoder.trainable_variables)

    with tf.GradientTape() as encoder_tape, tf.GradientTape() as decoder_tape:
        loss = cal_loss_step(batch, encoder, decoder, vae_mode=vae_mode)

    gradients_of_encoder = encoder_tape.gradient(loss, encoder.trainable_variables)
    gradients_of_decoder = decoder_tape.gradient(loss, decoder.trainable_variables)

    optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))
    optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))

    return loss</code></pre>
</details>
<div class="desc"><p>Trains the model on a batch of data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The mean of the distribution.</dd>
<dt><strong><code>log_var</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The log variance of the distribution.</dd>
<dt><strong><code>batch</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The batch of data.</dd>
<dt><strong><code>encoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>The encoder model.</dd>
<dt><strong><code>decoder</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>The decoder model.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>tf.keras.optimizers.Optimizer</code></dt>
<dd>The optimizer to use.</dd>
<dt><strong><code>vae_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use variational autoencoder mode. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code>
The loss value.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.vae_loss"><code class="name flex">
<span>def <span class="ident">vae_loss</span></span>(<span>y_true, y_pred, mean, log_var)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vae_loss(y_true, y_pred, mean, log_var):
    &#34;&#34;&#34;
    Variational autoencoder loss function.

    Parameters
    ----------
    y_true : `tf.Tensor`
        The true values.
    y_pred : `tf.Tensor`
        The predicted values.
    mean : `tf.Tensor`
        The mean of the distribution.
    log_var : `tf.Tensor`
        The log variance of the distribution.

    Returns
    -------
    `tf.Tensor`
    &#34;&#34;&#34;
    return mse_loss(y_true, y_pred) + kl_loss(mean, log_var)</code></pre>
</details>
<div class="desc"><p>Variational autoencoder loss function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_true</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The true values.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The predicted values.</dd>
<dt><strong><code>mean</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The mean of the distribution.</dd>
<dt><strong><code>log_var</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The log variance of the distribution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tf.Tensor</code></p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier"><code class="flex name class">
<span>class <span class="ident">AutoClassifier</span></span>
<span>(</span><span>input_shape_parm, num_classes, units, activation, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.keras.utils.register_keras_serializable(package=&#34;Custom&#34;, name=&#34;AutoClassifier&#34;)
class AutoClassifier(tf.keras.Model):
    &#34;&#34;&#34;
    An auto-classifier model that automatically determines the best classification strategy based on the input data.

    Parameters
    ----------
    input_shape_parm : `int`
        The shape of the input data.
    num_classes : `int`
        The number of classes in the dataset.
    units : `int`
        The number of neurons in each hidden layer.
    activation : `str`
        The type of activation function to use for the neural network layers.

    Keyword Arguments
    -----------------
    Additional keyword arguments to pass to the model.

    classifier_activation : `str`
        The activation function to use for the classifier layer. Default is `softmax`. If the activation function is not a classification function, the model can be used in regression problems.
    num_layers : `int`
        The number of hidden layers in the classifier. Default is 1.
    dropout : `float`
        The dropout rate to use in the classifier. Default is None.
    l2_reg : `float`
        The L2 regularization parameter. Default is 0.0.
    vae_mode : `bool`
        Whether to use variational autoencoder mode. Default is False.
    vae_units : `int`
        The number of units in the variational autoencoder. Default is 2.
    lora_mode : `bool`
        Whether to use LoRA layers. Default is False.
    lora_rank : `int`
        The rank of the LoRA layer. Default is 4.
    &#34;&#34;&#34;

    def __init__(self, input_shape_parm, num_classes, units, activation, **kwargs):
        super(AutoClassifier, self).__init__()
        self.input_shape_parm = input_shape_parm
        self.num_classes = num_classes
        self.units = units
        self.activation = activation

        self.encoder = None
        self.decoder = None
        self.classifier = None
        self.classifier_activation = kwargs.get(&#34;classifier_activation&#34;, &#34;softmax&#34;)
        self.num_layers = kwargs.get(&#34;num_layers&#34;, 1)
        self.dropout = kwargs.get(&#34;dropout&#34;, None)
        self.l2_reg = kwargs.get(&#34;l2_reg&#34;, 0.0)
        self.vae_mode = kwargs.get(&#34;vae_mode&#34;, False)
        self.vae_units = kwargs.get(&#34;vae_units&#34;, 2)
        self.lora_mode = kwargs.get(&#34;lora_mode&#34;, False)
        self.lora_rank = kwargs.get(&#34;lora_rank&#34;, 4)

    def build_encoder_decoder(self, input_shape):
        self.encoder = (
            tf.keras.Sequential(
                [
                    tf.keras.layers.Dense(
                        units=self.units,
                        activation=self.activation,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                    tf.keras.layers.Dense(
                        units=int(self.units / 2),
                        activation=self.activation,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                ],
                name=&#34;encoder&#34;,
            )
            if not self.encoder
            else self.encoder
        )

        self.decoder = (
            tf.keras.Sequential(
                [
                    tf.keras.layers.Dense(
                        units=self.units,
                        activation=self.activation,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                    tf.keras.layers.Dense(
                        units=self.input_shape_parm,
                        activation=self.activation,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                ],
                name=&#34;decoder&#34;,
            )
            if not self.decoder
            else self.decoder
        )

    def build(self, input_shape):
        if self.vae_mode:
            inputs = tf.keras.Input(shape=self.input_shape_parm, name=&#34;encoder_input&#34;)
            x = tf.keras.layers.Dense(
                units=self.units,
                kernel_regularizer=l2(self.l2_reg),
                kernel_initializer=&#34;he_normal&#34;,
            )(inputs)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation(self.activation)(x)
            x = tf.keras.layers.Dense(
                units=int(self.units / 2),
                kernel_regularizer=l2(self.l2_reg),
                kernel_initializer=&#34;he_normal&#34;,
                name=&#34;encoder_hidden&#34;,
            )(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation(self.activation)(x)

            mean = tf.keras.layers.Dense(2, name=&#34;mean&#34;)(x)
            log_var = tf.keras.layers.Dense(2, name=&#34;log_var&#34;)(x)
            log_var = tf.keras.layers.Lambda(lambda x: x + 1e-7)(log_var)

            self.encoder = (
                tf.keras.Model(inputs, [mean, log_var], name=&#34;vae_encoder&#34;)
                if not self.encoder
                else self.encoder
            )
            self.decoder = (
                tf.keras.Sequential(
                    [
                        tf.keras.layers.Dense(
                            units=self.units,
                            kernel_regularizer=l2(self.l2_reg),
                        ),
                        tf.keras.layers.BatchNormalization(),
                        tf.keras.layers.Activation(self.activation),
                        tf.keras.layers.Dense(
                            units=self.input_shape_parm,
                            kernel_regularizer=l2(self.l2_reg),
                        ),
                        tf.keras.layers.BatchNormalization(),
                        tf.keras.layers.Activation(self.activation),
                    ],
                    name=&#34;vae_decoder&#34;,
                )
                if not self.decoder
                else self.decoder
            )

        else:
            self.build_encoder_decoder(input_shape)

        self.classifier = tf.keras.Sequential()
        if self.num_layers &gt; 1 and not self.lora_mode:
            for _ in range(self.num_layers - 1):
                self.classifier.add(
                    tf.keras.layers.Dense(
                        units=self.units,
                        activation=self.activation,
                        kernel_regularizer=l2(self.l2_reg),
                    )
                )
                if self.dropout:
                    self.classifier.add(tf.keras.layers.Dropout(self.dropout))

        elif self.lora_mode:
            for _ in range(self.num_layers - 1):
                self.classifier.add(
                    LoRALayer(units=self.units, rank=self.lora_rank, name=f&#34;LoRA_{_}&#34;)
                )
                self.classifier.add(tf.keras.layers.Activation(self.activation))
                if self.dropout:
                    self.classifier.add(tf.keras.layers.Dropout(self.dropout))

        self.classifier.add(
            tf.keras.layers.Dense(
                units=self.num_classes,
                activation=self.classifier_activation,
                kernel_regularizer=l2(self.l2_reg),
            )
        )

    def train_encoder_decoder(
        self, data, epochs, batch_size, validation_split=0.2, patience=10, **kwargs
    ):
        &#34;&#34;&#34;
        Trains the encoder and decoder on the input data.

        Parameters
        ----------
        data : `tf.data.Dataset`, `np.ndarray`
            The input data.
        epochs : `int`
            The number of epochs to train for.
        batch_size : `int`
            The batch size to use.
        validation_split : `float`
            The proportion of the dataset to use for validation. Default is 0.2.
        patience : `int`
            The number of epochs to wait before early stopping. Default is 10.

        Keyword Arguments
        -----------------
        Additional keyword arguments to pass to the model.
        &#34;&#34;&#34;
        verbose = kwargs.get(&#34;verbose&#34;, True)
        optimizer = kwargs.get(&#34;optimizer&#34;, tf.keras.optimizers.Adam())
        dummy_input = tf.convert_to_tensor(tf.random.normal([1, self.input_shape_parm]))
        self.build(dummy_input.shape)
        if not self.vae_mode:
            dummy_output = self.encoder(dummy_input)
            self.decoder(dummy_output)
        else:
            mean, log_var = self.encoder(dummy_input)
            dummy_output = sampling(mean, log_var)
            self.decoder(dummy_output)

        if isinstance(data, np.ndarray):
            data = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)
            data = data.map(lambda x: tf.cast(x, tf.float32))

        early_stopping = EarlyStopping(patience=patience)
        train_batches = data.take(int((1 - validation_split) * len(data)))
        val_batches = data.skip(int((1 - validation_split) * len(data)))
        for epoch in range(epochs):
            for train_batch, val_batch in zip(train_batches, val_batches):
                loss_train = train_step(
                    train_batch, self.encoder, self.decoder, optimizer, self.vae_mode
                )
                loss_val = cal_loss_step(
                    val_batch, self.encoder, self.decoder, self.vae_mode, False
                )

            early_stopping(loss_train)

            if early_stopping.stop_training:
                print(f&#34;Early stopping triggered at epoch {epoch}.&#34;)
                break

            if epoch % 10 == 0 and verbose:
                print(
                    f&#34;Epoch {epoch}: Train Loss: {loss_train:.6f} Validation Loss: {loss_val:.6f}&#34;
                )
        self.freeze_encoder_decoder()

    def call(self, x):
        if self.vae_mode:
            mean, log_var = self.encoder(x)
            encoded = sampling(mean, log_var)
        else:
            encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        combined = tf.concat([decoded, encoded], axis=1)
        classification = self.classifier(combined)
        return classification

    def freeze_encoder_decoder(self):
        &#34;&#34;&#34;
        Freezes the encoder and decoder layers to prevent them from being updated during training.
        &#34;&#34;&#34;
        for layer in self.encoder.layers:
            layer.trainable = False
        for layer in self.decoder.layers:
            layer.trainable = False

    def unfreeze_encoder_decoder(self):
        &#34;&#34;&#34;
        Unfreezes the encoder and decoder layers allowing them to be updated during training.
        &#34;&#34;&#34;
        for layer in self.encoder.layers:
            layer.trainable = True
        for layer in self.decoder.layers:
            layer.trainable = True

    def set_encoder_decoder(self, source_model):
        &#34;&#34;&#34;
        Sets the encoder and decoder layers from another AutoClassifier instance,
        ensuring compatibility in dimensions. Only works if vae_mode is False.

        Parameters
        -----------
        source_model : AutoClassifier
            The source model to copy the encoder and decoder layers from.

        Raises
        -------
        ValueError
            If the input shape or units of the source model do not match.
        &#34;&#34;&#34;
        if not isinstance(source_model, AutoClassifier):
            raise ValueError(&#34;Source model must be an instance of AutoClassifier.&#34;)

        if self.input_shape_parm != source_model.input_shape_parm:
            raise ValueError(
                f&#34;Incompatible input shape. Expected {self.input_shape_parm}, got {source_model.input_shape_parm}.&#34;
            )
        if self.units != source_model.units:
            raise ValueError(
                f&#34;Incompatible number of units. Expected {self.units}, got {source_model.units}.&#34;
            )
        self.encoder, self.decoder = tf.keras.Sequential(), tf.keras.Sequential()
        for i, layer in enumerate(source_model.encoder.layers):
            if isinstance(layer, tf.keras.layers.Dense):
                dummy_input = tf.convert_to_tensor(tf.random.normal([1, layer.input_shape[1]]))
                dense_layer = tf.keras.layers.Dense(
                    units=layer.units,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                )
                dense_layer.build(dummy_input.shape)
                self.encoder.add(dense_layer)
                self.encoder.layers[i].set_weights(layer.get_weights())
            elif not isinstance(layer, InputLayer):
                raise ValueError(f&#34;Layer type {type(layer)} not supported for copying.&#34;)

        for i, layer in enumerate(source_model.decoder.layers):
            if isinstance(layer, tf.keras.layers.Dense):
                dummy_input = tf.convert_to_tensor(tf.random.normal([1, layer.input_shape[1]]))
                dense_layer = tf.keras.layers.Dense(
                    units=layer.units,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                )
                dense_layer.build(dummy_input.shape)
                self.decoder.add(dense_layer)
                self.decoder.layers[i].set_weights(layer.get_weights())
            elif not isinstance(layer, InputLayer):
                raise ValueError(f&#34;Layer type {type(layer)} not supported for copying.&#34;)

    def get_config(self):
        config = {
            &#34;input_shape_parm&#34;: self.input_shape_parm,
            &#34;num_classes&#34;: self.num_classes,
            &#34;units&#34;: self.units,
            &#34;activation&#34;: self.activation,
            &#34;classifier_activation&#34;: self.classifier_activation,
            &#34;num_layers&#34;: self.num_layers,
            &#34;dropout&#34;: self.dropout,
            &#34;l2_reg&#34;: self.l2_reg,
            &#34;vae_mode&#34;: self.vae_mode,
            &#34;vae_units&#34;: self.vae_units,
            &#34;lora_mode&#34;: self.lora_mode,
            &#34;lora_rank&#34;: self.lora_rank,
        }
        base_config = super(AutoClassifier, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(
            input_shape_parm=config[&#34;input_shape_parm&#34;],
            num_classes=config[&#34;num_classes&#34;],
            units=config[&#34;units&#34;],
            activation=config[&#34;activation&#34;],
            classifier_activation=config[&#34;classifier_activation&#34;],
            num_layers=config[&#34;num_layers&#34;],
            dropout=config[&#34;dropout&#34;],
            l2_reg=config[&#34;l2_reg&#34;],
            vae_mode=config[&#34;vae_mode&#34;],
            vae_units=config[&#34;vae_units&#34;],
            lora_mode=config[&#34;lora_mode&#34;],
            lora_rank=config[&#34;lora_rank&#34;],
        )</code></pre>
</details>
<div class="desc"><p>An auto-classifier model that automatically determines the best classification strategy based on the input data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape_parm</code></strong> :&ensp;<code>int</code></dt>
<dd>The shape of the input data.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of classes in the dataset.</dd>
<dt><strong><code>units</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of neurons in each hidden layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of activation function to use for the neural network layers.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments to pass to the model.</p>
<p>classifier_activation : <code>str</code>
The activation function to use for the classifier layer. Default is <code>softmax</code>. If the activation function is not a classification function, the model can be used in regression problems.
num_layers : <code>int</code>
The number of hidden layers in the classifier. Default is 1.
dropout : <code>float</code>
The dropout rate to use in the classifier. Default is None.
l2_reg : <code>float</code>
The L2 regularization parameter. Default is 0.0.
vae_mode : <code>bool</code>
Whether to use variational autoencoder mode. Default is False.
vae_units : <code>int</code>
The number of units in the variational autoencoder. Default is 2.
lora_mode : <code>bool</code>
Whether to use LoRA layers. Default is False.
lora_rank : <code>int</code>
The rank of the LoRA layer. Default is 4.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.models.model.Model</li>
<li>keras.src.backend.tensorflow.trainer.TensorFlowTrainer</li>
<li>keras.src.trainers.trainer.Trainer</li>
<li>keras.src.layers.layer.Layer</li>
<li>keras.src.backend.tensorflow.layer.TFLayer</li>
<li>keras.src.backend.tensorflow.trackable.KerasAutoTrackable</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.ops.operation.Operation</li>
<li>keras.src.saving.keras_saveable.KerasSaveable</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an operation from its config.</p>
<p>This method is the reverse of <code>get_config</code>, capable of instantiating the
same operation from the config dictionary.</p>
<p>Note: If you override this method, you might receive a serialized dtype
config, which is a <code>dict</code>. You can deserialize it as follows:</p>
<pre><code class="language-python">if &quot;dtype&quot; in config and isinstance(config[&quot;dtype&quot;], dict):
    policy = dtype_policies.deserialize(config[&quot;dtype&quot;])
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the output of <code>get_config</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An operation instance.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    if self.vae_mode:
        inputs = tf.keras.Input(shape=self.input_shape_parm, name=&#34;encoder_input&#34;)
        x = tf.keras.layers.Dense(
            units=self.units,
            kernel_regularizer=l2(self.l2_reg),
            kernel_initializer=&#34;he_normal&#34;,
        )(inputs)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation(self.activation)(x)
        x = tf.keras.layers.Dense(
            units=int(self.units / 2),
            kernel_regularizer=l2(self.l2_reg),
            kernel_initializer=&#34;he_normal&#34;,
            name=&#34;encoder_hidden&#34;,
        )(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation(self.activation)(x)

        mean = tf.keras.layers.Dense(2, name=&#34;mean&#34;)(x)
        log_var = tf.keras.layers.Dense(2, name=&#34;log_var&#34;)(x)
        log_var = tf.keras.layers.Lambda(lambda x: x + 1e-7)(log_var)

        self.encoder = (
            tf.keras.Model(inputs, [mean, log_var], name=&#34;vae_encoder&#34;)
            if not self.encoder
            else self.encoder
        )
        self.decoder = (
            tf.keras.Sequential(
                [
                    tf.keras.layers.Dense(
                        units=self.units,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Activation(self.activation),
                    tf.keras.layers.Dense(
                        units=self.input_shape_parm,
                        kernel_regularizer=l2(self.l2_reg),
                    ),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Activation(self.activation),
                ],
                name=&#34;vae_decoder&#34;,
            )
            if not self.decoder
            else self.decoder
        )

    else:
        self.build_encoder_decoder(input_shape)

    self.classifier = tf.keras.Sequential()
    if self.num_layers &gt; 1 and not self.lora_mode:
        for _ in range(self.num_layers - 1):
            self.classifier.add(
                tf.keras.layers.Dense(
                    units=self.units,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                )
            )
            if self.dropout:
                self.classifier.add(tf.keras.layers.Dropout(self.dropout))

    elif self.lora_mode:
        for _ in range(self.num_layers - 1):
            self.classifier.add(
                LoRALayer(units=self.units, rank=self.lora_rank, name=f&#34;LoRA_{_}&#34;)
            )
            self.classifier.add(tf.keras.layers.Activation(self.activation))
            if self.dropout:
                self.classifier.add(tf.keras.layers.Dropout(self.dropout))

    self.classifier.add(
        tf.keras.layers.Dense(
            units=self.num_classes,
            activation=self.classifier_activation,
            kernel_regularizer=l2(self.l2_reg),
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.build_encoder_decoder"><code class="name flex">
<span>def <span class="ident">build_encoder_decoder</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_encoder_decoder(self, input_shape):
    self.encoder = (
        tf.keras.Sequential(
            [
                tf.keras.layers.Dense(
                    units=self.units,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                ),
                tf.keras.layers.Dense(
                    units=int(self.units / 2),
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                ),
            ],
            name=&#34;encoder&#34;,
        )
        if not self.encoder
        else self.encoder
    )

    self.decoder = (
        tf.keras.Sequential(
            [
                tf.keras.layers.Dense(
                    units=self.units,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                ),
                tf.keras.layers.Dense(
                    units=self.input_shape_parm,
                    activation=self.activation,
                    kernel_regularizer=l2(self.l2_reg),
                ),
            ],
            name=&#34;decoder&#34;,
        )
        if not self.decoder
        else self.decoder
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x):
    if self.vae_mode:
        mean, log_var = self.encoder(x)
        encoded = sampling(mean, log_var)
    else:
        encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    combined = tf.concat([decoded, encoded], axis=1)
    classification = self.classifier(combined)
    return classification</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.freeze_encoder_decoder"><code class="name flex">
<span>def <span class="ident">freeze_encoder_decoder</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def freeze_encoder_decoder(self):
    &#34;&#34;&#34;
    Freezes the encoder and decoder layers to prevent them from being updated during training.
    &#34;&#34;&#34;
    for layer in self.encoder.layers:
        layer.trainable = False
    for layer in self.decoder.layers:
        layer.trainable = False</code></pre>
</details>
<div class="desc"><p>Freezes the encoder and decoder layers to prevent them from being updated during training.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#34;input_shape_parm&#34;: self.input_shape_parm,
        &#34;num_classes&#34;: self.num_classes,
        &#34;units&#34;: self.units,
        &#34;activation&#34;: self.activation,
        &#34;classifier_activation&#34;: self.classifier_activation,
        &#34;num_layers&#34;: self.num_layers,
        &#34;dropout&#34;: self.dropout,
        &#34;l2_reg&#34;: self.l2_reg,
        &#34;vae_mode&#34;: self.vae_mode,
        &#34;vae_units&#34;: self.vae_units,
        &#34;lora_mode&#34;: self.lora_mode,
        &#34;lora_rank&#34;: self.lora_rank,
    }
    base_config = super(AutoClassifier, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<div class="desc"><p>Returns the config of the object.</p>
<p>An object config is a Python dictionary (serializable)
containing the information needed to re-instantiate it.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.set_encoder_decoder"><code class="name flex">
<span>def <span class="ident">set_encoder_decoder</span></span>(<span>self, source_model)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_encoder_decoder(self, source_model):
    &#34;&#34;&#34;
    Sets the encoder and decoder layers from another AutoClassifier instance,
    ensuring compatibility in dimensions. Only works if vae_mode is False.

    Parameters
    -----------
    source_model : AutoClassifier
        The source model to copy the encoder and decoder layers from.

    Raises
    -------
    ValueError
        If the input shape or units of the source model do not match.
    &#34;&#34;&#34;
    if not isinstance(source_model, AutoClassifier):
        raise ValueError(&#34;Source model must be an instance of AutoClassifier.&#34;)

    if self.input_shape_parm != source_model.input_shape_parm:
        raise ValueError(
            f&#34;Incompatible input shape. Expected {self.input_shape_parm}, got {source_model.input_shape_parm}.&#34;
        )
    if self.units != source_model.units:
        raise ValueError(
            f&#34;Incompatible number of units. Expected {self.units}, got {source_model.units}.&#34;
        )
    self.encoder, self.decoder = tf.keras.Sequential(), tf.keras.Sequential()
    for i, layer in enumerate(source_model.encoder.layers):
        if isinstance(layer, tf.keras.layers.Dense):
            dummy_input = tf.convert_to_tensor(tf.random.normal([1, layer.input_shape[1]]))
            dense_layer = tf.keras.layers.Dense(
                units=layer.units,
                activation=self.activation,
                kernel_regularizer=l2(self.l2_reg),
            )
            dense_layer.build(dummy_input.shape)
            self.encoder.add(dense_layer)
            self.encoder.layers[i].set_weights(layer.get_weights())
        elif not isinstance(layer, InputLayer):
            raise ValueError(f&#34;Layer type {type(layer)} not supported for copying.&#34;)

    for i, layer in enumerate(source_model.decoder.layers):
        if isinstance(layer, tf.keras.layers.Dense):
            dummy_input = tf.convert_to_tensor(tf.random.normal([1, layer.input_shape[1]]))
            dense_layer = tf.keras.layers.Dense(
                units=layer.units,
                activation=self.activation,
                kernel_regularizer=l2(self.l2_reg),
            )
            dense_layer.build(dummy_input.shape)
            self.decoder.add(dense_layer)
            self.decoder.layers[i].set_weights(layer.get_weights())
        elif not isinstance(layer, InputLayer):
            raise ValueError(f&#34;Layer type {type(layer)} not supported for copying.&#34;)</code></pre>
</details>
<div class="desc"><p>Sets the encoder and decoder layers from another AutoClassifier instance,
ensuring compatibility in dimensions. Only works if vae_mode is False.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>source_model</code></strong> :&ensp;<code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code></dt>
<dd>The source model to copy the encoder and decoder layers from.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the input shape or units of the source model do not match.</dd>
</dl></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.train_encoder_decoder"><code class="name flex">
<span>def <span class="ident">train_encoder_decoder</span></span>(<span>self, data, epochs, batch_size, validation_split=0.2, patience=10, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_encoder_decoder(
    self, data, epochs, batch_size, validation_split=0.2, patience=10, **kwargs
):
    &#34;&#34;&#34;
    Trains the encoder and decoder on the input data.

    Parameters
    ----------
    data : `tf.data.Dataset`, `np.ndarray`
        The input data.
    epochs : `int`
        The number of epochs to train for.
    batch_size : `int`
        The batch size to use.
    validation_split : `float`
        The proportion of the dataset to use for validation. Default is 0.2.
    patience : `int`
        The number of epochs to wait before early stopping. Default is 10.

    Keyword Arguments
    -----------------
    Additional keyword arguments to pass to the model.
    &#34;&#34;&#34;
    verbose = kwargs.get(&#34;verbose&#34;, True)
    optimizer = kwargs.get(&#34;optimizer&#34;, tf.keras.optimizers.Adam())
    dummy_input = tf.convert_to_tensor(tf.random.normal([1, self.input_shape_parm]))
    self.build(dummy_input.shape)
    if not self.vae_mode:
        dummy_output = self.encoder(dummy_input)
        self.decoder(dummy_output)
    else:
        mean, log_var = self.encoder(dummy_input)
        dummy_output = sampling(mean, log_var)
        self.decoder(dummy_output)

    if isinstance(data, np.ndarray):
        data = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)
        data = data.map(lambda x: tf.cast(x, tf.float32))

    early_stopping = EarlyStopping(patience=patience)
    train_batches = data.take(int((1 - validation_split) * len(data)))
    val_batches = data.skip(int((1 - validation_split) * len(data)))
    for epoch in range(epochs):
        for train_batch, val_batch in zip(train_batches, val_batches):
            loss_train = train_step(
                train_batch, self.encoder, self.decoder, optimizer, self.vae_mode
            )
            loss_val = cal_loss_step(
                val_batch, self.encoder, self.decoder, self.vae_mode, False
            )

        early_stopping(loss_train)

        if early_stopping.stop_training:
            print(f&#34;Early stopping triggered at epoch {epoch}.&#34;)
            break

        if epoch % 10 == 0 and verbose:
            print(
                f&#34;Epoch {epoch}: Train Loss: {loss_train:.6f} Validation Loss: {loss_val:.6f}&#34;
            )
    self.freeze_encoder_decoder()</code></pre>
</details>
<div class="desc"><p>Trains the encoder and decoder on the input data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>tf.data.Dataset<code>,</code>np.ndarray</code></dt>
<dd>The input data.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to train for.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The batch size to use.</dd>
<dt><strong><code>validation_split</code></strong> :&ensp;<code>float</code></dt>
<dd>The proportion of the dataset to use for validation. Default is 0.2.</dd>
<dt><strong><code>patience</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to wait before early stopping. Default is 10.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments to pass to the model.</p></div>
</dd>
<dt id="likelihood.models.deep.autoencoders.AutoClassifier.unfreeze_encoder_decoder"><code class="name flex">
<span>def <span class="ident">unfreeze_encoder_decoder</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unfreeze_encoder_decoder(self):
    &#34;&#34;&#34;
    Unfreezes the encoder and decoder layers allowing them to be updated during training.
    &#34;&#34;&#34;
    for layer in self.encoder.layers:
        layer.trainable = True
    for layer in self.decoder.layers:
        layer.trainable = True</code></pre>
</details>
<div class="desc"><p>Unfreezes the encoder and decoder layers allowing them to be updated during training.</p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.models.deep.autoencoders.EarlyStopping"><code class="flex name class">
<span>class <span class="ident">EarlyStopping</span></span>
<span>(</span><span>patience=10, min_delta=0.001)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = np.inf
        self.counter = 0
        self.stop_training = False

    def __call__(self, current_loss):
        if self.best_loss - current_loss &gt; self.min_delta:
            self.best_loss = current_loss
            self.counter = 0
        else:
            self.counter += 1

        if self.counter &gt;= self.patience:
            self.stop_training = True</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.models.deep" href="index.html">likelihood.models.deep</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="likelihood.models.deep.autoencoders.build_model" href="#likelihood.models.deep.autoencoders.build_model">build_model</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.cal_loss_step" href="#likelihood.models.deep.autoencoders.cal_loss_step">cal_loss_step</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.call_existing_code" href="#likelihood.models.deep.autoencoders.call_existing_code">call_existing_code</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.check_for_nans" href="#likelihood.models.deep.autoencoders.check_for_nans">check_for_nans</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.kl_loss" href="#likelihood.models.deep.autoencoders.kl_loss">kl_loss</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.mse_loss" href="#likelihood.models.deep.autoencoders.mse_loss">mse_loss</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.sampling" href="#likelihood.models.deep.autoencoders.sampling">sampling</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.setup_model" href="#likelihood.models.deep.autoencoders.setup_model">setup_model</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.train_step" href="#likelihood.models.deep.autoencoders.train_step">train_step</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.vae_loss" href="#likelihood.models.deep.autoencoders.vae_loss">vae_loss</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.models.deep.autoencoders.AutoClassifier" href="#likelihood.models.deep.autoencoders.AutoClassifier">AutoClassifier</a></code></h4>
<ul class="">
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.build" href="#likelihood.models.deep.autoencoders.AutoClassifier.build">build</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.build_encoder_decoder" href="#likelihood.models.deep.autoencoders.AutoClassifier.build_encoder_decoder">build_encoder_decoder</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.call" href="#likelihood.models.deep.autoencoders.AutoClassifier.call">call</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.freeze_encoder_decoder" href="#likelihood.models.deep.autoencoders.AutoClassifier.freeze_encoder_decoder">freeze_encoder_decoder</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.from_config" href="#likelihood.models.deep.autoencoders.AutoClassifier.from_config">from_config</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.get_config" href="#likelihood.models.deep.autoencoders.AutoClassifier.get_config">get_config</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.set_encoder_decoder" href="#likelihood.models.deep.autoencoders.AutoClassifier.set_encoder_decoder">set_encoder_decoder</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.train_encoder_decoder" href="#likelihood.models.deep.autoencoders.AutoClassifier.train_encoder_decoder">train_encoder_decoder</a></code></li>
<li><code><a title="likelihood.models.deep.autoencoders.AutoClassifier.unfreeze_encoder_decoder" href="#likelihood.models.deep.autoencoders.AutoClassifier.unfreeze_encoder_decoder">unfreeze_encoder_decoder</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.models.deep.autoencoders.EarlyStopping" href="#likelihood.models.deep.autoencoders.EarlyStopping">EarlyStopping</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
