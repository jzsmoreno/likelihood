<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>likelihood.models.regression API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.models.regression</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.models.regression.AbstractArima"><code class="flex name class">
<span>class <span class="ident">AbstractArima</span></span>
<span>(</span><span>datapoints: numpy.ndarray, noise: float = 0, tol: float = 0.0001)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"><p>A class that implements the auto-regressive arima (1, 0, 0) model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points for training.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Noise level for the model, by default 0</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Tolerance for convergence, by default 1e-4</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points for training.</dd>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps to predict.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code></dt>
<dd>Noise level for the model.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>int</code></dt>
<dd>Order of autoregressive part.</dd>
<dt><strong><code>q</code></strong> :&ensp;<code>int</code></dt>
<dd>Order of moving average part.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>Tolerance for convergence.</dd>
<dt><strong><code>nwalkers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of walkers for sampling.</dd>
<dt><strong><code>mov</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of iterations.</dd>
<dt><strong><code>theta_trained</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Trained parameters of the model.</dd>
</dl>
<p>Initialize the ARIMA model.</p>
<h2 id="parameters_1">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points for training.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Noise level for the model, by default 0</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Tolerance for convergence, by default 1e-4</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="likelihood.models.utils.FeaturesArima" href="utils.html#likelihood.models.utils.FeaturesArima">FeaturesArima</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="likelihood.models.regression.Arima" href="#likelihood.models.regression.Arima">Arima</a></li>
<li><a title="likelihood.models.regression.FourierRegression" href="#likelihood.models.regression.FourierRegression">FourierRegression</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.models.regression.AbstractArima.datapoints"><code class="name">var <span class="ident">datapoints</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.mov"><code class="name">var <span class="ident">mov</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.n_steps"><code class="name">var <span class="ident">n_steps</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.noise"><code class="name">var <span class="ident">noise</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.nwalkers"><code class="name">var <span class="ident">nwalkers</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.p"><code class="name">var <span class="ident">p</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.q"><code class="name">var <span class="ident">q</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.theta_trained"><code class="name">var <span class="ident">theta_trained</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.tol"><code class="name">var <span class="ident">tol</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractArima(FeaturesArima):
    &#34;&#34;&#34;A class that implements the auto-regressive arima (1, 0, 0) model

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    noise : `float`, optional
        Noise level for the model, by default 0
    tol : `float`, optional
        Tolerance for convergence, by default 1e-4

    Attributes
    ----------
    datapoints : `np.ndarray`
        The input data points for training.
    n_steps : `int`
        Number of steps to predict.
    noise : `float`
        Noise level for the model.
    p : `int`
        Order of autoregressive part.
    q : `int`
        Order of moving average part.
    tol : `float`
        Tolerance for convergence.
    nwalkers : `int`
        Number of walkers for sampling.
    mov : `int`
        Maximum number of iterations.
    theta_trained : `np.ndarray`
        Trained parameters of the model.
    &#34;&#34;&#34;

    __slots__ = [
        &#34;datapoints&#34;,
        &#34;n_steps&#34;,
        &#34;noise&#34;,
        &#34;p&#34;,
        &#34;q&#34;,
        &#34;tol&#34;,
        &#34;nwalkers&#34;,
        &#34;mov&#34;,
        &#34;theta_trained&#34;,
    ]

    def __init__(self, datapoints: np.ndarray, noise: float = 0, tol: float = 1e-4):
        &#34;&#34;&#34;Initialize the ARIMA model.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points for training.
        noise : `float`, optional
            Noise level for the model, by default 0
        tol : `float`, optional
            Tolerance for convergence, by default 1e-4
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        self.p = datapoints.shape[0]
        self.q = 0
        self.tol = tol
        self.n_steps = 0

    def model(self, datapoints: np.ndarray, theta: list, mode=True):
        &#34;&#34;&#34;Compute the model forward pass.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        theta : `list`
            Model parameters.
        mode : `bool`, optional
            Forward pass mode, by default True

        Returns
        -------
        `np.ndarray`
            Model output.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        return super().forward(datapoints, theta, mode, noise)

    def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
        &#34;&#34;&#34;Extract vector of data points.

        Parameters
        ----------
        datapoints : `np.ndarray`
            The input data points.
        n_steps : `int`, optional
            Number of steps to consider, by default 0

        Returns
        -------
        `np.ndarray`
            Extracted data points vector.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        self.n_steps = n_steps

        return datapoints[n_steps:]

    def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
        &#34;&#34;&#34;Train the model using sampling method.

        Parameters
        ----------
        nwalkers : `int`, optional
            Number of walkers for sampling, by default 10
        mov : `int`, optional
            Maximum number of iterations, by default 200
        weights : `bool`, optional
            Whether to use weights in sampling, by default False
        &#34;&#34;&#34;
        datapoints = self.datapoints
        xvec = self.xvec
        self.nwalkers = nwalkers
        self.mov = mov

        assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
        model = self.model
        n = self.p + self.q
        theta = np.random.rand(n)
        x_vec = xvec(datapoints)

        if weights:
            par, error = walkers(
                nwalkers,
                x_vec,
                datapoints,
                model,
                theta=self.theta_trained,
                mov=mov,
                tol=self.tol,
                figname=None,
            )
        else:
            par, error = walkers(
                nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
            )

        index = np.where(error == np.min(error))[0][0]
        trained = np.array(par[index])

        self.theta_trained = trained

    def predict(self, n_steps: int = 0):
        &#34;&#34;&#34;Make predictions for future steps.

        Parameters
        ----------
        n_steps : `int`, optional
            Number of steps to predict, by default 0

        Returns
        -------
        `np.ndarray`
            Predicted values.
        &#34;&#34;&#34;
        self.n_steps = n_steps
        datapoints = self.datapoints
        model = self.model
        theta_trained = self.theta_trained
        y_pred = model(datapoints, theta_trained)

        for i in range(n_steps):
            self.datapoints = y_pred[i:]
            y_new = model(datapoints, theta_trained, mode=False)
            y_pred = y_pred.tolist()
            y_pred.append(y_new)
            y_pred = np.array(y_pred)

        return np.array(y_pred)

    def save_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
            pickle.dump(self.theta_trained, file)

    def load_model(self, name: str = &#34;model&#34;):
        with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
            self.theta_trained = pickle.load(file)

    def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
        rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
        square_error = np.sqrt((y_pred - y_val) ** 2)
        accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
        accuracy /= np.sum(square_error)
        print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
        print(&#34;RMSE: {:.4f}&#34;.format(rmse))

    def plot_pred(
        self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
    ):
        sns.set_theme(style=&#34;whitegrid&#34;)
        plt.figure(figsize=(5, 3))
        n = self.n_steps
        y_mean = np.mean(y_pred, axis=0)
        y_std = np.std(y_pred, axis=0)
        if ci &lt; 0.95:
            Z = (ci / 0.90) * 1.64
        else:
            Z = (ci / 0.95) * 1.96
        plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
        plt.plot(
            y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
        )
        plt.fill_between(
            range(y_pred.shape[0])[-n:],
            (y_pred - Z * y_std)[-n:],
            (y_pred + Z * y_std)[-n:],
            alpha=0.2,
            color=sns.color_palette(&#34;deep&#34;)[1],
        )
        plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
        plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
        plt.ylabel(&#34;y&#34;, fontsize=12)
        plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
        plt.legend(loc=&#34;upper left&#34;, fontsize=9)
        if mode:
            plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
        plt.tight_layout()
        plt.show()

    def summary(self):
        print(&#34;\nSummary:&#34;)
        print(&#34;-----------------------&#34;)
        print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
        print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
        print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.regression.AbstractArima.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self, y_val: numpy.ndarray, y_pred: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval(self, y_val: np.ndarray, y_pred: np.ndarray):
    rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))
    square_error = np.sqrt((y_pred - y_val) ** 2)
    accuracy = np.sum(square_error[np.where(square_error &lt; rmse)])
    accuracy /= np.sum(square_error)
    print(&#34;Accuracy: {:.4f}&#34;.format(accuracy))
    print(&#34;RMSE: {:.4f}&#34;.format(rmse))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, name: str = 'model')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self, name: str = &#34;model&#34;):
    with open(name + &#34;.pkl&#34;, &#34;rb&#34;) as file:
        self.theta_trained = pickle.load(file)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.model"><code class="name flex">
<span>def <span class="ident">model</span></span>(<span>self, datapoints: numpy.ndarray, theta: list, mode=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model(self, datapoints: np.ndarray, theta: list, mode=True):
    &#34;&#34;&#34;Compute the model forward pass.

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points.
    theta : `list`
        Model parameters.
    mode : `bool`, optional
        Forward pass mode, by default True

    Returns
    -------
    `np.ndarray`
        Model output.
    &#34;&#34;&#34;
    datapoints = self.datapoints
    noise = self.noise
    self.theta_trained = theta

    return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"><p>Compute the model forward pass.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points.</dd>
<dt><strong><code>theta</code></strong> :&ensp;<code>list</code></dt>
<dd>Model parameters.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Forward pass mode, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>np.ndarray</code>
Model output.</p></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.plot_pred"><code class="name flex">
<span>def <span class="ident">plot_pred</span></span>(<span>self,<br>y_real: numpy.ndarray,<br>y_pred: numpy.ndarray,<br>ci: float = 0.9,<br>mode: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_pred(
    self, y_real: np.ndarray, y_pred: np.ndarray, ci: float = 0.90, mode: bool = True
):
    sns.set_theme(style=&#34;whitegrid&#34;)
    plt.figure(figsize=(5, 3))
    n = self.n_steps
    y_mean = np.mean(y_pred, axis=0)
    y_std = np.std(y_pred, axis=0)
    if ci &lt; 0.95:
        Z = (ci / 0.90) * 1.64
    else:
        Z = (ci / 0.95) * 1.96
    plt.plot(y_pred, label=&#34;Predicted&#34;, linewidth=2, color=sns.color_palette(&#34;deep&#34;)[1])
    plt.plot(
        y_real, &#34;.--&#34;, label=&#34;Real&#34;, color=sns.color_palette(&#34;deep&#34;)[0], alpha=0.6, markersize=6
    )
    plt.fill_between(
        range(y_pred.shape[0])[-n:],
        (y_pred - Z * y_std)[-n:],
        (y_pred + Z * y_std)[-n:],
        alpha=0.2,
        color=sns.color_palette(&#34;deep&#34;)[1],
    )
    plt.title(&#34;Predicted vs Real Values with Confidence Interval&#34;, fontsize=12)
    plt.xlabel(&#34;Time Steps&#34;, fontsize=12)
    plt.ylabel(&#34;y&#34;, fontsize=12)
    plt.grid(True, linestyle=&#34;--&#34;, alpha=0.7)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    print(f&#34;Confidence Interval: ±{Z * y_std:.4f}&#34;)
    plt.legend(loc=&#34;upper left&#34;, fontsize=9)
    if mode:
        plt.savefig(f&#34;pred_{n}.png&#34;, dpi=300)
    plt.tight_layout()
    plt.show()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, n_steps: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, n_steps: int = 0):
    &#34;&#34;&#34;Make predictions for future steps.

    Parameters
    ----------
    n_steps : `int`, optional
        Number of steps to predict, by default 0

    Returns
    -------
    `np.ndarray`
        Predicted values.
    &#34;&#34;&#34;
    self.n_steps = n_steps
    datapoints = self.datapoints
    model = self.model
    theta_trained = self.theta_trained
    y_pred = model(datapoints, theta_trained)

    for i in range(n_steps):
        self.datapoints = y_pred[i:]
        y_new = model(datapoints, theta_trained, mode=False)
        y_pred = y_pred.tolist()
        y_pred.append(y_new)
        y_pred = np.array(y_pred)

    return np.array(y_pred)</code></pre>
</details>
<div class="desc"><p>Make predictions for future steps.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of steps to predict, by default 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>np.ndarray</code>
Predicted values.</p></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, name: str = 'model')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self, name: str = &#34;model&#34;):
    with open(name + &#34;.pkl&#34;, &#34;wb&#34;) as file:
        pickle.dump(self.theta_trained, file)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.summary"><code class="name flex">
<span>def <span class="ident">summary</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def summary(self):
    print(&#34;\nSummary:&#34;)
    print(&#34;-----------------------&#34;)
    print(&#34;Lenght of theta: {}&#34;.format(len(self.theta_trained)))
    print(&#34;Mean of theta: {:.4f}&#34;.format(np.mean(self.theta_trained)))
    print(&#34;-----------------------&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nwalkers: int = 10, mov: int = 200, weights: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nwalkers: int = 10, mov: int = 200, weights: bool = False):
    &#34;&#34;&#34;Train the model using sampling method.

    Parameters
    ----------
    nwalkers : `int`, optional
        Number of walkers for sampling, by default 10
    mov : `int`, optional
        Maximum number of iterations, by default 200
    weights : `bool`, optional
        Whether to use weights in sampling, by default False
    &#34;&#34;&#34;
    datapoints = self.datapoints
    xvec = self.xvec
    self.nwalkers = nwalkers
    self.mov = mov

    assert self.nwalkers &lt;= self.mov, &#34;n_walkers must be less or equal than mov&#34;
    model = self.model
    n = self.p + self.q
    theta = np.random.rand(n)
    x_vec = xvec(datapoints)

    if weights:
        par, error = walkers(
            nwalkers,
            x_vec,
            datapoints,
            model,
            theta=self.theta_trained,
            mov=mov,
            tol=self.tol,
            figname=None,
        )
    else:
        par, error = walkers(
            nwalkers, x_vec, datapoints, model, theta, mov=mov, tol=self.tol, figname=None
        )

    index = np.where(error == np.min(error))[0][0]
    trained = np.array(par[index])

    self.theta_trained = trained</code></pre>
</details>
<div class="desc"><p>Train the model using sampling method.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>nwalkers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of walkers for sampling, by default 10</dd>
<dt><strong><code>mov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of iterations, by default 200</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use weights in sampling, by default False</dd>
</dl></div>
</dd>
<dt id="likelihood.models.regression.AbstractArima.xvec"><code class="name flex">
<span>def <span class="ident">xvec</span></span>(<span>self, datapoints: numpy.ndarray, n_steps: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xvec(self, datapoints: np.ndarray, n_steps: int = 0):
    &#34;&#34;&#34;Extract vector of data points.

    Parameters
    ----------
    datapoints : `np.ndarray`
        The input data points.
    n_steps : `int`, optional
        Number of steps to consider, by default 0

    Returns
    -------
    `np.ndarray`
        Extracted data points vector.
    &#34;&#34;&#34;
    datapoints = self.datapoints
    self.n_steps = n_steps

    return datapoints[n_steps:]</code></pre>
</details>
<div class="desc"><p>Extract vector of data points.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points.</dd>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of steps to consider, by default 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>np.ndarray</code>
Extracted data points vector.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="likelihood.models.utils.FeaturesArima" href="utils.html#likelihood.models.utils.FeaturesArima">FeaturesArima</a></b></code>:
<ul class="hlist">
<li><code><a title="likelihood.models.utils.FeaturesArima.average" href="utils.html#likelihood.models.utils.FeaturesArima.average">average</a></code></li>
<li><code><a title="likelihood.models.utils.FeaturesArima.forward" href="utils.html#likelihood.models.utils.FeaturesArima.forward">forward</a></code></li>
<li><code><a title="likelihood.models.utils.FeaturesArima.integrated" href="utils.html#likelihood.models.utils.FeaturesArima.integrated">integrated</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="likelihood.models.regression.Arima"><code class="flex name class">
<span>class <span class="ident">Arima</span></span>
<span>(</span><span>datapoints: numpy.ndarray,<br>p: float = 1,<br>d: int = 0,<br>q: float = 0,<br>noise: float = 0,<br>tol: float = 1e-05)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"><p>A class that implements the (p, d, q) ARIMA model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of points to train the ARIMA model.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code></dt>
<dd>Number of auto-regressive terms (ratio). By default it is set to <code>1</code>.</dd>
<dt><strong><code>d</code></strong> :&ensp;<code>int</code></dt>
<dd>Degree of differencing. By default it is set to <code>0</code>.</dd>
<dt><strong><code>q</code></strong> :&ensp;<code>float</code></dt>
<dd>Number of forecast errors in the model (ratio). By default it is set to <code>0</code>.</dd>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps to predict ahead.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code></dt>
<dd>Amount of noise added during training.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>Tolerance for convergence checks.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The values of <code>p</code>, <code>q</code> are scaled based on the length of <code>datapoints</code>.</p>
<p>Initializes the ARIMA model with given parameters.</p>
<h2 id="parameters_1">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of points to train the ARIMA model.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code></dt>
<dd>Auto-regressive term (scaled by length of data).</dd>
<dt><strong><code>d</code></strong> :&ensp;<code>int</code></dt>
<dd>Degree of differencing.</dd>
<dt><strong><code>q</code></strong> :&ensp;<code>float</code></dt>
<dd>Moving average term (scaled by length of data).</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code></dt>
<dd>Noise level for training.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>Tolerance for numerical convergence.</dd>
</dl>
<h2 id="returns_1">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="likelihood.models.regression.AbstractArima" href="#likelihood.models.regression.AbstractArima">AbstractArima</a></li>
<li><a title="likelihood.models.utils.FeaturesArima" href="utils.html#likelihood.models.utils.FeaturesArima">FeaturesArima</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.models.regression.Arima.d"><code class="name">var <span class="ident">d</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.datapoints"><code class="name">var <span class="ident">datapoints</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.noise"><code class="name">var <span class="ident">noise</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.p"><code class="name">var <span class="ident">p</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.q"><code class="name">var <span class="ident">q</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.theta_trained"><code class="name">var <span class="ident">theta_trained</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.Arima.tol"><code class="name">var <span class="ident">tol</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Arima(AbstractArima):
    &#34;&#34;&#34;A class that implements the (p, d, q) ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        A set of points to train the ARIMA model.
    p : float
        Number of auto-regressive terms (ratio). By default it is set to `1`.
    d : int
        Degree of differencing. By default it is set to `0`.
    q : float
        Number of forecast errors in the model (ratio). By default it is set to `0`.
    n_steps : int
        Number of steps to predict ahead.
    noise : float
        Amount of noise added during training.
    tol : float
        Tolerance for convergence checks.

    Returns
    -------
    None

    Notes
    -----
    The values of `p`, `q` are scaled based on the length of `datapoints`.
    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints&#34;, &#34;noise&#34;, &#34;p&#34;, &#34;d&#34;, &#34;q&#34;, &#34;tol&#34;, &#34;theta_trained&#34;]

    def __init__(
        self,
        datapoints: np.ndarray,
        p: float = 1,
        d: int = 0,
        q: float = 0,
        noise: float = 0,
        tol: float = 1e-5,
    ):
        &#34;&#34;&#34;Initializes the ARIMA model with given parameters.

        Parameters
        ----------
        datapoints : np.ndarray
            A set of points to train the ARIMA model.
        p : float
            Auto-regressive term (scaled by length of data).
        d : int
            Degree of differencing.
        q : float
            Moving average term (scaled by length of data).
        noise : float
            Noise level for training.
        tol : float
            Tolerance for numerical convergence.

        Returns
        -------
        None
        &#34;&#34;&#34;
        self.datapoints = datapoints
        self.noise = noise
        assert p &gt; 0 and p &lt;= 1, &#34;p must be less than 1 but greater than 0&#34;
        self.p = int(p * len(datapoints))
        assert d &gt;= 0 and d &lt;= 1, &#34;p must be less than 1 but greater than or equal to 0&#34;
        self.d = d
        self.q = int(q * len(datapoints))
        self.tol = tol

    def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
        &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

        Parameters
        ----------
        datapoints : np.ndarray
            The input data used for modeling.
        theta : list
            Model parameters.
        mode : bool
            If True, computes in forward mode; otherwise in backward mode.

        Returns
        -------
        y_vec : np.ndarray
            Predicted values according to the ARIMA model.
        &#34;&#34;&#34;
        datapoints = self.datapoints
        noise = self.noise
        self.theta_trained = theta

        assert type(self.d) == int, &#34;d must be 0 or 1&#34;

        if self.d != 0 or self.q != 0:
            if self.d != 0:
                y_sum = super().integrated(datapoints)
                norm_datapoints = np.linalg.norm(datapoints)
                norm_y_sum = np.linalg.norm(y_sum)
                if norm_y_sum != 0 and norm_datapoints != 0:
                    y_sum = cal_average(
                        np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                    )
            else:
                y_sum = datapoints.copy()

            y_sum_regr = y_sum[-self.p :]
            y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
            if self.q != 0:
                y_sum_average = super().average(y_sum[-self.q :])
                y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
                y_sum_average_magnitude = np.linalg.norm(y_sum_average)

                if y_sum_average_magnitude &gt; y_vec_magnitude:
                    scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                    y_sum_average = y_sum_average * scaling_factor
                theta_mean = np.mean(theta[-self.q :])
                if abs(theta_mean) &gt; 1:
                    additional_scaling_factor = 1.0 - abs(theta_mean)
                    y_sum_average = y_sum_average * additional_scaling_factor
                y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
                if mode:
                    y_vec = y_regr_vec.copy()
                    for i in reversed(range(y_average_vec.shape[0])):
                        y_vec[i] += y_average_vec[i]
                else:
                    y_vec = y_regr_vec + y_average_vec
            else:
                y_vec = y_regr_vec
            return y_vec
        else:
            return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.regression.Arima.model"><code class="name flex">
<span>def <span class="ident">model</span></span>(<span>self, datapoints: numpy.ndarray, theta: list, mode: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model(self, datapoints: np.ndarray, theta: list, mode: bool = True):
    &#34;&#34;&#34;Computes the prior probability or prediction based on ARIMA model.

    Parameters
    ----------
    datapoints : np.ndarray
        The input data used for modeling.
    theta : list
        Model parameters.
    mode : bool
        If True, computes in forward mode; otherwise in backward mode.

    Returns
    -------
    y_vec : np.ndarray
        Predicted values according to the ARIMA model.
    &#34;&#34;&#34;
    datapoints = self.datapoints
    noise = self.noise
    self.theta_trained = theta

    assert type(self.d) == int, &#34;d must be 0 or 1&#34;

    if self.d != 0 or self.q != 0:
        if self.d != 0:
            y_sum = super().integrated(datapoints)
            norm_datapoints = np.linalg.norm(datapoints)
            norm_y_sum = np.linalg.norm(y_sum)
            if norm_y_sum != 0 and norm_datapoints != 0:
                y_sum = cal_average(
                    np.abs(y_sum * (norm_datapoints / norm_y_sum)) * np.sign(datapoints), 0.05
                )
        else:
            y_sum = datapoints.copy()

        y_sum_regr = y_sum[-self.p :]
        y_regr_vec = super().forward(y_sum_regr, theta[0 : self.p], mode, 0)
        if self.q != 0:
            y_sum_average = super().average(y_sum[-self.q :])
            y_vec_magnitude = np.linalg.norm(y_regr_vec.copy())
            y_sum_average_magnitude = np.linalg.norm(y_sum_average)

            if y_sum_average_magnitude &gt; y_vec_magnitude:
                scaling_factor = y_vec_magnitude / y_sum_average_magnitude
                y_sum_average = y_sum_average * scaling_factor
            theta_mean = np.mean(theta[-self.q :])
            if abs(theta_mean) &gt; 1:
                additional_scaling_factor = 1.0 - abs(theta_mean)
                y_sum_average = y_sum_average * additional_scaling_factor
            y_average_vec = super().forward(y_sum_average, theta[-self.q :], mode, 0)
            if mode:
                y_vec = y_regr_vec.copy()
                for i in reversed(range(y_average_vec.shape[0])):
                    y_vec[i] += y_average_vec[i]
            else:
                y_vec = y_regr_vec + y_average_vec
        else:
            y_vec = y_regr_vec
        return y_vec
    else:
        return super().forward(datapoints, theta, mode, noise)</code></pre>
</details>
<div class="desc"><p>Computes the prior probability or prediction based on ARIMA model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data used for modeling.</dd>
<dt><strong><code>theta</code></strong> :&ensp;<code>list</code></dt>
<dd>Model parameters.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, computes in forward mode; otherwise in backward mode.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y_vec</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Predicted values according to the ARIMA model.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="likelihood.models.regression.AbstractArima" href="#likelihood.models.regression.AbstractArima">AbstractArima</a></b></code>:
<ul class="hlist">
<li><code><a title="likelihood.models.regression.AbstractArima.average" href="utils.html#likelihood.models.utils.FeaturesArima.average">average</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.forward" href="utils.html#likelihood.models.utils.FeaturesArima.forward">forward</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.integrated" href="utils.html#likelihood.models.utils.FeaturesArima.integrated">integrated</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.predict" href="#likelihood.models.regression.AbstractArima.predict">predict</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.train" href="#likelihood.models.regression.AbstractArima.train">train</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.xvec" href="#likelihood.models.regression.AbstractArima.xvec">xvec</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="likelihood.models.regression.FourierRegression"><code class="flex name class">
<span>class <span class="ident">FourierRegression</span></span>
<span>(</span><span>datapoints: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"><p>A class that implements the arima model with FFT noise filtering</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>A set of points to train the arima model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>It is the number of predicted points. It is necessary
to apply predict(n_steps) followed by fit()</dd>
</dl>
<p>Initialize the ARIMA model.</p>
<h2 id="parameters_1">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The input data points for training.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Noise level for the model, by default 0</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Tolerance for convergence, by default 1e-4</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="likelihood.models.regression.AbstractArima" href="#likelihood.models.regression.AbstractArima">AbstractArima</a></li>
<li><a title="likelihood.models.utils.FeaturesArima" href="utils.html#likelihood.models.utils.FeaturesArima">FeaturesArima</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.models.regression.FourierRegression.datapoints_"><code class="name">var <span class="ident">datapoints_</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.mode"><code class="name">var <span class="ident">mode</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.mov"><code class="name">var <span class="ident">mov</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.n_walkers"><code class="name">var <span class="ident">n_walkers</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.name"><code class="name">var <span class="ident">name</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.sigma"><code class="name">var <span class="ident">sigma</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FourierRegression(AbstractArima):
    &#34;&#34;&#34;A class that implements the arima model with FFT noise filtering

    Parameters
    ----------
    datapoints : np.array
        A set of points to train the arima model.

    Returns
    -------
    new_datapoints : np.array
        It is the number of predicted points. It is necessary
        to apply predict(n_steps) followed by fit()

    &#34;&#34;&#34;

    __slots__ = [&#34;datapoints_&#34;, &#34;sigma&#34;, &#34;mode&#34;, &#34;mov&#34;, &#34;n_walkers&#34;, &#34;name&#34;]

    def __init__(self, datapoints: np.ndarray):
        self.datapoints_ = datapoints

    def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
        self.sigma = sigma
        self.mode = mode
        self.mov = mov

        datapoints = self.datapoints_
        self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)

    def predict(
        self, n_steps: int, n_walkers: int = 1, name: str = &#34;fourier_model&#34;, save: bool = True
    ):
        self.n_walkers = n_walkers
        self.name = name
        mov = self.mov

        assert self.n_walkers &lt;= mov, &#34;n_walkers must be less or equal than mov&#34;

        new_datapoints = []
        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().train(n_walkers, mov)
            if save:
                super().save_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints

    def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
        new_datapoints = []

        for i in range(self.datapoints_.shape[0]):
            super().__init__(self.datapoints_[i, :])
            super().load_model(str(i) + &#34;_&#34; + name)
            y_pred_ = super().predict(n_steps)
            new_datapoints.append(y_pred_)

        new_datapoints = np.array(new_datapoints)
        new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

        return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.models.regression.FourierRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, sigma: int = 0, mov: int = 200, mode: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, sigma: int = 0, mov: int = 200, mode: bool = False):
    self.sigma = sigma
    self.mode = mode
    self.mov = mov

    datapoints = self.datapoints_
    self.datapoints_, _ = fft_denoise(datapoints, sigma, mode)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.models.regression.FourierRegression.load_predict"><code class="name flex">
<span>def <span class="ident">load_predict</span></span>(<span>self, n_steps: int, name: str = 'fourier_model')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_predict(self, n_steps: int, name: str = &#34;fourier_model&#34;):
    new_datapoints = []

    for i in range(self.datapoints_.shape[0]):
        super().__init__(self.datapoints_[i, :])
        super().load_model(str(i) + &#34;_&#34; + name)
        y_pred_ = super().predict(n_steps)
        new_datapoints.append(y_pred_)

    new_datapoints = np.array(new_datapoints)
    new_datapoints = np.reshape(new_datapoints, (len(new_datapoints), -1))

    return new_datapoints</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="likelihood.models.regression.AbstractArima" href="#likelihood.models.regression.AbstractArima">AbstractArima</a></b></code>:
<ul class="hlist">
<li><code><a title="likelihood.models.regression.AbstractArima.average" href="utils.html#likelihood.models.utils.FeaturesArima.average">average</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.forward" href="utils.html#likelihood.models.utils.FeaturesArima.forward">forward</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.integrated" href="utils.html#likelihood.models.utils.FeaturesArima.integrated">integrated</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.model" href="#likelihood.models.regression.AbstractArima.model">model</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.predict" href="#likelihood.models.regression.AbstractArima.predict">predict</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.train" href="#likelihood.models.regression.AbstractArima.train">train</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.xvec" href="#likelihood.models.regression.AbstractArima.xvec">xvec</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.models" href="index.html">likelihood.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.models.regression.AbstractArima" href="#likelihood.models.regression.AbstractArima">AbstractArima</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.models.regression.AbstractArima.datapoints" href="#likelihood.models.regression.AbstractArima.datapoints">datapoints</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.eval" href="#likelihood.models.regression.AbstractArima.eval">eval</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.load_model" href="#likelihood.models.regression.AbstractArima.load_model">load_model</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.model" href="#likelihood.models.regression.AbstractArima.model">model</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.mov" href="#likelihood.models.regression.AbstractArima.mov">mov</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.n_steps" href="#likelihood.models.regression.AbstractArima.n_steps">n_steps</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.noise" href="#likelihood.models.regression.AbstractArima.noise">noise</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.nwalkers" href="#likelihood.models.regression.AbstractArima.nwalkers">nwalkers</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.p" href="#likelihood.models.regression.AbstractArima.p">p</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.plot_pred" href="#likelihood.models.regression.AbstractArima.plot_pred">plot_pred</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.predict" href="#likelihood.models.regression.AbstractArima.predict">predict</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.q" href="#likelihood.models.regression.AbstractArima.q">q</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.save_model" href="#likelihood.models.regression.AbstractArima.save_model">save_model</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.summary" href="#likelihood.models.regression.AbstractArima.summary">summary</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.theta_trained" href="#likelihood.models.regression.AbstractArima.theta_trained">theta_trained</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.tol" href="#likelihood.models.regression.AbstractArima.tol">tol</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.train" href="#likelihood.models.regression.AbstractArima.train">train</a></code></li>
<li><code><a title="likelihood.models.regression.AbstractArima.xvec" href="#likelihood.models.regression.AbstractArima.xvec">xvec</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.models.regression.Arima" href="#likelihood.models.regression.Arima">Arima</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.models.regression.Arima.d" href="#likelihood.models.regression.Arima.d">d</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.datapoints" href="#likelihood.models.regression.Arima.datapoints">datapoints</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.model" href="#likelihood.models.regression.Arima.model">model</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.noise" href="#likelihood.models.regression.Arima.noise">noise</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.p" href="#likelihood.models.regression.Arima.p">p</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.q" href="#likelihood.models.regression.Arima.q">q</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.theta_trained" href="#likelihood.models.regression.Arima.theta_trained">theta_trained</a></code></li>
<li><code><a title="likelihood.models.regression.Arima.tol" href="#likelihood.models.regression.Arima.tol">tol</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.models.regression.FourierRegression" href="#likelihood.models.regression.FourierRegression">FourierRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.models.regression.FourierRegression.datapoints_" href="#likelihood.models.regression.FourierRegression.datapoints_">datapoints_</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.fit" href="#likelihood.models.regression.FourierRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.load_predict" href="#likelihood.models.regression.FourierRegression.load_predict">load_predict</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.mode" href="#likelihood.models.regression.FourierRegression.mode">mode</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.mov" href="#likelihood.models.regression.FourierRegression.mov">mov</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.n_walkers" href="#likelihood.models.regression.FourierRegression.n_walkers">n_walkers</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.name" href="#likelihood.models.regression.FourierRegression.name">name</a></code></li>
<li><code><a title="likelihood.models.regression.FourierRegression.sigma" href="#likelihood.models.regression.FourierRegression.sigma">sigma</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
