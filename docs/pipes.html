<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>likelihood.pipes API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.pipes</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.pipes.Pipeline"><code class="flex name class">
<span>class <span class="ident">Pipeline</span></span>
<span>(</span><span>config_path: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Pipeline:
    def __init__(self, config_path: str):
        &#34;&#34;&#34;
        Initialize the pipeline with a JSON configuration file.

        Parameters
        ----------
        config_path : str
            Path to the JSON config defining target column and preprocessing steps.
        &#34;&#34;&#34;
        self.config = self._load_config(config_path)
        self.target_col = self.config[&#34;target_column&#34;]
        self.steps = self.config[&#34;preprocessing_steps&#34;]
        self.compute_importance = self.config.get(&#34;compute_feature_importance&#34;, False)
        self.fitted_components: Dict[str, object] = {}
        self.columns_bin_sizes: Dict[str, int] | None = None

    def _load_config(self, config_path: str) -&gt; Dict:
        &#34;&#34;&#34;Load and validate the JSON configuration.&#34;&#34;&#34;
        with open(config_path, &#34;r&#34;) as f:
            config = json.load(f)

        assert &#34;target_column&#34; in config, &#34;Config must specify &#39;target_column&#39;&#34;
        assert &#34;preprocessing_steps&#34; in config, &#34;Config must specify &#39;preprocessing_steps&#39;&#34;
        return config

    def fit(self, df: pd.DataFrame) -&gt; Tuple[pd.DataFrame, np.ndarray, Optional[np.ndarray]]:
        &#34;&#34;&#34;
        Fit preprocessing components on the input DataFrame and return cleaned X/y.

        Parameters
        ----------
        df : pd.DataFrame
            Input data with features + target column.

        Returns
        -------
        X : pd.DataFrame
            Cleaned feature matrix.
        y : np.ndarray
            Target vector (from self.target_col).
        importances : Optional[np.ndarray]
            Feature importance scores (if compute_feature_importance=True).
        &#34;&#34;&#34;
        y = df[self.target_col].values
        X = df.drop(columns=[self.target_col]).copy()

        initial_info = {
            &#34;shape&#34;: X.shape,
            &#34;columns&#34;: list(X.columns),
            &#34;dtypes&#34;: X.dtypes.apply(lambda x: x.name).to_dict(),
            &#34;missing_values&#34;: X.isnull().sum().to_dict(),
        }

        steps_info = []
        for step in self.steps:
            step_name = step[&#34;name&#34;]
            params = step.get(&#34;params&#34;, {})
            step_info = {
                &#34;step_name&#34;: step_name,
                &#34;parameters&#34;: params,
                &#34;description&#34;: self._get_step_description(step_name),
            }
            step_info[&#34;input_columns&#34;] = list(X.columns)

            X = self._apply_step(step_name, X, fit=True, **params)

            step_info[&#34;output_shape&#34;] = X.shape
            step_info[&#34;output_columns&#34;] = list(X.columns)
            step_info[&#34;output_dtypes&#34;] = X.dtypes.apply(lambda x: x.name).to_dict()

            steps_info.append(step_info)

        final_info = {
            &#34;shape&#34;: X.shape,
            &#34;columns&#34;: list(X.columns),
            &#34;dtypes&#34;: X.dtypes.apply(lambda x: x.name).to_dict(),
            &#34;missing_values&#34;: X.isnull().sum().to_dict(),
        }

        self.documentation = {
            &#34;initial_dataset&#34;: initial_info,
            &#34;processing_steps&#34;: steps_info,
            &#34;final_dataset&#34;: final_info,
        }

        importances = None
        if self.compute_importance:
            numeric_X = X.select_dtypes(include=[&#34;float&#34;])
            numeric_columns = numeric_X.columns.tolist()
            model = LinearRegression()
            model.fit(numeric_X.T.values, y)
            importances = model.get_importances()
            df_scores = pd.DataFrame([importances], columns=numeric_columns)
            df_scores_abs = df_scores.abs()
        df_scores_norm = (
            df_scores_abs / df_scores_abs.to_numpy().sum()
            if isinstance(importances, np.ndarray)
            else pd.DataFrame()
        )
        return X, y, df_scores_norm

    def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Apply fitted preprocessing steps to new data (no target column needed).

        Parameters
        ----------
        df : pd.DataFrame
            New data to transform.

        Returns
        -------
        X_transformed : pd.DataFrame
            Cleaned feature matrix.
        &#34;&#34;&#34;
        X = df.copy()
        for step_name, _ in self.fitted_components.items():
            X = self._apply_step(step_name, X, fit=False)

        return X

    def get_doc(
        self, save_to_file: bool = True, file_name: str = &#34;data_processing_report.html&#34;
    ) -&gt; None:
        &#34;&#34;&#34;
        Generate an HTML report from `self.documentation` for pipeline documentation.

        Parameters
        ----------
        save_to_file : bool, optional
            Whether to save generated HTML content to a file. Default is True.
        file_name : str, optional
            Filename for output when `save_to_file` is True. Default is &#34;data_processing_report.html&#34;.
        &#34;&#34;&#34;

        generate_html_pipeline(self.documentation, save_to_file=save_to_file, file_name=file_name)

    def _apply_step(self, step_name: str, X: pd.DataFrame, fit: bool, **params) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Dispatch to the correct handler for a preprocessing step.&#34;&#34;&#34;
        handlers = {
            &#34;DataScaler&#34;: self._handle_datascaler,
            &#34;DataFrameEncoder&#34;: self._handle_dataframeencoder,
            &#34;remove_collinearity&#34;: self._handle_remove_collinearity,
            &#34;TransformRange&#34;: self._handle_transformrange,
            &#34;OneHotEncoder&#34;: self._handle_onehotencoder,
            &#34;SimpleImputer&#34;: self._handle_simpleimputer,
        }

        if step_name not in handlers:
            raise ValueError(
                f&#34;Step &#39;{step_name}&#39; not supported. Supported steps: {list(handlers.keys())}&#34;
            )

        return handlers[step_name](X, fit=fit, **params)

    def _get_step_description(self, step_name: str) -&gt; str:
        &#34;&#34;&#34;Return a description of what each preprocessing step does.&#34;&#34;&#34;
        descriptions = {
            &#34;DataScaler&#34;: &#34;Scales numerical features using normalization&#34;,
            &#34;DataFrameEncoder&#34;: &#34;Encodes categorical variables and normalizes to numerical features&#34;,
            &#34;remove_collinearity&#34;: &#34;Removes highly correlated features to reduce multicollinearity&#34;,
            &#34;TransformRange&#34;: &#34;Bins continuous features into discrete ranges&#34;,
            &#34;OneHotEncoder&#34;: &#34;Converts categorical variables into binary variables&#34;,
            &#34;SimpleImputer&#34;: &#34;Handles missing values by imputing with multiple linear regression strategies&#34;,
        }

        return descriptions.get(step_name, f&#34;Unknown preprocessing step: {step_name}&#34;)

    # ------------------------------ Step Handlers ------------------------------
    def _handle_datascaler(self, X: pd.DataFrame, fit: bool, n: int = 1) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Handle DataScaler (fits on training data, applies to all).&#34;&#34;&#34;
        numeric_X = X.select_dtypes(include=[&#34;float&#34;])
        numeric_columns = numeric_X.columns.tolist()
        n = None if n == 0 else n
        if fit:
            scaler = DataScaler(numeric_X.values.T, n=n)
            self.fitted_components[&#34;DataScaler&#34;] = scaler
            numeric_X = pd.DataFrame(scaler.rescale().T, columns=numeric_X.columns)
        else:
            scaler = self.fitted_components[&#34;DataScaler&#34;]
            numeric_X = pd.DataFrame(
                scaler.rescale(numeric_X.values.T).T, columns=numeric_X.columns
            )
        for col in numeric_columns:
            X[col] = numeric_X[col]
        return X

    def _handle_dataframeencoder(
        self, X: pd.DataFrame, fit: bool, norm_method: str = &#34;mean&#34;
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Handle DataFrameEncoder (fits encoders/normalizers).&#34;&#34;&#34;
        if fit:
            encoder = DataFrameEncoder(X)
            encoded_X = encoder.encode(norm_method=norm_method)
            self.fitted_components[&#34;DataFrameEncoder&#34;] = encoder
            return encoded_X
        else:
            encoder = self.fitted_components[&#34;DataFrameEncoder&#34;]
            encoder._df = X
            return encoder.encode()

    def _handle_remove_collinearity(
        self, X: pd.DataFrame, fit: bool, threshold: float = 0.9
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Handle collinearity removal (fits by selecting columns to drop).&#34;&#34;&#34;
        numeric_X = X.select_dtypes(include=[&#34;float&#34;])
        numeric_columns = numeric_X.columns.tolist()
        categorical_columns = set(X.columns) - set(numeric_columns)
        if fit:
            cleaned_X = remove_collinearity(numeric_X, threshold=threshold)
            dropped_cols = set(X.columns) - set(cleaned_X.columns) - categorical_columns
            self.fitted_components[&#34;remove_collinearity&#34;] = dropped_cols
            return X.drop(columns=dropped_cols)
        else:
            dropped_cols = self.fitted_components[&#34;remove_collinearity&#34;]
            return X.drop(columns=dropped_cols)

    def _handle_transformrange(
        self, X: pd.DataFrame, fit: bool, columns_bin_sizes: Dict[str, int] | None = None
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Handle TransformRange (bin numerical features into ranges).&#34;&#34;&#34;
        if fit:
            transformer = TransformRange(columns_bin_sizes)
            cleaned_X = transformer.transform(X)
            self.fitted_components[&#34;TransformRange&#34;] = transformer
            self.columns_bin_sizes = columns_bin_sizes
            return cleaned_X
        else:
            transformer = self.fitted_components[&#34;TransformRange&#34;]
            return transformer.transform(X, fit=False)

    def _handle_onehotencoder(
        self, X: pd.DataFrame, fit: bool, columns: List[str] | None = None
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Handle OneHotEncoder (fits on categorical columns).&#34;&#34;&#34;
        if fit:
            tmp_df = X.drop(columns=columns)
            encoder = OneHotEncoder()
            category_to_indices = {}
            for col in columns:
                unique_values = X[col].unique()
                category_to_indices[col] = {value: i for i, value in enumerate(unique_values)}
                encoded_X = encoder.encode(
                    X[col].values
                    if isinstance(unique_values[0], int)
                    else X[col].map(category_to_indices[col])
                )
                tmp_df = pd.concat([tmp_df, pd.DataFrame(encoded_X, columns=unique_values)], axis=1)
            self.fitted_components[&#34;OneHotEncoder&#34;] = (encoder, columns, category_to_indices)
        else:
            encoder, columns, category_to_indices = self.fitted_components[&#34;OneHotEncoder&#34;]
            tmp_df = X.drop(columns=columns)
            for col in columns:
                unique_values = list(category_to_indices[col].keys())
                encoded_X = encoder.encode(
                    (
                        X[col].values
                        if isinstance(unique_values[0], int)
                        else X[col].map(category_to_indices[col])
                    ),
                    fit=False,
                )
                tmp_df = pd.concat([tmp_df, pd.DataFrame(encoded_X, columns=unique_values)], axis=1)
        return tmp_df

    def _handle_simpleimputer(
        self,
        X: pd.DataFrame,
        fit: bool,
        use_scaler: bool = False,
        boundary: bool = True,
    ) -&gt; pd.DataFrame:
        &#34;Handle SimpleImputer (fit on numerical and categorical columns).&#34;
        if fit:
            use_scaler = True if use_scaler == 1 else False
            imputer = SimpleImputer(use_scaler=use_scaler)
            tmp_df = imputer.fit_transform(X, boundary=boundary)
            self.fitted_components[&#34;SimpleImputer&#34;] = imputer
            return tmp_df
        else:
            imputer = self.fitted_components[&#34;SimpleImputer&#34;]
            return imputer.transform(X, boundary=boundary)

    def save(self, filepath: str) -&gt; None:
        &#34;&#34;&#34;
        Save the fitted pipeline state to a file using pickle.

        Parameters
        ----------
        filepath : str
            Path where the serialized pipeline will be saved.
        &#34;&#34;&#34;
        import pickle

        save_dict = {
            &#34;config&#34;: self.config,
            &#34;fitted_components&#34;: self.fitted_components,
            &#34;target_col&#34;: self.target_col,
            &#34;steps&#34;: self.steps,
            &#34;compute_importance&#34;: self.compute_importance,
            &#34;columns_bin_sizes&#34;: self.columns_bin_sizes,
            &#34;documentation&#34;: self.documentation,
        }

        filepath = filepath + &#34;.pkl&#34; if not filepath.endswith(&#34;.pkl&#34;) else filepath

        with open(filepath, &#34;wb&#34;) as f:
            pickle.dump(save_dict, f)

    @classmethod
    def load(cls, filepath: str) -&gt; &#34;Pipeline&#34;:
        &#34;&#34;&#34;
        Load a fitted pipeline from a file.

        Parameters
        ----------
        filepath : str
            Path to the serialized pipeline file.

        Returns
        -------
        pipeline : Pipeline
            Reconstructed pipeline instance with fitted components.
        &#34;&#34;&#34;
        import pickle

        filepath = filepath + &#34;.pkl&#34; if not filepath.endswith(&#34;.pkl&#34;) else filepath

        with open(filepath, &#34;rb&#34;) as f:
            save_dict = pickle.load(f)

        pipeline = cls.__new__(cls)

        pipeline.config = save_dict[&#34;config&#34;]
        pipeline.fitted_components = save_dict[&#34;fitted_components&#34;]
        pipeline.target_col = save_dict[&#34;target_col&#34;]
        pipeline.steps = save_dict[&#34;steps&#34;]
        pipeline.compute_importance = save_dict[&#34;compute_importance&#34;]
        pipeline.columns_bin_sizes = save_dict[&#34;columns_bin_sizes&#34;]
        pipeline.documentation = save_dict[&#34;documentation&#34;]

        return pipeline</code></pre>
</details>
<div class="desc"><p>Initialize the pipeline with a JSON configuration file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the JSON config defining target column and preprocessing steps.</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="likelihood.pipes.Pipeline.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>filepath: str) ‑> <a title="likelihood.pipes.Pipeline" href="#likelihood.pipes.Pipeline">Pipeline</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load a fitted pipeline from a file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the serialized pipeline file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pipeline</code></strong> :&ensp;<code><a title="likelihood.pipes.Pipeline" href="#likelihood.pipes.Pipeline">Pipeline</a></code></dt>
<dd>Reconstructed pipeline instance with fitted components.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.pipes.Pipeline.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, df: pandas.core.frame.DataFrame) ‑> Tuple[pandas.core.frame.DataFrame, numpy.ndarray, numpy.ndarray | None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, df: pd.DataFrame) -&gt; Tuple[pd.DataFrame, np.ndarray, Optional[np.ndarray]]:
    &#34;&#34;&#34;
    Fit preprocessing components on the input DataFrame and return cleaned X/y.

    Parameters
    ----------
    df : pd.DataFrame
        Input data with features + target column.

    Returns
    -------
    X : pd.DataFrame
        Cleaned feature matrix.
    y : np.ndarray
        Target vector (from self.target_col).
    importances : Optional[np.ndarray]
        Feature importance scores (if compute_feature_importance=True).
    &#34;&#34;&#34;
    y = df[self.target_col].values
    X = df.drop(columns=[self.target_col]).copy()

    initial_info = {
        &#34;shape&#34;: X.shape,
        &#34;columns&#34;: list(X.columns),
        &#34;dtypes&#34;: X.dtypes.apply(lambda x: x.name).to_dict(),
        &#34;missing_values&#34;: X.isnull().sum().to_dict(),
    }

    steps_info = []
    for step in self.steps:
        step_name = step[&#34;name&#34;]
        params = step.get(&#34;params&#34;, {})
        step_info = {
            &#34;step_name&#34;: step_name,
            &#34;parameters&#34;: params,
            &#34;description&#34;: self._get_step_description(step_name),
        }
        step_info[&#34;input_columns&#34;] = list(X.columns)

        X = self._apply_step(step_name, X, fit=True, **params)

        step_info[&#34;output_shape&#34;] = X.shape
        step_info[&#34;output_columns&#34;] = list(X.columns)
        step_info[&#34;output_dtypes&#34;] = X.dtypes.apply(lambda x: x.name).to_dict()

        steps_info.append(step_info)

    final_info = {
        &#34;shape&#34;: X.shape,
        &#34;columns&#34;: list(X.columns),
        &#34;dtypes&#34;: X.dtypes.apply(lambda x: x.name).to_dict(),
        &#34;missing_values&#34;: X.isnull().sum().to_dict(),
    }

    self.documentation = {
        &#34;initial_dataset&#34;: initial_info,
        &#34;processing_steps&#34;: steps_info,
        &#34;final_dataset&#34;: final_info,
    }

    importances = None
    if self.compute_importance:
        numeric_X = X.select_dtypes(include=[&#34;float&#34;])
        numeric_columns = numeric_X.columns.tolist()
        model = LinearRegression()
        model.fit(numeric_X.T.values, y)
        importances = model.get_importances()
        df_scores = pd.DataFrame([importances], columns=numeric_columns)
        df_scores_abs = df_scores.abs()
    df_scores_norm = (
        df_scores_abs / df_scores_abs.to_numpy().sum()
        if isinstance(importances, np.ndarray)
        else pd.DataFrame()
    )
    return X, y, df_scores_norm</code></pre>
</details>
<div class="desc"><p>Fit preprocessing components on the input DataFrame and return cleaned X/y.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Input data with features + target column.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Cleaned feature matrix.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target vector (from self.target_col).</dd>
<dt><strong><code>importances</code></strong> :&ensp;<code>Optional[np.ndarray]</code></dt>
<dd>Feature importance scores (if compute_feature_importance=True).</dd>
</dl></div>
</dd>
<dt id="likelihood.pipes.Pipeline.get_doc"><code class="name flex">
<span>def <span class="ident">get_doc</span></span>(<span>self, save_to_file: bool = True, file_name: str = 'data_processing_report.html') ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_doc(
    self, save_to_file: bool = True, file_name: str = &#34;data_processing_report.html&#34;
) -&gt; None:
    &#34;&#34;&#34;
    Generate an HTML report from `self.documentation` for pipeline documentation.

    Parameters
    ----------
    save_to_file : bool, optional
        Whether to save generated HTML content to a file. Default is True.
    file_name : str, optional
        Filename for output when `save_to_file` is True. Default is &#34;data_processing_report.html&#34;.
    &#34;&#34;&#34;

    generate_html_pipeline(self.documentation, save_to_file=save_to_file, file_name=file_name)</code></pre>
</details>
<div class="desc"><p>Generate an HTML report from <code>self.documentation</code> for pipeline documentation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>save_to_file</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to save generated HTML content to a file. Default is True.</dd>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Filename for output when <code>save_to_file</code> is True. Default is "data_processing_report.html".</dd>
</dl></div>
</dd>
<dt id="likelihood.pipes.Pipeline.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filepath: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, filepath: str) -&gt; None:
    &#34;&#34;&#34;
    Save the fitted pipeline state to a file using pickle.

    Parameters
    ----------
    filepath : str
        Path where the serialized pipeline will be saved.
    &#34;&#34;&#34;
    import pickle

    save_dict = {
        &#34;config&#34;: self.config,
        &#34;fitted_components&#34;: self.fitted_components,
        &#34;target_col&#34;: self.target_col,
        &#34;steps&#34;: self.steps,
        &#34;compute_importance&#34;: self.compute_importance,
        &#34;columns_bin_sizes&#34;: self.columns_bin_sizes,
        &#34;documentation&#34;: self.documentation,
    }

    filepath = filepath + &#34;.pkl&#34; if not filepath.endswith(&#34;.pkl&#34;) else filepath

    with open(filepath, &#34;wb&#34;) as f:
        pickle.dump(save_dict, f)</code></pre>
</details>
<div class="desc"><p>Save the fitted pipeline state to a file using pickle.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path where the serialized pipeline will be saved.</dd>
</dl></div>
</dd>
<dt id="likelihood.pipes.Pipeline.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, df: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Apply fitted preprocessing steps to new data (no target column needed).

    Parameters
    ----------
    df : pd.DataFrame
        New data to transform.

    Returns
    -------
    X_transformed : pd.DataFrame
        Cleaned feature matrix.
    &#34;&#34;&#34;
    X = df.copy()
    for step_name, _ in self.fitted_components.items():
        X = self._apply_step(step_name, X, fit=False)

    return X</code></pre>
</details>
<div class="desc"><p>Apply fitted preprocessing steps to new data (no target column needed).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>New data to transform.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_transformed</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Cleaned feature matrix.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood" href="index.html">likelihood</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.pipes.Pipeline" href="#likelihood.pipes.Pipeline">Pipeline</a></code></h4>
<ul class="">
<li><code><a title="likelihood.pipes.Pipeline.fit" href="#likelihood.pipes.Pipeline.fit">fit</a></code></li>
<li><code><a title="likelihood.pipes.Pipeline.get_doc" href="#likelihood.pipes.Pipeline.get_doc">get_doc</a></code></li>
<li><code><a title="likelihood.pipes.Pipeline.load" href="#likelihood.pipes.Pipeline.load">load</a></code></li>
<li><code><a title="likelihood.pipes.Pipeline.save" href="#likelihood.pipes.Pipeline.save">save</a></code></li>
<li><code><a title="likelihood.pipes.Pipeline.transform" href="#likelihood.pipes.Pipeline.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
