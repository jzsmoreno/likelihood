<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>likelihood.tools.models_tools API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.tools.models_tools</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.tools.models_tools.apply_lora"><code class="name flex">
<span>def <span class="ident">apply_lora</span></span>(<span>model, rank=4)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_lora(model, rank=4):
    inputs = tf.keras.Input(shape=model.input_shape[1:])
    x = inputs

    for layer in model.layers:
        if isinstance(layer, tf.keras.layers.Dense):
            print(f&#34;Applying LoRA to layer {layer.name}&#34;)
            x = LoRALayer(units=layer.units, rank=rank)(x)
        else:
            x = layer(x)
    new_model = tf.keras.Model(inputs=inputs, outputs=x)
    return new_model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.models_tools.collect_experience"><code class="name flex">
<span>def <span class="ident">collect_experience</span></span>(<span>env: Any,<br>model: torch.nn.modules.module.Module,<br>gamma: float = 0.99,<br>lambda_parameter: float = 0.95,<br>penalty_for_done_state: float = -1.0,<br>tolerance: int = inf,<br>verbose: bool = False) ‑> tuple[typing.List[tuple], typing.List[float], typing.List[float], typing.List[float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collect_experience(
    env: Any,
    model: torch.nn.Module,
    gamma: float = 0.99,
    lambda_parameter: float = 0.95,
    penalty_for_done_state: float = -1.0,
    tolerance: int = float(&#34;inf&#34;),
    verbose: bool = False,
) -&gt; tuple[List[tuple], List[float], List[float], List[float]]:
    &#34;&#34;&#34;Gathers experience samples from an environment using a reinforcement learning model.

    Parameters
    ----------
    env : `Any`
        The environment to collect experience from.
    model : `torch.nn.Module`
        The reinforcement learning model (e.g., a torch neural network).
    gamma : float, optional
        Discount factor for future rewards, default=0.99.
    lambda_parameter : float, optional
        TD error correction parameter, default=0.95.
    penalty_for_done_state : float, optional
        Penalty applied to the state when the environment reaches a terminal state, default=-1.0.

    Returns
    -------
    trajectory : `list[tuple]`
        The return trajectory (state, selected_option, action, reward, next_state, terminate, done).
    returns : `list[float]`
        The list of cumulative returns.
    advantages : `list[float]`
        The list of advantage terms for each step.
    old_probs : `list[float]`
        The list of old policy probabilities for each step.
    &#34;&#34;&#34;
    device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
    model = model.to(device)
    state = env.reset()
    done = False
    trajectory = []
    old_probs = []
    tolerance_count = 0

    while not done and tolerance_count &lt; tolerance:
        state = state[0] if isinstance(state, tuple) else state
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)

        option_probs, action_probs, termination_probs, selected_option, action = model(state_tensor)

        action = torch.multinomial(action_probs, 1).item()
        option = torch.multinomial(option_probs, 1).item()
        old_probs.append(action_probs[0, action].item())

        terminate = torch.bernoulli(termination_probs).item() &gt; 0.5
        signature = env.step.__code__
        if signature.co_argcount &gt; 2:
            next_state, reward, done, truncated, info = env.step(action, option)
        else:
            next_state, reward, done, truncated, info = env.step(action)

        if done:
            reward = penalty_for_done_state
        tolerance_count += 1
        trajectory.append((state, selected_option, action, reward, next_state, terminate, done))
        state = next_state
        if verbose:
            print_trajectory_info(
                state, selected_option, action, reward, next_state, terminate, done
            )

    returns = []
    advantages = []
    G = 0
    delta = 0

    for t in reversed(range(len(trajectory))):
        state, selected_option, action, reward, next_state, terminate, done = trajectory[t]

        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        _, action_probs, _, _, _ = model(state_tensor)

        if t == len(trajectory) - 1:
            G = reward
            advantages.insert(0, G - action_probs[0, action].item())
        else:
            next_state_tensor = (
                torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)
            )
            _, next_action_probs, _, _, _ = model(next_state_tensor)

            delta = (
                reward
                + gamma * next_action_probs[0, action].item()
                - action_probs[0, action].item()
            )
            G = reward + gamma * G
            advantages.insert(
                0, delta + gamma * lambda_parameter * advantages[0] if advantages else delta
            )

        returns.insert(0, G)

    return trajectory, returns, advantages, old_probs</code></pre>
</details>
<div class="desc"><p>Gathers experience samples from an environment using a reinforcement learning model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>Any</code></dt>
<dd>The environment to collect experience from.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>The reinforcement learning model (e.g., a torch neural network).</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Discount factor for future rewards, default=0.99.</dd>
<dt><strong><code>lambda_parameter</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>TD error correction parameter, default=0.95.</dd>
<dt><strong><code>penalty_for_done_state</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Penalty applied to the state when the environment reaches a terminal state, default=-1.0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>trajectory</code></strong> :&ensp;<code>list[tuple]</code></dt>
<dd>The return trajectory (state, selected_option, action, reward, next_state, terminate, done).</dd>
<dt><strong><code>returns</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>The list of cumulative returns.</dd>
<dt><strong><code>advantages</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>The list of advantage terms for each step.</dd>
<dt><strong><code>old_probs</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>The list of old policy probabilities for each step.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.models_tools.graph_metrics"><code class="name flex">
<span>def <span class="ident">graph_metrics</span></span>(<span>adj_matrix: numpy.ndarray, eigenvector_threshold: float = 1e-06) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def graph_metrics(adj_matrix: np.ndarray, eigenvector_threshold: float = 1e-6) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Calculate various graph metrics based on the given adjacency matrix and return them in a single DataFrame.

    Parameters
    ----------
    adj_matrix : `np.ndarray`
        The adjacency matrix representing the graph, where each element denotes the presence/weight of an edge between nodes.
    eigenvector_threshold : `float`
        A threshold for the eigenvector centrality calculation, used to determine the cutoff for small eigenvalues. Default is `1e-6`.

    Returns
    -------
    metrics_df : pd.DataFrame
        A DataFrame containing the following graph metrics as columns.
        - `Degree`: The degree of each node, representing the number of edges connected to each node.
        - `DegreeCentrality`: Degree centrality values for each node, indicating the number of direct connections each node has.
        - `ClusteringCoefficient`: Clustering coefficient values for each node, representing the degree to which nodes cluster together.
        - `EigenvectorCentrality`: Eigenvector centrality values, indicating the influence of a node in the graph based on the eigenvectors of the adjacency matrix.
        - `BetweennessCentrality`: Betweenness centrality values, representing the extent to which a node lies on the shortest paths between other nodes.
        - `ClosenessCentrality`: Closeness centrality values, indicating the inverse of the average shortest path distance from a node to all other nodes in the graph.
        - `Assortativity`: The assortativity coefficient of the graph, measuring the tendency of nodes to connect to similar nodes.

    Notes
    -----
    The returned DataFrame will have one row for each node and one column for each of the computed metrics.
    &#34;&#34;&#34;
    adj_matrix = adj_matrix.astype(int)
    G = nx.from_numpy_array(adj_matrix)
    degree_centrality = nx.degree_centrality(G)
    clustering_coeff = nx.clustering(G)
    try:
        eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=500)
    except nx.PowerIterationFailedConvergence:
        print(&#34;Power iteration failed to converge. Returning NaN for eigenvector centrality.&#34;)
        eigenvector_centrality = {node: float(&#34;nan&#34;) for node in G.nodes()}

    for node, centrality in eigenvector_centrality.items():
        if centrality &lt; eigenvector_threshold:
            eigenvector_centrality[node] = 0.0
    degree = dict(G.degree())
    betweenness_centrality = nx.betweenness_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    assortativity = nx.degree_assortativity_coefficient(G)
    metrics_df = pd.DataFrame(
        {
            &#34;Degree&#34;: degree,
            &#34;DegreeCentrality&#34;: degree_centrality,
            &#34;ClusteringCoefficient&#34;: clustering_coeff,
            &#34;EigenvectorCentrality&#34;: eigenvector_centrality,
            &#34;BetweennessCentrality&#34;: betweenness_centrality,
            &#34;ClosenessCentrality&#34;: closeness_centrality,
        }
    )
    metrics_df[&#34;Assortativity&#34;] = assortativity

    return metrics_df</code></pre>
</details>
<div class="desc"><p>Calculate various graph metrics based on the given adjacency matrix and return them in a single DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>adj_matrix</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The adjacency matrix representing the graph, where each element denotes the presence/weight of an edge between nodes.</dd>
<dt><strong><code>eigenvector_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>A threshold for the eigenvector centrality calculation, used to determine the cutoff for small eigenvalues. Default is <code>1e-6</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>metrics_df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the following graph metrics as columns.
- <code>Degree</code>: The degree of each node, representing the number of edges connected to each node.
- <code>DegreeCentrality</code>: Degree centrality values for each node, indicating the number of direct connections each node has.
- <code>ClusteringCoefficient</code>: Clustering coefficient values for each node, representing the degree to which nodes cluster together.
- <code>EigenvectorCentrality</code>: Eigenvector centrality values, indicating the influence of a node in the graph based on the eigenvectors of the adjacency matrix.
- <code>BetweennessCentrality</code>: Betweenness centrality values, representing the extent to which a node lies on the shortest paths between other nodes.
- <code>ClosenessCentrality</code>: Closeness centrality values, indicating the inverse of the average shortest path distance from a node to all other nodes in the graph.
- <code>Assortativity</code>: The assortativity coefficient of the graph, measuring the tendency of nodes to connect to similar nodes.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The returned DataFrame will have one row for each node and one column for each of the computed metrics.</p></div>
</dd>
<dt id="likelihood.tools.models_tools.ppo_loss"><code class="name flex">
<span>def <span class="ident">ppo_loss</span></span>(<span>advantages: torch.Tensor,<br>old_action_probs: torch.Tensor,<br>action_probs: torch.Tensor,<br>epsilon: float = 0.2)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppo_loss(
    advantages: torch.Tensor,
    old_action_probs: torch.Tensor,
    action_probs: torch.Tensor,
    epsilon: float = 0.2,
):
    &#34;&#34;&#34;Computes the Proximal Policy Optimization (PPO) loss using the clipped objective.

    Parameters
    ----------
    advantages : `torch.Tensor`
        The advantages (delta) for each action taken, calculated as the difference between returns and value predictions.
    old_action_probs : `torch.Tensor`
        The action probabilities from the previous policy (before the current update).
    action_probs : `torch.Tensor`
        The action probabilities from the current policy (after the update).
    epsilon : `float`, optional, default=0.2
        The clipping parameter that limits how much the policy can change between updates.

    Returns
    -------
    loss : `torch.Tensor`
        The PPO loss, averaged across the batch of samples. The loss is computed using the clipped objective to penalize large policy updates.
    &#34;&#34;&#34;

    if advantages.dim() == 1:
        advantages = advantages.unsqueeze(-1)

    log_ratio = torch.log(action_probs + 1e-8) - torch.log(old_action_probs + 1e-8)
    ratio = torch.exp(log_ratio)  # π(a|s) / π_old(a|s)

    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)

    loss = -torch.min(ratio * advantages, clipped_ratio * advantages)

    return loss.mean()</code></pre>
</details>
<div class="desc"><p>Computes the Proximal Policy Optimization (PPO) loss using the clipped objective.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>advantages</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The advantages (delta) for each action taken, calculated as the difference between returns and value predictions.</dd>
<dt><strong><code>old_action_probs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The action probabilities from the previous policy (before the current update).</dd>
<dt><strong><code>action_probs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The action probabilities from the current policy (after the update).</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional, default=<code>0.2</code></dt>
<dd>The clipping parameter that limits how much the policy can change between updates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The PPO loss, averaged across the batch of samples. The loss is computed using the clipped objective to penalize large policy updates.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.models_tools.print_trajectory_info"><code class="name flex">
<span>def <span class="ident">print_trajectory_info</span></span>(<span>state, selected_option, action, reward, next_state, terminate, done)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_trajectory_info(state, selected_option, action, reward, next_state, terminate, done):
    print(&#34;=&#34; * 50)
    print(&#34;TRAJECTORY INFO&#34;.center(50, &#34;=&#34;))
    print(&#34;=&#34; * 50)
    print(f&#34;State: {state}&#34;)
    print(f&#34;Selected Option: {selected_option}&#34;)
    print(f&#34;Action: {action}&#34;)
    print(f&#34;Reward: {reward}&#34;)
    print(f&#34;Next State: {next_state}&#34;)
    print(f&#34;Terminate: {terminate}&#34;)
    print(f&#34;Done: {done}&#34;)
    print(&#34;=&#34; * 50)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.models_tools.remove_collinearity"><code class="name flex">
<span>def <span class="ident">remove_collinearity</span></span>(<span>df: pandas.core.frame.DataFrame, threshold: float = 0.9)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_collinearity(df: pd.DataFrame, threshold: float = 0.9):
    &#34;&#34;&#34;
    Removes highly collinear features from the DataFrame based on a correlation threshold.

    This function calculates the correlation matrix of the DataFrame and removes columns
    that are highly correlated with any other column in the DataFrame. It uses an absolute
    correlation value greater than the specified threshold to identify which columns to drop.

    Parameters
    ----------
    df : `pd.DataFrame`
        The input DataFrame containing numerical data.
    threshold : `float`
        The correlation threshold above which features will be removed. Default is `0.9`.

    Returns
    -------
    df_reduced : `pd.DataFrame`
        A DataFrame with highly collinear features removed.
    &#34;&#34;&#34;
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [
        column for column in upper_triangle.columns if any(upper_triangle[column] &gt; threshold)
    ]
    df_reduced = df.drop(columns=to_drop)

    return df_reduced</code></pre>
</details>
<div class="desc"><p>Removes highly collinear features from the DataFrame based on a correlation threshold.</p>
<p>This function calculates the correlation matrix of the DataFrame and removes columns
that are highly correlated with any other column in the DataFrame. It uses an absolute
correlation value greater than the specified threshold to identify which columns to drop.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The input DataFrame containing numerical data.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>The correlation threshold above which features will be removed. Default is <code>0.9</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_reduced</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>A DataFrame with highly collinear features removed.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.models_tools.suppress_warnings"><code class="name flex">
<span>def <span class="ident">suppress_warnings</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suppress_warnings(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        with warnings.catch_warnings():
            warnings.simplefilter(&#34;ignore&#34;)
            return func(*args, **kwargs)

    return wrapper</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.models_tools.train_and_insights"><code class="name flex">
<span>def <span class="ident">train_and_insights</span></span>(<span>x_data: numpy.ndarray,<br>y_act: numpy.ndarray,<br>model: keras.src.models.model.Model,<br>patience: int = 3,<br>reg: bool = False,<br>frac: float = 1.0,<br>**kwargs: Dict | None) ‑> keras.src.models.model.Model</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_and_insights(
    x_data: np.ndarray,
    y_act: np.ndarray,
    model: tf.keras.Model,
    patience: int = 3,
    reg: bool = False,
    frac: float = 1.0,
    **kwargs: Optional[Dict],
) -&gt; tf.keras.Model:
    &#34;&#34;&#34;
    Train a Keras model and provide insights on the training and validation metrics.

    Parameters
    ----------
    x_data : `np.ndarray`
        Input data for training the model.
    y_act : `np.ndarray`
        Actual labels corresponding to x_data.
    model : `tf.keras.Model`
        The Keras model to train.
    patience : `int`
        The patience parameter for early stopping callback (default is 3).
    reg : `bool`
        Flag to determine if residual analysis should be performed (default is `False`).
    frac : `float`
        Fraction of data to use (default is 1.0).

    Keyword Arguments
    -----------------
    Additional keyword arguments passed to the `model.fit` function, such as validation split and callbacks.

    Returns
    -------
    `tf.keras.Model`
        The trained model after fitting.
    &#34;&#34;&#34;
    validation_split = kwargs.get(&#34;validation_split&#34;, 0.2)
    callback = kwargs.get(
        &#34;callback&#34;, [tf.keras.callbacks.EarlyStopping(monitor=&#34;val_loss&#34;, patience=patience)]
    )

    for key in [&#34;validation_split&#34;, &#34;callback&#34;]:
        if key in kwargs:
            del kwargs[key]

    history = model.fit(
        x_data,
        y_act,
        validation_split=validation_split,
        verbose=False,
        callbacks=callback,
        **kwargs,
    )

    hist = pd.DataFrame(history.history)
    hist[&#34;epoch&#34;] = history.epoch

    columns = hist.columns
    train_err, train_metric = columns[0], columns[1]
    val_err, val_metric = columns[2], columns[3]
    train_err, val_err = hist[train_err].values, hist[val_err].values

    with suppress_prints():
        n = int(len(x_data) * frac)
        y_pred = model.predict(x_data[:n])
        y_act = y_act[:n]

    if reg:
        residual(y_act, y_pred)
        residual_hist(y_act, y_pred)
        act_pred(y_act, y_pred)

    loss_curve(hist[&#34;epoch&#34;].values, train_err, val_err)

    return model</code></pre>
</details>
<div class="desc"><p>Train a Keras model and provide insights on the training and validation metrics.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Input data for training the model.</dd>
<dt><strong><code>y_act</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Actual labels corresponding to x_data.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>tf.keras.Model</code></dt>
<dd>The Keras model to train.</dd>
<dt><strong><code>patience</code></strong> :&ensp;<code>int</code></dt>
<dd>The patience parameter for early stopping callback (default is 3).</dd>
<dt><strong><code>reg</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to determine if residual analysis should be performed (default is <code>False</code>).</dd>
<dt><strong><code>frac</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of data to use (default is 1.0).</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments passed to the <code>model.fit</code> function, such as validation split and callbacks.</p>
<h2 id="returns">Returns</h2>
<p><code>tf.keras.Model</code>
The trained model after fitting.</p></div>
</dd>
<dt id="likelihood.tools.models_tools.train_model_with_episodes"><code class="name flex">
<span>def <span class="ident">train_model_with_episodes</span></span>(<span>model: torch.nn.modules.module.Module,<br>optimizer: torch.optim.optimizer.Optimizer,<br>env: Any,<br>num_episodes: int,<br>episode_patience: int = 5,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_model_with_episodes(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    env: Any,
    num_episodes: int,
    episode_patience: int = 5,
    **kwargs,
):
    &#34;&#34;&#34;Trains a model via reinforcement learning episodes.

    Parameters
    ----------
    model : `torch.nn.Module`
        The model to be trained.
    optimizer : `torch.optim.Optimizer`
        The optimizer for the model.
    env : `Any`
        The environment used for the episodes.
    num_episodes : `int`
        The number of episodes to train.

    Keyword Arguments
    -----------------
    Additional keyword arguments passed to the `train_option_critic` function.

    num_epochs : `int`
        Number of training epochs.
    batch_size : `int`
        Batch size per training step.
    gamma : `float`
        Discount factor for future rewards.
    device : `str`
        Target device (e.g., &#34;cpu&#34; or &#34;cuda&#34;).
    beta : `float`
        Critic learning rate hyperparameter.
    patience : `int`
        Early stopping patience in epochs.

    Returns
    -------
    model : `torch.nn.Module`
        The trained model.
    best_loss_so_far : `float`
        The best loss value observed during training.
    &#34;&#34;&#34;
    previous_weights = model.state_dict()
    best_loss_so_far = float(&#34;inf&#34;)
    loss_window = []
    average_loss = 0.0
    no_improvement_count = 0

    print(f&#34;{&#39;Episode&#39;:&lt;12} {&#39;Loss&#39;:&lt;8} {&#39;Best Loss&#39;:&lt;17} {&#39;Status&#39;:&lt;15} {&#39;Avg Loss&#39;:&lt;4}&#34;)
    print(&#34;=&#34; * 70)

    NEW_BEST_COLOR = &#34;\033[92m&#34;
    REVERT_COLOR = &#34;\033[91m&#34;
    RESET_COLOR = &#34;\033[0m&#34;
    advantages_per_episode = []

    for episode in range(num_episodes):
        model, loss, advantages = train_option_critic(model, optimizer, env, **kwargs)
        advantages_per_episode.extend(advantages)

        loss_window.append(loss)
        average_loss = sum(loss_window) / len(loss_window)

        if loss &lt; best_loss_so_far:
            best_loss_so_far = loss
            previous_weights = model.state_dict()
            no_improvement_count = 0
            status = f&#34;{NEW_BEST_COLOR}Updated{RESET_COLOR}&#34;
        else:
            model.load_state_dict(previous_weights)
            no_improvement_count += 1
            status = f&#34;{REVERT_COLOR}No Improvement{RESET_COLOR}&#34;
        print(
            f&#34;{episode + 1:&lt;8} {loss:&lt;12.4f} {best_loss_so_far:&lt;15.4f} {status:&lt;25} {average_loss:&lt;12.4f}&#34;
        )
        print(&#34;=&#34; * 70)

        if no_improvement_count &gt;= episode_patience:
            print(f&#34;\nNo improvement for {episode_patience} episodes. Stopping early.&#34;)
            break

    print(f&#34;\nTraining complete. Final best loss: {best_loss_so_far:.4f}&#34;)
    sns.set_theme(style=&#34;whitegrid&#34;)
    plt.figure(figsize=(5, 3))
    plt.plot(
        range(len(advantages_per_episode)),
        advantages_per_episode,
        marker=None,
        markersize=6,
        color=sns.color_palette(&#34;deep&#34;)[0],
        linestyle=&#34;-&#34;,
        linewidth=2,
    )
    plt.xscale(&#34;log&#34;)

    plt.xlabel(&#34;Epoch&#34;, fontsize=12)
    plt.ylabel(&#34;Average Advantages&#34;, fontsize=12)
    plt.grid(True, which=&#34;both&#34;, axis=&#34;both&#34;, linestyle=&#34;--&#34;, linewidth=0.5, alpha=0.6)
    plt.tight_layout()
    plt.show()

    return model, best_loss_so_far</code></pre>
</details>
<div class="desc"><p>Trains a model via reinforcement learning episodes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>The model to be trained.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>The optimizer for the model.</dd>
<dt><strong><code>env</code></strong> :&ensp;<code>Any</code></dt>
<dd>The environment used for the episodes.</dd>
<dt><strong><code>num_episodes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of episodes to train.</dd>
</dl>
<h2 id="keyword-arguments">Keyword Arguments</h2>
<p>Additional keyword arguments passed to the <code><a title="likelihood.tools.models_tools.train_option_critic" href="#likelihood.tools.models_tools.train_option_critic">train_option_critic()</a></code> function.</p>
<p>num_epochs : <code>int</code>
Number of training epochs.
batch_size : <code>int</code>
Batch size per training step.
gamma : <code>float</code>
Discount factor for future rewards.
device : <code>str</code>
Target device (e.g., "cpu" or "cuda").
beta : <code>float</code>
Critic learning rate hyperparameter.
patience : <code>int</code>
Early stopping patience in epochs.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>The trained model.</dd>
<dt><strong><code>best_loss_so_far</code></strong> :&ensp;<code>float</code></dt>
<dd>The best loss value observed during training.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.models_tools.train_option_critic"><code class="name flex">
<span>def <span class="ident">train_option_critic</span></span>(<span>model: torch.nn.modules.module.Module,<br>optimizer: torch.optim.optimizer.Optimizer,<br>env: Any,<br>num_epochs: int = 1000,<br>batch_size: int = 32,<br>device: str = 'cpu',<br>beta: float = 0.01,<br>epsilon: float = 0.2,<br>patience: int = 15,<br>verbose: bool = False,<br>**kwargs) ‑> tuple[torch.nn.modules.module.Module, float]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_option_critic(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    env: Any,
    num_epochs: int = 1_000,
    batch_size: int = 32,
    device: str = &#34;cpu&#34;,
    beta: float = 1e-2,
    epsilon: float = 0.2,
    patience: int = 15,
    verbose: bool = False,
    **kwargs,
) -&gt; tuple[torch.nn.Module, float]:
    &#34;&#34;&#34;Trains an option critic model with the provided environment and hyperparameters.

    Parameters
    ----------
    model : `nn.Module`
        The neural network model to train.
    optimizer : `torch.optim.Optimizer`
        The optimizer for model updates.
    env : `Any`
        The environment for training.
    num_epochs : `int`
        Number of training epochs.
    batch_size : `int`
        Batch size per training step.
    device : `str`
        Target device (e.g., &#34;cpu&#34; or &#34;cuda&#34;).
    beta : `float`
        Critic learning rate hyperparameter.
    epsilon : `float`, optional, default=0.2
        The clipping parameter that limits how much the policy can change between updates.
    patience : `int`
        Early stopping patience in epochs.

    Returns
    -------
    model : `nn.Module`
        Trained model.
    avg_epoch_loss : `float`
        Average loss per epoch over training.
    &#34;&#34;&#34;
    losses = []
    best_loss_so_far = float(&#34;inf&#34;)
    best_advantage_so_far = 0.0
    patience_counter = 0
    patience_counter_advantage = 0
    device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
    model = model.to(device)
    advantages_per_epoch = []

    for epoch in range(num_epochs):
        trajectory, returns, advantages, old_probs = collect_experience(env, model, **kwargs)
        avg_advantage = sum(advantages) / len(advantages)
        advantages_per_epoch.append(avg_advantage)

        states = torch.tensor(np.array([t[0] for t in trajectory]), dtype=torch.float32).to(device)
        actions = torch.tensor([t[2] for t in trajectory], dtype=torch.long).to(device)
        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)
        advantages_tensor = torch.tensor(advantages, dtype=torch.float32).to(device)
        old_probs_tensor = torch.tensor(old_probs, dtype=torch.float32).view(-1, 1).to(device)

        dataset = TensorDataset(
            states, actions, returns_tensor, advantages_tensor, old_probs_tensor
        )
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        epoch_loss = 0
        num_batches = 0

        for (
            batch_states,
            batch_actions,
            batch_returns,
            batch_advantages,
            batch_old_probs,
        ) in dataloader:
            optimizer.zero_grad()

            option_probs, action_probs, termination_probs, selected_option, action = model(
                batch_states
            )

            batch_current_probs = action_probs.gather(1, batch_actions.unsqueeze(1))
            ppo_loss_value = ppo_loss(
                batch_advantages, batch_old_probs, batch_current_probs, epsilon=epsilon
            )

            entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)

            loss = ppo_loss_value + beta * entropy.mean()
            avg_advantages = batch_advantages.mean().item()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1

            if avg_advantages &gt; best_advantage_so_far:
                best_advantage_so_far = avg_advantages
                patience_counter_advantage = 0
            else:
                patience_counter_advantage += 1
            if patience_counter_advantage &gt;= patience:
                if verbose:
                    print(
                        f&#34;Early stopping at epoch {epoch} after {patience} epochs without advantage improvement.&#34;
                    )
                break

            epoch_loss += loss.item()
            num_batches += 1

        if num_batches &gt; 0:
            avg_epoch_loss = epoch_loss / num_batches
            losses.append(avg_epoch_loss)

            if avg_epoch_loss &lt; best_loss_so_far:
                best_loss_so_far = avg_epoch_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter &gt;= patience:
                if verbose:
                    print(
                        f&#34;Early stopping at epoch {epoch} after {patience} epochs without improvement.&#34;
                    )
                break

            if verbose:
                if epoch % (num_epochs // 10) == 0:
                    print(f&#34;Epoch {epoch}/{num_epochs} - Avg Loss: {avg_epoch_loss:.4f}&#34;)

    return model, avg_epoch_loss, advantages_per_epoch</code></pre>
</details>
<div class="desc"><p>Trains an option critic model with the provided environment and hyperparameters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>The neural network model to train.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>The optimizer for model updates.</dd>
<dt><strong><code>env</code></strong> :&ensp;<code>Any</code></dt>
<dd>The environment for training.</dd>
<dt><strong><code>num_epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of training epochs.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size per training step.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>Target device (e.g., "cpu" or "cuda").</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>float</code></dt>
<dd>Critic learning rate hyperparameter.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional, default=<code>0.2</code></dt>
<dd>The clipping parameter that limits how much the policy can change between updates.</dd>
<dt><strong><code>patience</code></strong> :&ensp;<code>int</code></dt>
<dd>Early stopping patience in epochs.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>Trained model.</dd>
<dt><strong><code>avg_epoch_loss</code></strong> :&ensp;<code>float</code></dt>
<dd>Average loss per epoch over training.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.tools.models_tools.LoRALayer"><code class="flex name class">
<span>class <span class="ident">LoRALayer</span></span>
<span>(</span><span>units, rank=4, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.keras.utils.register_keras_serializable(package=&#34;Custom&#34;, name=&#34;LoRALayer&#34;)
class LoRALayer(tf.keras.layers.Layer):
    def __init__(self, units, rank=4, **kwargs):
        super(LoRALayer, self).__init__(**kwargs)
        self.units = units
        self.rank = rank

    def build(self, input_shape):
        input_dim = input_shape[-1]
        print(f&#34;Input shape: {input_shape}&#34;)

        if self.rank &gt; input_dim:
            raise ValueError(
                f&#34;Rank ({self.rank}) cannot be greater than input dimension ({input_dim}).&#34;
            )
        if self.rank &gt; self.units:
            raise ValueError(
                f&#34;Rank ({self.rank}) cannot be greater than number of units ({self.units}).&#34;
            )

        self.A = self.add_weight(
            shape=(input_dim, self.rank), initializer=&#34;random_normal&#34;, trainable=True, name=&#34;A&#34;
        )
        self.B = self.add_weight(
            shape=(self.rank, self.units), initializer=&#34;random_normal&#34;, trainable=True, name=&#34;B&#34;
        )
        print(f&#34;Dense weights shape: {input_dim}x{self.units}&#34;)
        print(f&#34;LoRA weights shape: A{self.A.shape}, B{self.B.shape}&#34;)

    def call(self, inputs):
        lora_output = tf.matmul(tf.matmul(inputs, self.A), self.B)
        return lora_output</code></pre>
</details>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables). State can be
created:</p>
<ul>
<li>in <code>__init__()</code>, for instance via <code>self.add_weight()</code>;</li>
<li>in the optional <code>build()</code> method, which is invoked by the first
<code>__call__()</code> to the layer, and supplies the shape(s) of the input(s),
which may not have been known at initialization time.</li>
</ul>
<p>Layers are recursively composable: If you assign a Layer instance as an
attribute of another Layer, the outer layer will start tracking the weights
created by the inner layer. Nested layers should be instantiated in the
<code>__init__()</code> method or <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainable</code></strong></dt>
<dd>Boolean, whether the layer's variables should be trainable.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String name of the layer.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. Can also be a
<code>keras.DTypePolicy</code>, which allows the computation and weight dtype
to differ. Defaults to <code>None</code>. <code>None</code> means to use
<code>keras.config.dtype_policy()</code>, which is a <code>float32</code> policy unless
set to different value (via <code>keras.config.set_dtype_policy()</code>).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Dtype of the layer's weights. Alias of <code>layer.variable_dtype</code>.</dd>
<dt><strong><code>variable_dtype</code></strong></dt>
<dd>Dtype of the layer's weights.</dd>
<dt><strong><code>compute_dtype</code></strong></dt>
<dd>The dtype of the layer's computations.
Layers automatically cast inputs to this dtype, which causes
the computations and output to also be in this dtype.
When mixed precision is used with a
<code>keras.DTypePolicy</code>, this will be different
than <code>variable_dtype</code>.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e.
whether its potentially-trainable weights should be returned
as part of <code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer weights
that do not depend on input shapes, using <code>add_weight()</code>,
or other state.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>, or other
state. <code>__call__()</code> will automatically build the layer
(if it has not been built yet) by calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying
the layer to the input arguments.
Two reserved keyword arguments you can optionally use in <code>call()</code> are:
1. <code>training</code> (boolean, whether the call is in inference mode or
training mode).
2. <code>mask</code> (boolean tensor encoding masked timesteps in the input,
used e.g. in RNN layers).
A typical signature for this method is <code>call(self, inputs)</code>, and user
could optionally add <code>training</code> and <code>mask</code> if the layer need them.</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration
used to initialize this layer. If the keys differ from the arguments
in <code>__init__()</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):
    def __init__(self, units=32):
        super().__init__()
        self.units = units

    # Create the state of the layer (weights)
    def build(self, input_shape):
        self.kernel = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer=&quot;glorot_uniform&quot;,
            trainable=True,
            name=&quot;kernel&quot;,
        )
        self.bias = self.add_weight(
            shape=(self.units,),
            initializer=&quot;zeros&quot;,
            trainable=True,
            name=&quot;bias&quot;,
        )

    # Defines the computation
    def call(self, inputs):
        return ops.matmul(inputs, self.kernel) + self.bias

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(ops.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = self.add_weight(
        shape=(),
        initializer=&quot;zeros&quot;,
        trainable=False,
        name=&quot;total&quot;,
      )

  def call(self, inputs):
      self.total.assign(self.total + ops.sum(inputs))
      return self.total

my_sum = ComputeSum(2)
x = ops.ones((2, 2))
y = my_sum(x)

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.layers.layer.Layer</li>
<li>keras.src.backend.tensorflow.layer.TFLayer</li>
<li>keras.src.backend.tensorflow.trackable.KerasAutoTrackable</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.ops.operation.Operation</li>
<li>keras.src.saving.keras_saveable.KerasSaveable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.models_tools.LoRALayer.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    input_dim = input_shape[-1]
    print(f&#34;Input shape: {input_shape}&#34;)

    if self.rank &gt; input_dim:
        raise ValueError(
            f&#34;Rank ({self.rank}) cannot be greater than input dimension ({input_dim}).&#34;
        )
    if self.rank &gt; self.units:
        raise ValueError(
            f&#34;Rank ({self.rank}) cannot be greater than number of units ({self.units}).&#34;
        )

    self.A = self.add_weight(
        shape=(input_dim, self.rank), initializer=&#34;random_normal&#34;, trainable=True, name=&#34;A&#34;
    )
    self.B = self.add_weight(
        shape=(self.rank, self.units), initializer=&#34;random_normal&#34;, trainable=True, name=&#34;B&#34;
    )
    print(f&#34;Dense weights shape: {input_dim}x{self.units}&#34;)
    print(f&#34;LoRA weights shape: A{self.A.shape}, B{self.B.shape}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.models_tools.LoRALayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    lora_output = tf.matmul(tf.matmul(inputs, self.A), self.B)
    return lora_output</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.models_tools.TransformRange"><code class="flex name class">
<span>class <span class="ident">TransformRange</span></span>
<span>(</span><span>columns_bin_sizes: Dict[str, int])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformRange:
    &#34;&#34;&#34;
    Generates a new DataFrame with ranges represented as strings.

    Transforms numerical columns into categorical range bins with descriptive labels.
    &#34;&#34;&#34;

    def __init__(self, columns_bin_sizes: Dict[str, int]) -&gt; None:
        &#34;&#34;&#34;Initializes the class with the original DataFrame.

        Parameters
        ----------
        columns_bin_sizes : `dict`
            A dictionary where the keys are column names and the values are the bin sizes.

        Raises
        ------
        TypeError
            If df is not a pandas DataFrame.
        &#34;&#34;&#34;
        self.info = {}
        self.columns_bin_sizes = columns_bin_sizes

    def _create_bins_and_labels(
        self, min_val: Union[int, float], max_val: Union[int, float], bin_size: int
    ) -&gt; Tuple[np.ndarray, List[str]]:
        &#34;&#34;&#34;
        Creates the bin edges and their labels.

        Parameters
        ----------
        min_val : `int` or `float`
            The minimum value for the range.
        max_val : `int` or `float`
            The maximum value for the range.
        bin_size : `int`
            The size of each bin.

        Returns
        -------
        bins : `np.ndarray`
            The bin edges.
        labels : `list`
            The labels for the bins.

        Raises
        ------
        ValueError
            If bin_size is not positive or if min_val &gt;= max_val.
        &#34;&#34;&#34;
        if bin_size &lt;= 0:
            raise ValueError(&#34;bin_size must be positive&#34;)
        if min_val &gt;= max_val:
            raise ValueError(&#34;min_val must be less than max_val&#34;)

        start = int(min_val)
        end = int(max_val) + bin_size

        bins = np.arange(start, end + 1, bin_size)

        if bins[-1] &lt;= max_val:
            bins = np.append(bins, max_val + 1)

        lower_bin_edge = -np.inf
        upper_bin_edge = np.inf

        labels = [f&#34;{int(bins[i])}-{int(bins[i+1] - 1)}&#34; for i in range(len(bins) - 1)]
        end = int(bins[-1] - 1)
        bins = bins.tolist()
        bins.insert(0, lower_bin_edge)
        bins.append(upper_bin_edge)
        labels.insert(0, f&#34;&lt; {start}&#34;)
        labels.append(f&#34;&gt; {end}&#34;)
        return bins, labels

    def _transform_column_to_ranges(
        self, df: pd.DataFrame, column: str, bin_size: int, fit: bool = True
    ) -&gt; pd.Series:
        &#34;&#34;&#34;
        Transforms a column in the DataFrame into range bins.

        Parameters
        ----------
        df : `pd.DataFrame`
            The original DataFrame to transform.
        column : `str`
            The name of the column to transform.
        bin_size : `int`
            The size of each bin.

        Returns
        -------
        `pd.Series`
            A Series with the range labels.

        Raises
        ------
        KeyError
            If column is not found in the DataFrame.
        ValueError
            If bin_size is not positive or if column contains non-numeric data.
        &#34;&#34;&#34;
        if not isinstance(df, pd.DataFrame):
            raise TypeError(&#34;df must be a pandas DataFrame&#34;)
        df_ = df.copy()  # Create a copy to avoid modifying the original
        numeric_series = pd.to_numeric(df_[column], errors=&#34;coerce&#34;)
        if fit:
            self.df = df_.copy()
            if column not in df_.columns:
                raise KeyError(f&#34;Column &#39;{column}&#39; not found in DataFrame&#34;)

            if bin_size &lt;= 0:
                raise ValueError(&#34;bin_size must be positive&#34;)

            if numeric_series.isna().all():
                raise ValueError(f&#34;Column &#39;{column}&#39; contains no valid numeric data&#34;)

            min_val = numeric_series.min()
            max_val = numeric_series.max()

            if min_val == max_val:
                return pd.Series(
                    [f&#34;{int(min_val)}-{int(max_val)}&#34;] * len(df_), name=f&#34;{column}_range&#34;
                )
            self.info[column] = {&#34;min_value&#34;: min_val, &#34;max_value&#34;: max_val, &#34;range&#34;: bin_size}
        else:
            min_val = self.info[column][&#34;min_value&#34;]
            max_val = self.info[column][&#34;max_value&#34;]
            bin_size = self.info[column][&#34;range&#34;]

        bins, labels = self._create_bins_and_labels(min_val, max_val, bin_size)
        return pd.cut(numeric_series, bins=bins, labels=labels, right=False, include_lowest=True)

    def transform(
        self, df: pd.DataFrame, drop_original: bool = False, fit: bool = True
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Creates a new DataFrame with range columns.

        Parameters
        ----------
        df : `pd.DataFrame`
            The original DataFrame to transform.
        drop_original : `bool`, optional
            If True, drops original columns from the result, by default False
        fit : `bool`, default=True
            Whether to compute bin edges based on the data (True) or use predefined binning (False).

        Returns
        -------
        `pd.DataFrame`
            A DataFrame with the transformed range columns.

        Raises
        ------
        TypeError
            If columns_bin_sizes is not a dictionary.
        &#34;&#34;&#34;
        if not isinstance(self.columns_bin_sizes, dict):
            raise TypeError(&#34;columns_bin_sizes must be a dictionary&#34;)

        if not self.columns_bin_sizes:
            return pd.DataFrame()

        range_columns = {}
        for column, bin_size in self.columns_bin_sizes.items():
            range_columns[f&#34;{column}_range&#34;] = self._transform_column_to_ranges(
                df, column, bin_size, fit
            )

        result_df = pd.DataFrame(range_columns)

        if not drop_original:
            original_cols = [col for col in df.columns if col not in self.columns_bin_sizes]
            if original_cols:
                result_df = pd.concat([df[original_cols], result_df], axis=1)

        return result_df

    def get_range_info(self, column: str) -&gt; Dict[str, Union[int, float, List[str]]]:
        &#34;&#34;&#34;
        Get information about the range transformation for a specific column.

        Parameters
        ----------
        column : `str`
            The name of the column to analyze.

        Returns
        -------
        `dict`
            Dictionary containing min_val, max_val, bin_size, and labels.
        &#34;&#34;&#34;
        if column not in self.df.columns:
            raise KeyError(f&#34;Column &#39;{column}&#39; not found in DataFrame&#34;)

        numeric_series = pd.to_numeric(self.df[column], errors=&#34;coerce&#34;)
        min_val = numeric_series.min()
        max_val = numeric_series.max()

        return {
            &#34;min_value&#34;: min_val,
            &#34;max_value&#34;: max_val,
            &#34;range&#34;: max_val - min_val,
            &#34;column&#34;: column,
        }</code></pre>
</details>
<div class="desc"><p>Generates a new DataFrame with ranges represented as strings.</p>
<p>Transforms numerical columns into categorical range bins with descriptive labels.</p>
<p>Initializes the class with the original DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>columns_bin_sizes</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary where the keys are column names and the values are the bin sizes.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If df is not a pandas DataFrame.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.models_tools.TransformRange.get_range_info"><code class="name flex">
<span>def <span class="ident">get_range_info</span></span>(<span>self, column: str) ‑> Dict[str, int | float | List[str]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_range_info(self, column: str) -&gt; Dict[str, Union[int, float, List[str]]]:
    &#34;&#34;&#34;
    Get information about the range transformation for a specific column.

    Parameters
    ----------
    column : `str`
        The name of the column to analyze.

    Returns
    -------
    `dict`
        Dictionary containing min_val, max_val, bin_size, and labels.
    &#34;&#34;&#34;
    if column not in self.df.columns:
        raise KeyError(f&#34;Column &#39;{column}&#39; not found in DataFrame&#34;)

    numeric_series = pd.to_numeric(self.df[column], errors=&#34;coerce&#34;)
    min_val = numeric_series.min()
    max_val = numeric_series.max()

    return {
        &#34;min_value&#34;: min_val,
        &#34;max_value&#34;: max_val,
        &#34;range&#34;: max_val - min_val,
        &#34;column&#34;: column,
    }</code></pre>
</details>
<div class="desc"><p>Get information about the range transformation for a specific column.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>column</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the column to analyze.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>dict</code>
Dictionary containing min_val, max_val, bin_size, and labels.</p></div>
</dd>
<dt id="likelihood.tools.models_tools.TransformRange.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self,<br>df: pandas.core.frame.DataFrame,<br>drop_original: bool = False,<br>fit: bool = True) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(
    self, df: pd.DataFrame, drop_original: bool = False, fit: bool = True
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Creates a new DataFrame with range columns.

    Parameters
    ----------
    df : `pd.DataFrame`
        The original DataFrame to transform.
    drop_original : `bool`, optional
        If True, drops original columns from the result, by default False
    fit : `bool`, default=True
        Whether to compute bin edges based on the data (True) or use predefined binning (False).

    Returns
    -------
    `pd.DataFrame`
        A DataFrame with the transformed range columns.

    Raises
    ------
    TypeError
        If columns_bin_sizes is not a dictionary.
    &#34;&#34;&#34;
    if not isinstance(self.columns_bin_sizes, dict):
        raise TypeError(&#34;columns_bin_sizes must be a dictionary&#34;)

    if not self.columns_bin_sizes:
        return pd.DataFrame()

    range_columns = {}
    for column, bin_size in self.columns_bin_sizes.items():
        range_columns[f&#34;{column}_range&#34;] = self._transform_column_to_ranges(
            df, column, bin_size, fit
        )

    result_df = pd.DataFrame(range_columns)

    if not drop_original:
        original_cols = [col for col in df.columns if col not in self.columns_bin_sizes]
        if original_cols:
            result_df = pd.concat([df[original_cols], result_df], axis=1)

    return result_df</code></pre>
</details>
<div class="desc"><p>Creates a new DataFrame with range columns.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The original DataFrame to transform.</dd>
<dt><strong><code>drop_original</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, drops original columns from the result, by default False</dd>
<dt><strong><code>fit</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to compute bin edges based on the data (True) or use predefined binning (False).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>pd.DataFrame</code>
A DataFrame with the transformed range columns.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If columns_bin_sizes is not a dictionary.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.models_tools.suppress_prints"><code class="flex name class">
<span>class <span class="ident">suppress_prints</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class suppress_prints:
    def __enter__(self):
        self.original_stdout = sys.stdout
        sys.stdout = open(os.devnull, &#34;w&#34;)

    def __exit__(self, exc_type, exc_value, traceback):
        sys.stdout.close()
        sys.stdout = self.original_stdout</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.tools" href="index.html">likelihood.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="likelihood.tools.models_tools.apply_lora" href="#likelihood.tools.models_tools.apply_lora">apply_lora</a></code></li>
<li><code><a title="likelihood.tools.models_tools.collect_experience" href="#likelihood.tools.models_tools.collect_experience">collect_experience</a></code></li>
<li><code><a title="likelihood.tools.models_tools.graph_metrics" href="#likelihood.tools.models_tools.graph_metrics">graph_metrics</a></code></li>
<li><code><a title="likelihood.tools.models_tools.ppo_loss" href="#likelihood.tools.models_tools.ppo_loss">ppo_loss</a></code></li>
<li><code><a title="likelihood.tools.models_tools.print_trajectory_info" href="#likelihood.tools.models_tools.print_trajectory_info">print_trajectory_info</a></code></li>
<li><code><a title="likelihood.tools.models_tools.remove_collinearity" href="#likelihood.tools.models_tools.remove_collinearity">remove_collinearity</a></code></li>
<li><code><a title="likelihood.tools.models_tools.suppress_warnings" href="#likelihood.tools.models_tools.suppress_warnings">suppress_warnings</a></code></li>
<li><code><a title="likelihood.tools.models_tools.train_and_insights" href="#likelihood.tools.models_tools.train_and_insights">train_and_insights</a></code></li>
<li><code><a title="likelihood.tools.models_tools.train_model_with_episodes" href="#likelihood.tools.models_tools.train_model_with_episodes">train_model_with_episodes</a></code></li>
<li><code><a title="likelihood.tools.models_tools.train_option_critic" href="#likelihood.tools.models_tools.train_option_critic">train_option_critic</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.tools.models_tools.LoRALayer" href="#likelihood.tools.models_tools.LoRALayer">LoRALayer</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.models_tools.LoRALayer.build" href="#likelihood.tools.models_tools.LoRALayer.build">build</a></code></li>
<li><code><a title="likelihood.tools.models_tools.LoRALayer.call" href="#likelihood.tools.models_tools.LoRALayer.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.models_tools.TransformRange" href="#likelihood.tools.models_tools.TransformRange">TransformRange</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.models_tools.TransformRange.get_range_info" href="#likelihood.tools.models_tools.TransformRange.get_range_info">get_range_info</a></code></li>
<li><code><a title="likelihood.tools.models_tools.TransformRange.transform" href="#likelihood.tools.models_tools.TransformRange.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.models_tools.suppress_prints" href="#likelihood.tools.models_tools.suppress_prints">suppress_prints</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
