<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>likelihood.tools.tools API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.tools.tools</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.tools.tools.cal_average"><code class="name flex">
<span>def <span class="ident">cal_average</span></span>(<span>y: numpy.ndarray, alpha: float = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the moving average of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>1</code>. By default it is set to <code>1</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>average</code></strong> :&ensp;<code>float</code></dt>
<dd>The average of the data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.cal_missing_values"><code class="name flex">
<span>def <span class="ident">cal_missing_values</span></span>(<span>df: pandas.core.frame.DataFrame) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the percentage of missing (<code>NaN</code>/<code>NaT</code>) values per column in a dataframe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The input dataframe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>None</code>
Prints out a table with columns as index and percentages of missing values as data.</p></div>
</dd>
<dt id="likelihood.tools.tools.calculate_probability"><code class="name flex">
<span>def <span class="ident">calculate_probability</span></span>(<span>x: numpy.ndarray, points: int = 1, cond: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the probability of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>points</code></strong> :&ensp;<code>int</code></dt>
<dd>An integer value. By default it is set to <code>1</code>.</dd>
<dt><strong><code>cond</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the probability of the data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>x: numpy.ndarray, poly: int = 9, inv: bool = False, plot: bool = False, savename: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the cumulative distribution function of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>poly</code></strong> :&ensp;<code>int</code></dt>
<dd>An integer value. By default it is set to <code>9</code>.</dd>
<dt><strong><code>inv</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cdf_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the cumulative distribution function.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.check_nan_inf"><code class="name flex">
<span>def <span class="ident">check_nan_inf</span></span>(<span>df: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Check for <code>NaN</code> and <code>Inf</code> values in the <code>DataFrame</code>. If any are found removes them.</p></div>
</dd>
<dt id="likelihood.tools.tools.difference_quotient"><code class="name flex">
<span>def <span class="ident">difference_quotient</span></span>(<span>f: Callable, x: float, h: float) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the difference quotient of <code>f</code> evaluated at <code>x</code> and <code>x + h</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x)</code> : <code>Callable</code></dt>
<dt>function.</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>Independent term.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>Step size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>(f(x + h) - f(x)) / h</code> : <code>float</code>
Difference quotient of <code>f</code> evaluated at <code>x</code>.</p></div>
</dd>
<dt id="likelihood.tools.tools.estimate_gradient"><code class="name flex">
<span>def <span class="ident">estimate_gradient</span></span>(<span>f: Callable, v: numpy.ndarray, h: float = 0.0001) ‑> List[numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the gradient of <code>f</code> at <code>v</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt>Function to differentiate.</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector` | `np.array</code></dt>
<dd>1D array representing vector <code>v=(x0,...,xi)</code>.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float`. By default it is set to `1e-4</code></dt>
<dd>The step size used to approximate the derivative.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>grad_f</code></strong> :&ensp;<code>List[np.array]</code></dt>
<dd>A list containing the estimated gradients of each component of <code>f</code> evaluated at <code>v</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.fft_denoise"><code class="name flex">
<span>def <span class="ident">fft_denoise</span></span>(<span>dataset: numpy.ndarray, sigma: float = 0, mode: bool = True) ‑> Tuple[numpy.ndarray, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the noise removal using the Fast Fourier Transform.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the noised data.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>1</code>. By default it is set to <code>0</code>.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean value. By default it is set to <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the denoised data.</dd>
<dt><strong><code>period</code></strong> :&ensp;<code>float</code></dt>
<dd>period of the function described by the dataset</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.generate_feature_yaml"><code class="name flex">
<span>def <span class="ident">generate_feature_yaml</span></span>(<span>df: pandas.core.frame.DataFrame, ignore_features: List[str] = None, yaml_string: bool = False) ‑> Union[Dict, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a YAML string containing information about ordinal, numeric, and categorical features
based on the given DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The DataFrame containing the data.</dd>
<dt><strong><code>ignore_features</code></strong> :&ensp;<code>List[<code>str</code>]</code></dt>
<dd>A list of features to ignore.</dd>
<dt><strong><code>yaml_string</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>True</code>, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>feature_info</code></strong> :&ensp;<code>Dict` | `str</code></dt>
<dd>A dictionary with four keys ('ordinal_features', 'numeric_features', 'categorical_features', 'ignore_features')
mapping to lists of feature names. Or a YAML formatted string if <code>yaml_string</code> is <code>True</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.generate_series"><code class="name flex">
<span>def <span class="ident">generate_series</span></span>(<span>n: int, n_steps: int, incline: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function that generates <code>n</code> series of length <code>n_steps</code></p></div>
</dd>
<dt id="likelihood.tools.tools.get_period"><code class="name flex">
<span>def <span class="ident">get_period</span></span>(<span>dataset: numpy.ndarray) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the periodicity of a <code>dataset</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the <code>dataset</code> describing the function over which the period is calculated</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>period</code></strong> :&ensp;<code>float</code></dt>
<dd>period of the function described by the <code>dataset</code></dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.mean_square_error"><code class="name flex">
<span>def <span class="ident">mean_square_error</span></span>(<span>y_true: numpy.ndarray, y_pred: numpy.ndarray, print_error: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the Root Mean Squared Error</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_true</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the true values.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the predicted values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>RMSE</code></strong> :&ensp;<code>float</code></dt>
<dd>The Root Mean Squared Error.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.minibatches"><code class="name flex">
<span>def <span class="ident">minibatches</span></span>(<span>dataset: List, batch_size: int, shuffle: bool = True) ‑> List</span>
</code></dt>
<dd>
<div class="desc"><p>Generates 'batch_size'-sized minibatches from the dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>List</code></dt>
<dd>The data to be divided into mini-batch.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Specifies the size of each mini-batch.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code></dt>
<dd>If set <code>True</code>, the data will be shuffled before dividing it into mini-batches.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>List[List]</code>
A list of lists containing the mini-batches. Each sublist is a separate mini-batch with length <code>batch_size</code>.</p></div>
</dd>
<dt id="likelihood.tools.tools.partial_difference_quotient"><code class="name flex">
<span>def <span class="ident">partial_difference_quotient</span></span>(<span>f: Callable, v: numpy.ndarray, i: int, h: float) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the partial difference quotient of <code>f</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt>Function to differentiate.</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector` | `np.array</code></dt>
<dd>1D array representing vector <code>v=(x0,...,xi)</code>.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>Step size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>(f(w) - f(v)) / h</code> : <code>np.array</code>
the <code>i-th</code> partial difference quotient of <code>f</code> at <code>v</code></p></div>
</dd>
<dt id="likelihood.tools.tools.sigmoide"><code class="name flex">
<span>def <span class="ident">sigmoide</span></span>(<span>x: float) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>The sigmoid function</p></div>
</dd>
<dt id="likelihood.tools.tools.sigmoide_inv"><code class="name flex">
<span>def <span class="ident">sigmoide_inv</span></span>(<span>y: float) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the inverse of the sigmoid function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>float</code></dt>
<dd>the number to evaluate the function</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>float</code>
value of evaluated function</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder"><code class="flex name class">
<span>class <span class="ident">DataFrameEncoder</span></span>
<span>(</span><span>data: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Allows encoding and decoding Dataframes</p>
<p>Sets the columns of the <code>DataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [
        &#34;_df&#34;,
        &#34;_names&#34;,
        &#34;_encode_columns&#34;,
        &#34;encoding_list&#34;,
        &#34;decoding_list&#34;,
        &#34;median_list&#34;,
    ]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []
        self.median_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        self.median_list = labelencoder[3]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if len(self._df[i].unique()) &gt; 1:
                    median_value = len(self._df[i].unique()) // 2
                else:
                    median_value = 1.0
                if norm_method == &#34;median&#34;:
                    self._df[i] = self._df[i].astype(&#34;float64&#34;)
                    self._df[i] = self._df[i] / median_value
                    self.median_list.append(median_value)
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            if len(self.median_list) == len(self._encode_columns):
                median_mode = True
            else:
                median_mode = False
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
                    if median_mode:
                        self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                        self._df[colname] = self._df[colname] / self.median_list[num]
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                    if median_mode:
                        df_decoded[i] = df_decoded[i] * self.median_list[j]
                        df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump(
                [self.encoding_list, self.decoding_list, self._encode_columns, self.median_list], f
            )

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decoding_list"><code class="name">var <span class="ident">decoding_list</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encoding_list"><code class="name">var <span class="ident">encoding_list</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.median_list"><code class="name">var <span class="ident">median_list</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Decodes the <code>int</code> type columns of the <code>DataFrame</code></p></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, path_to_save: str = './', **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Encodes the <code>object</code> type columns of the dataframe</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>save_mode (<code>bool</code>): An optional integer parameter. By default it is set to <code>True</code></li>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
<li>norm_method (<code>str</code>): An optional string parameter to perform normalization. By default it is set to <code>None</code></li>
</ul></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.get_dictionaries"><code class="name flex">
<span>def <span class="ident">get_dictionaries</span></span>(<span>self) ‑> Tuple[List[dict], List[dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Allows to return the <code>list</code> of dictionaries for <code>encoding</code> and <code>decoding</code></p></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.load_config"><code class="name flex">
<span>def <span class="ident">load_config</span></span>(<span>self, path_to_dictionaries: str = './', **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads dictionaries from a given directory</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
</ul></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, path_to_save: str, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the encoders and decoders using the <code>DataFrame</code></p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.DataScaler"><code class="flex name class">
<span>class <span class="ident">DataScaler</span></span>
<span>(</span><span>dataset: numpy.ndarray, n: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>numpy array <code>scaler</code> and <code>rescaler</code></p>
<p>Initializes the parameters required for scaling the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;]

    def __init__(self, dataset: ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self) -&gt; ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;

        mu = []
        sigma = []
        fitting = []
        self.data_scaled = np.copy(self.dataset_)
        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.data_scaled"><code class="name">var <span class="ident">data_scaled</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.dataset_"><code class="name">var <span class="ident">dataset_</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.transpose"><code class="name">var <span class="ident">transpose</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.values"><code class="name">var <span class="ident">values</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.rescale"><code class="name flex">
<span>def <span class="ident">rescale</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a standard rescaling of the data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_scaled</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.scale"><code class="name flex">
<span>def <span class="ident">scale</span></span>(<span>self, dataset_: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the inverse operation to the rescale function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the rescaled data.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection"><code class="flex name class">
<span>class <span class="ident">FeatureSelection</span></span>
<span>(</span><span>not_features: list[str] = [])</span>
</code></dt>
<dd>
<div class="desc"><p>Generate the data graph using a variation of the feature selection algorithm.</p>
<ul>
<li>The method <code>get_digraph</code> returns the network based on the feature selection method.</li>
</ul>
<p>The initializer of the class. The initial parameter is a list of strings with variables to discard.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        # Assign and clean dataset
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        # We construct string from causal_graph
        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
        numeric_scaled = self.scaler.rescale()
        numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
        curr_dataset[numeric_df.columns] = numeric_df

        # We construct dictionary to save index for scaling
        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        # Iterate over all the columns to obtain their importances.
        for index_column, column in enumerate(columns):

            # Variable to predict
            Y = curr_dataset[column]

            # We check whether it is numerical or categorical.
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                # Linear regression model
                Model = LinearRegression()

                # Auxiliary dataset without the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                # We train

                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                # We obtain importance
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()

                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)

                # Mapping to one-hot
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                # PASSING 0 -&gt; 0.5 and 1 -&gt; 0.73105
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5

                # Delete the column in question
                X_aux = curr_dataset.drop([column], axis=1)

                # We encode
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)

                # We train
                Model.fit(encoded_df.to_numpy().T, train_y)

                # We obtain importance
                importance = Model.get_importances()
                w = Model.w

            # We obtain the $n$ most important ones
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            # We build the string for the column in question
            names_cols = list(X_aux.columns)
            # We store the indices, values and column names in a list of tuples.
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]
            # We store w&#39;s for predictions

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            # Add to general list
            self.all_features_imp_graph.append((column, features_imp_node))
            # We format it
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):
        # Assign data and clean dataset of unneeded columns

        if len(self.not_features) &gt; 0:
            # We remove unnecessary columns
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="likelihood.graph.graph.DynamicGraph" href="../graph/graph.html#likelihood.graph.graph.DynamicGraph">DynamicGraph</a></li>
<li><a title="likelihood.models.simulation.SimulationEngine" href="../models/simulation.html#likelihood.models.simulation.SimulationEngine">SimulationEngine</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.all_features_imp_graph"><code class="name">var <span class="ident">all_features_imp_graph</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.not_features"><code class="name">var <span class="ident">not_features</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.scaler"><code class="name">var <span class="ident">scaler</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.w_dict"><code class="name">var <span class="ident">w_dict</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.get_digraph"><code class="name flex">
<span>def <span class="ident">get_digraph</span></span>(<span>self, dataset: pandas.core.frame.DataFrame, n_importances: int) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Get directed graph showing importance of features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Dataset to be used for generating the graph.</dd>
<dt><strong><code>n_importances</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top importances to show in the graph.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>str</code>
A string representation of the directed graph.</p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LinearRegression"><code class="flex name class">
<span>class <span class="ident">LinearRegression</span></span>
</code></dt>
<dd>
<div class="desc"><p>class implementing multiple linear regression</p>
<p>The class initializer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray, verbose: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression"><code class="flex name class">
<span>class <span class="ident">LogisticRegression</span></span>
</code></dt>
<dd>
<div class="desc"><p>class implementing multiple logistic regression</p>
<p>The class initializer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: ndarray, values: ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: ndarray) -&gt; ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.


        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder"><code class="flex name class">
<span>class <span class="ident">OneHotEncoder</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class used to encode categorical variables.
It receives an array of integers and returns a binary array using the one-hot encoding method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OneHotEncoder:
    &#34;&#34;&#34;
    Class used to encode categorical variables.
    It receives an array of integers and returns a binary array using the one-hot encoding method.
    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;]

    def __init__(self) -&gt; None:
        pass

    def encode(self, x: ndarray | list):
        self.x = x

        if not isinstance(self.x, ndarray):
            self.x = np.array(self.x)  # If not numpy array then convert it

        y = np.zeros(
            (self.x.size, self.x.max() + 1)
        )  # Build matrix of (size num of entries) x (max value + 1)

        y[np.arange(self.x.size), self.x] = 1  # Label with ones

        return y

    def decode(self, x: ndarray | list) -&gt; ndarray:
        if not isinstance(x, ndarray):
            x = np.array(x)  # If not numpy array then convert it

        # We return the max values of each row
        y = np.argmax(x, axis=1)

        return y</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, x: numpy.ndarray | list) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, x: numpy.ndarray | list)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures"><code class="flex name class">
<span>class <span class="ident">PerformanceMeasures</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class with methods to measure performance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerformanceMeasures:
    &#34;&#34;&#34;Class with methods to measure performance&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        pass

    # Performance measure Res_T
    def f_mean(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        n = len(labels)

        F_vec = self._f1_score(y_true, y_pred, labels=labels)
        a = np.sum(F_vec)

        for i in range(len(F_vec)):
            print(&#34;F-measure of label &#34;, labels[i], &#34; -&gt; &#34;, F_vec[i])

        print(&#34;Mean of F-measure -&gt; &#34;, a / n)

    # Performance measure Res_P
    def resp(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        # We initialize sum counters
        sum1 = 0
        sum2 = 0

        # Calculamos T_C
        T_C = len(y_true)
        for i in range(len(labels)):
            # We calculate instances of the classes and their F-measures
            sum1 += (1 - ((y_true == labels[i]).sum() / T_C)) * self._fi_measure(
                y_true, y_pred, labels, i
            )
            sum2 += 1 - ((y_true == labels[i]).sum()) / T_C

        # Print the metric corresponding to the prediction vector
        print(&#34;Metric Res_p -&gt;&#34;, sum1 / sum2)

    def _fi_measure(self, y_true: ndarray, y_pred: ndarray, labels: list, i: int) -&gt; int:
        F_vec = self._f1_score(y_true, y_pred, labels=labels)

        return F_vec[i]  # We return the position of the f1-score corresponding to the label

    # Summary of the labels predicted
    def _summary_pred(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; None:
        count_mat = self._confu_mat(y_true, y_pred, labels)
        print(&#34;        &#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;|--&#34;, labels[i], &#34;--&#34;, end=&#34;&#34;)
            if i + 1 == len(labels):
                print(&#34;|&#34;, end=&#34;&#34;)
        for i in range(len(labels)):
            print(&#34;&#34;)
            print(&#34;|--&#34;, labels[i], &#34;--|&#34;, end=&#34;&#34;)
            for j in range(len(labels)):
                if j != 0:
                    print(&#34; &#34;, end=&#34;&#34;)
                print(&#34;  &#34;, int(count_mat[i, j]), &#34;  &#34;, end=&#34;&#34;)

    def _f1_score(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        f1_vec = np.zeros(len(labels))

        # Calculate confusion mat
        count_mat = self._confu_mat(y_true, y_pred, labels)

        # sums over columns
        sum1 = np.sum(count_mat, axis=0)
        # sums over rows
        sum2 = np.sum(count_mat, axis=1)
        # Iterate over labels to calculate f1 scores of each one
        for i in range(len(labels)):
            precision = count_mat[i, i] / (sum1[i])
            recall = count_mat[i, i] / (sum2[i])

            f1_vec[i] = 2 * ((precision * recall) / (precision + recall))

        return f1_vec

    # Returns confusion matrix of predictions
    def _confu_mat(self, y_true: ndarray, y_pred: ndarray, labels: list) -&gt; ndarray:
        labels = np.array(labels)
        count_mat = np.zeros((len(labels), len(labels)))

        for i in range(len(labels)):
            for j in range(len(y_pred)):
                if y_pred[j] == labels[i]:
                    if y_pred[j] == y_true[j]:
                        count_mat[i, i] += 1
                    else:
                        x = np.where(labels == y_true[j])
                        count_mat[i, x[0]] += 1

        return count_mat</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.PerformanceMeasures.f_mean"><code class="name flex">
<span>def <span class="ident">f_mean</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: list) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures.resp"><code class="name flex">
<span>def <span class="ident">resp</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: list) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.autocorr"><code class="flex name class">
<span>class <span class="ident">autocorr</span></span>
<span>(</span><span>x: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the autocorrelation of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the autocorrelation of the data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class autocorr:
    &#34;&#34;&#34;Calculates the autocorrelation of the data.

    Parameters
    ----------
    x : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the autocorrelation of the data.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray):
        self.x = x
        self.result = np.correlate(x, x, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Autocorrelation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.autocorr.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.autocorr.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.autocorr.z"><code class="name">var <span class="ident">z</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.autocorr.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.corr"><code class="flex name class">
<span>class <span class="ident">corr</span></span>
<span>(</span><span>x: numpy.ndarray, y: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the correlation of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the correlation of <code>x</code> and <code>y</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class corr:
    &#34;&#34;&#34;Calculates the correlation of the data.

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    y : `np.array`
        An array containing the data.

    Returns
    -------
    z : `np.array`
        An array containing the correlation of `x` and `y`.

    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: ndarray, y: ndarray):
        self.x = x
        self.y = y
        self.result = np.correlate(x, y, mode=&#34;full&#34;)
        self.z = self.result[self.result.size // 2 :]
        self.z = self.z / float(np.abs(self.z).max())

    def plot(self):
        plt.plot(range(len(self.z)), self.z, label=&#34;Correlation&#34;)
        plt.legend()
        plt.show()

    def __call__(self):
        return self.z</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.corr.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.corr.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.corr.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.corr.z"><code class="name">var <span class="ident">z</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.corr.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.tools" href="index.html">likelihood.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="likelihood.tools.tools.cal_average" href="#likelihood.tools.tools.cal_average">cal_average</a></code></li>
<li><code><a title="likelihood.tools.tools.cal_missing_values" href="#likelihood.tools.tools.cal_missing_values">cal_missing_values</a></code></li>
<li><code><a title="likelihood.tools.tools.calculate_probability" href="#likelihood.tools.tools.calculate_probability">calculate_probability</a></code></li>
<li><code><a title="likelihood.tools.tools.cdf" href="#likelihood.tools.tools.cdf">cdf</a></code></li>
<li><code><a title="likelihood.tools.tools.check_nan_inf" href="#likelihood.tools.tools.check_nan_inf">check_nan_inf</a></code></li>
<li><code><a title="likelihood.tools.tools.difference_quotient" href="#likelihood.tools.tools.difference_quotient">difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.estimate_gradient" href="#likelihood.tools.tools.estimate_gradient">estimate_gradient</a></code></li>
<li><code><a title="likelihood.tools.tools.fft_denoise" href="#likelihood.tools.tools.fft_denoise">fft_denoise</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_feature_yaml" href="#likelihood.tools.tools.generate_feature_yaml">generate_feature_yaml</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_series" href="#likelihood.tools.tools.generate_series">generate_series</a></code></li>
<li><code><a title="likelihood.tools.tools.get_period" href="#likelihood.tools.tools.get_period">get_period</a></code></li>
<li><code><a title="likelihood.tools.tools.mean_square_error" href="#likelihood.tools.tools.mean_square_error">mean_square_error</a></code></li>
<li><code><a title="likelihood.tools.tools.minibatches" href="#likelihood.tools.tools.minibatches">minibatches</a></code></li>
<li><code><a title="likelihood.tools.tools.partial_difference_quotient" href="#likelihood.tools.tools.partial_difference_quotient">partial_difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide" href="#likelihood.tools.tools.sigmoide">sigmoide</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide_inv" href="#likelihood.tools.tools.sigmoide_inv">sigmoide_inv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.tools.tools.DataFrameEncoder" href="#likelihood.tools.tools.DataFrameEncoder">DataFrameEncoder</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decode" href="#likelihood.tools.tools.DataFrameEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decoding_list" href="#likelihood.tools.tools.DataFrameEncoder.decoding_list">decoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encode" href="#likelihood.tools.tools.DataFrameEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encoding_list" href="#likelihood.tools.tools.DataFrameEncoder.encoding_list">encoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.get_dictionaries" href="#likelihood.tools.tools.DataFrameEncoder.get_dictionaries">get_dictionaries</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.load_config" href="#likelihood.tools.tools.DataFrameEncoder.load_config">load_config</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.median_list" href="#likelihood.tools.tools.DataFrameEncoder.median_list">median_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.train" href="#likelihood.tools.tools.DataFrameEncoder.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.DataScaler" href="#likelihood.tools.tools.DataScaler">DataScaler</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataScaler.data_scaled" href="#likelihood.tools.tools.DataScaler.data_scaled">data_scaled</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.dataset_" href="#likelihood.tools.tools.DataScaler.dataset_">dataset_</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.rescale" href="#likelihood.tools.tools.DataScaler.rescale">rescale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.scale" href="#likelihood.tools.tools.DataScaler.scale">scale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.transpose" href="#likelihood.tools.tools.DataScaler.transpose">transpose</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.values" href="#likelihood.tools.tools.DataScaler.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.FeatureSelection" href="#likelihood.tools.tools.FeatureSelection">FeatureSelection</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.FeatureSelection.X" href="#likelihood.tools.tools.FeatureSelection.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.all_features_imp_graph" href="#likelihood.tools.tools.FeatureSelection.all_features_imp_graph">all_features_imp_graph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.get_digraph" href="#likelihood.tools.tools.FeatureSelection.get_digraph">get_digraph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.not_features" href="#likelihood.tools.tools.FeatureSelection.not_features">not_features</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.scaler" href="#likelihood.tools.tools.FeatureSelection.scaler">scaler</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.w_dict" href="#likelihood.tools.tools.FeatureSelection.w_dict">w_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LinearRegression" href="#likelihood.tools.tools.LinearRegression">LinearRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LinearRegression.X" href="#likelihood.tools.tools.LinearRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.fit" href="#likelihood.tools.tools.LinearRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.get_importances" href="#likelihood.tools.tools.LinearRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.importance" href="#likelihood.tools.tools.LinearRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.predict" href="#likelihood.tools.tools.LinearRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.w" href="#likelihood.tools.tools.LinearRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.y" href="#likelihood.tools.tools.LinearRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LogisticRegression" href="#likelihood.tools.tools.LogisticRegression">LogisticRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LogisticRegression.X" href="#likelihood.tools.tools.LogisticRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.fit" href="#likelihood.tools.tools.LogisticRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.get_importances" href="#likelihood.tools.tools.LogisticRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.importance" href="#likelihood.tools.tools.LogisticRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.predict" href="#likelihood.tools.tools.LogisticRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.w" href="#likelihood.tools.tools.LogisticRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.y" href="#likelihood.tools.tools.LogisticRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.OneHotEncoder" href="#likelihood.tools.tools.OneHotEncoder">OneHotEncoder</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.OneHotEncoder.decode" href="#likelihood.tools.tools.OneHotEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.encode" href="#likelihood.tools.tools.OneHotEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.x" href="#likelihood.tools.tools.OneHotEncoder.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.PerformanceMeasures" href="#likelihood.tools.tools.PerformanceMeasures">PerformanceMeasures</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.f_mean" href="#likelihood.tools.tools.PerformanceMeasures.f_mean">f_mean</a></code></li>
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.resp" href="#likelihood.tools.tools.PerformanceMeasures.resp">resp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.autocorr" href="#likelihood.tools.tools.autocorr">autocorr</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.autocorr.plot" href="#likelihood.tools.tools.autocorr.plot">plot</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.result" href="#likelihood.tools.tools.autocorr.result">result</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.x" href="#likelihood.tools.tools.autocorr.x">x</a></code></li>
<li><code><a title="likelihood.tools.tools.autocorr.z" href="#likelihood.tools.tools.autocorr.z">z</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.corr" href="#likelihood.tools.tools.corr">corr</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.corr.plot" href="#likelihood.tools.tools.corr.plot">plot</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.result" href="#likelihood.tools.tools.corr.result">result</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.x" href="#likelihood.tools.tools.corr.x">x</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.y" href="#likelihood.tools.tools.corr.y">y</a></code></li>
<li><code><a title="likelihood.tools.tools.corr.z" href="#likelihood.tools.tools.corr.z">z</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
