<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>likelihood.tools.tools API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>likelihood.tools.tools</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="likelihood.tools.tools.cal_average"><code class="name flex">
<span>def <span class="ident">cal_average</span></span>(<span>y: numpy.ndarray, alpha: float = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cal_average(y: np.ndarray, alpha: float = 1):
    &#34;&#34;&#34;Calculates the moving average of the data

    Parameters
    ----------
    y : `np.array`
        An array containing the data.
    alpha : `float`
        A `float` between `0` and `1`. By default it is set to `1`.

    Returns
    -------
    average : `float`
        The average of the data.
    &#34;&#34;&#34;

    n = int(alpha * len(y))
    w = np.ones(n) / n
    average = np.convolve(y, w, mode=&#34;same&#34;) / np.convolve(np.ones_like(y), w, mode=&#34;same&#34;)
    return average</code></pre>
</details>
<div class="desc"><p>Calculates the moving average of the data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>A <code>float</code> between <code>0</code> and <code>1</code>. By default it is set to <code>1</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>average</code></strong> :&ensp;<code>float</code></dt>
<dd>The average of the data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.cal_missing_values"><code class="name flex">
<span>def <span class="ident">cal_missing_values</span></span>(<span>df: pandas.core.frame.DataFrame) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cal_missing_values(df: DataFrame) -&gt; None:
    &#34;&#34;&#34;Calculate the percentage of missing (`NaN`/`NaT`) values per column in a dataframe.

    Parameters
    ----------
    df : `DataFrame`
        The input dataframe.

    Returns
    -------
    `None` : Prints out a table with columns as index and percentages of missing values as data.
    &#34;&#34;&#34;

    col = df.columns
    print(&#34;Total size : &#34;, &#34;{:,}&#34;.format(len(df)))
    for i in col:
        print(
            str(i) + &#34; : &#34; f&#34;{(df.isnull().sum()[i]/(df.isnull().sum()[i]+df[i].count()))*100:.2f}%&#34;
        )</code></pre>
</details>
<div class="desc"><p>Calculate the percentage of missing (<code>NaN</code>/<code>NaT</code>) values per column in a dataframe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The input dataframe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>None</code> : Prints out a table with columns as index and percentages of missing values as data.</p></div>
</dd>
<dt id="likelihood.tools.tools.calculate_probability"><code class="name flex">
<span>def <span class="ident">calculate_probability</span></span>(<span>x: numpy.ndarray, points: int = 1, cond: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_probability(x: np.ndarray, points: int = 1, cond: bool = True) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculates the probability of the data based on the CDF fit.

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    points : `int`
        Number of points to consider for the final probability calculation.
    cond : `bool`
        Condition to use product (True) or sum (False) for the final probability check.

    Returns
    -------
    p : `np.array`
        Array containing the probabilities of the data.
    &#34;&#34;&#34;

    if len(x) == 0:
        raise ValueError(&#34;Input array &#39;x&#39; must not be empty.&#34;)

    fit, _, sorted_x = cdf(x)
    p = fit(x)

    if cond:
        prob_value = np.prod(p[-points])
        message = &#34;product&#34;
    else:
        prob_value = np.sum(p[-points])
        message = &#34;sum&#34;

    if 0 &lt;= prob_value &lt;= 1:
        print(f&#34;The model has a probability of {prob_value * 100:.2f}% based on the {message}.&#34;)
    else:
        print(&#34;\nThe probability of the data cannot be calculated.\n&#34;)

    return p</code></pre>
</details>
<div class="desc"><p>Calculates the probability of the data based on the CDF fit.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>points</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of points to consider for the final probability calculation.</dd>
<dt><strong><code>cond</code></strong> :&ensp;<code>bool</code></dt>
<dd>Condition to use product (True) or sum (False) for the final probability check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Array containing the probabilities of the data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>x: numpy.ndarray,<br>poly: int = 9,<br>inv: bool = False,<br>plot: bool = False,<br>savename: str = None) ‑> tuple</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdf(
    x: np.ndarray, poly: int = 9, inv: bool = False, plot: bool = False, savename: str = None
) -&gt; tuple:
    &#34;&#34;&#34;Calculates the cumulative distribution function of the data.

    Parameters
    ----------
    x : `np.array`
        An array containing the data.
    poly : `int`
        Degree of the polynomial fit. By default it is set to `9`.
    inv : `bool`
        If True, calculate the inverse CDF (quantile function).
    plot : `bool`
        If True, plot the results.
    savename : `str`, optional
        Filename to save the plot.

    Returns
    -------
    fit : `np.poly1d`
        Polynomial fit of the CDF or quantile function.
    cdf_values : `np.array`
        Cumulative distribution values.
    sorted_x : `np.array`
        Sorted input data.
    &#34;&#34;&#34;

    if len(x) == 0:
        raise ValueError(&#34;Input array &#39;x&#39; must not be empty.&#34;)

    cdf_values = np.cumsum(x) / np.sum(x)
    sorted_x = np.sort(x)

    probabilities = np.linspace(0, 1, len(sorted_x))

    if inv:
        fit = np.polyfit(probabilities, sorted_x, poly)
        f = np.poly1d(fit)
        plot_label = &#34;Quantile Function&#34;
        x_values = probabilities
        y_values = sorted_x
    else:
        fit = np.polyfit(sorted_x, probabilities, poly)
        f = np.poly1d(fit)
        plot_label = &#34;Cumulative Distribution Function&#34;
        x_values = sorted_x
        y_values = cdf_values

    if plot:
        plt.figure()
        plt.plot(x_values, y_values, &#34;o&#34;, label=&#34;data&#34;)
        plt.plot(x_values, f(x_values), &#34;r--&#34;, label=&#34;fit&#34;)
        plt.title(plot_label)
        plt.xlabel(&#34;Probability&#34; if inv else &#34;Value&#34;)
        plt.ylabel(&#34;Value&#34; if inv else &#34;Probability&#34;)
        plt.legend()
        if savename:
            plt.savefig(savename, dpi=300)
        plt.show()

    return f, cdf_values, sorted_x</code></pre>
</details>
<div class="desc"><p>Calculates the cumulative distribution function of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the data.</dd>
<dt><strong><code>poly</code></strong> :&ensp;<code>int</code></dt>
<dd>Degree of the polynomial fit. By default it is set to <code>9</code>.</dd>
<dt><strong><code>inv</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, calculate the inverse CDF (quantile function).</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, plot the results.</dd>
<dt><strong><code>savename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Filename to save the plot.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fit</code></strong> :&ensp;<code>np.poly1d</code></dt>
<dd>Polynomial fit of the CDF or quantile function.</dd>
<dt><strong><code>cdf_values</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Cumulative distribution values.</dd>
<dt><strong><code>sorted_x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Sorted input data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.check_nan_inf"><code class="name flex">
<span>def <span class="ident">check_nan_inf</span></span>(<span>df: pandas.core.frame.DataFrame, verbose: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_nan_inf(df: DataFrame, verbose: bool = False) -&gt; DataFrame:
    &#34;&#34;&#34;
    Checks for NaN and Inf values in the DataFrame. If any are found, they will be removed.

    Parameters
    ----------
    df : DataFrame
        The input DataFrame to be checked.

    Returns
    ----------
    DataFrame
        A new DataFrame with NaN and Inf values removed.
    &#34;&#34;&#34;

    nan_values = df.isnull().values.any()
    inf_values = np.isinf(df.select_dtypes(include=&#34;number&#34;)).values.any()

    nan_count = df.isnull().values.sum()
    inf_count = np.isinf(df.select_dtypes(include=&#34;number&#34;)).values.sum()

    if nan_values:
        (
            print(
                &#34;UserWarning: Some rows may have been deleted due to the existence of NaN values.&#34;
            )
            if verbose
            else None
        )
        df.dropna(inplace=True)

    if inf_values:
        (
            print(
                &#34;UserWarning: Some rows may have been deleted due to the existence of Inf values.&#34;
            )
            if verbose
            else None
        )
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)

    print(f&#34;NaN values removed: &#34;, &#34;{:,}&#34;.format(nan_count))
    print(f&#34;Infinite values removed: &#34;, &#34;{:,}&#34;.format(inf_count))

    return df</code></pre>
</details>
<div class="desc"><p>Checks for NaN and Inf values in the DataFrame. If any are found, they will be removed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The input DataFrame to be checked.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataFrame</code></dt>
<dd>A new DataFrame with NaN and Inf values removed.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.difference_quotient"><code class="name flex">
<span>def <span class="ident">difference_quotient</span></span>(<span>f: Callable, x: float, h: float) ‑> Callable</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def difference_quotient(f: Callable, x: float, h: float) -&gt; Callable:
    &#34;&#34;&#34;Calculates the difference quotient of `f` evaluated at `x` and `x + h`

    Parameters
    ----------
    `f(x)` : `Callable`
        function.
    x : `float`
        Independent term.
    h : `float`
        Step size.

    Returns
    -------
    `(f(x + h) - f(x)) / h` : `float`
        Difference quotient of `f` evaluated at `x`.

    &#34;&#34;&#34;

    return (f(x + h) - f(x)) / h</code></pre>
</details>
<div class="desc"><p>Calculates the difference quotient of <code>f</code> evaluated at <code>x</code> and <code>x + h</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x)</code> : <code>Callable</code></dt>
<dt>function.</dt>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>Independent term.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>Step size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>(f(x + h) - f(x)) / h</code> : <code>float</code>
Difference quotient of <code>f</code> evaluated at <code>x</code>.</p></div>
</dd>
<dt id="likelihood.tools.tools.estimate_gradient"><code class="name flex">
<span>def <span class="ident">estimate_gradient</span></span>(<span>f: Callable, v: numpy.ndarray, h: float = 0.0001) ‑> List[numpy.ndarray]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_gradient(f: Callable, v: np.ndarray, h: float = 1e-4) -&gt; List[np.ndarray]:
    &#34;&#34;&#34;Calculates the gradient of `f` at `v`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
        Function to differentiate.
    v : `Vector` | `np.array`
        1D array representing vector `v=(x0,...,xi)`.
    h : `float`. By default it is set to `1e-4`
        The step size used to approximate the derivative.

    Returns
    -------
    grad_f : `List[np.array]`
        A list containing the estimated gradients of each component of `f` evaluated at `v`.
    &#34;&#34;&#34;
    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]</code></pre>
</details>
<div class="desc"><p>Calculates the gradient of <code>f</code> at <code>v</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt>Function to differentiate.</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector` | `np.array</code></dt>
<dd>1D array representing vector <code>v=(x0,...,xi)</code>.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float`. By default it is set to `1e-4</code></dt>
<dd>The step size used to approximate the derivative.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>grad_f</code></strong> :&ensp;<code>List[np.array]</code></dt>
<dd>A list containing the estimated gradients of each component of <code>f</code> evaluated at <code>v</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.fft_denoise"><code class="name flex">
<span>def <span class="ident">fft_denoise</span></span>(<span>dataset: numpy.ndarray, sigma: float = 0, mode: bool = True) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft_denoise(
    dataset: np.ndarray, sigma: float = 0, mode: bool = True
) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;Performs noise removal using the Fast Fourier Transform.

    Parameters
    ----------
    dataset : `np.ndarray`
        An array containing the noised data. Expected shape (num_samples, num_points).
    sigma : `float`, default=0
        A float between 0 and 1 representing the threshold for noise filtering.
    mode : `bool`, default=True
        If True, print progress messages.

    Returns
    -------
    denoised_dataset : `np.ndarray`
        An array containing the denoised data with the same shape as `dataset`.
    periods : `np.ndarray`
        Array of estimated periods for each sample in `dataset`.
    &#34;&#34;&#34;

    if not (0 &lt;= sigma &lt;= 1):
        raise ValueError(&#34;sigma must be between 0 and 1&#34;)

    num_samples, n_points = dataset.shape
    denoised_dataset = np.zeros_like(dataset)
    periods = np.zeros(num_samples)

    freq = (1 / n_points) * np.arange(n_points)
    L = np.arange(1, np.floor(n_points / 2), dtype=int)

    for i in range(num_samples):
        fhat = np.fft.fft(dataset[i, :], n_points)
        PSD = fhat * np.conj(fhat) / n_points
        threshold = np.mean(PSD) + sigma * np.std(PSD)
        indices = PSD &gt; threshold

        PSDclean = PSD * indices
        fhat_cleaned = fhat * indices

        denoised_signal = np.fft.ifft(fhat_cleaned).real
        denoised_dataset[i, :] = denoised_signal

        peak_index = L[np.argmax(np.abs(fhat[L]))]
        periods[i] = 1 / (2 * freq[peak_index])

        if mode:
            print(f&#34;The {i+1}-th row of the dataset has been denoised.&#34;)
            print(f&#34;The estimated period is {round(periods[i], 4)}&#34;)

    return denoised_dataset, periods</code></pre>
</details>
<div class="desc"><p>Performs noise removal using the Fast Fourier Transform.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the noised data. Expected shape (num_samples, num_points).</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code>, default=<code>0</code></dt>
<dd>A float between 0 and 1 representing the threshold for noise filtering.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, print progress messages.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>denoised_dataset</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the denoised data with the same shape as <code>dataset</code>.</dd>
<dt><strong><code>periods</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array of estimated periods for each sample in <code>dataset</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.generate_feature_yaml"><code class="name flex">
<span>def <span class="ident">generate_feature_yaml</span></span>(<span>df: pandas.core.frame.DataFrame,<br>ignore_features: List[str] = None,<br>yaml_string: bool = False) ‑> Dict | str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_feature_yaml(
    df: DataFrame, ignore_features: List[str] = None, yaml_string: bool = False
) -&gt; Dict | str:
    &#34;&#34;&#34;
    Generate a YAML string containing information about ordinal, numeric, and categorical features
    based on the given DataFrame.

    Parameters
    ----------
    df : `pd.DataFrame`
        The DataFrame containing the data.
    ignore_features : `List[`str`]`
        A list of features to ignore.
    yaml_string : `bool`
        If `True`, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is `False`.

    Returns
    -------
    feature_info : `Dict` | `str`
        A dictionary with four keys (&#39;ordinal_features&#39;, &#39;numeric_features&#39;, &#39;categorical_features&#39;, &#39;ignore_features&#39;)
        mapping to lists of feature names. Or a YAML formatted string if `yaml_string` is `True`.
    &#34;&#34;&#34;
    ignore_features = ignore_features or []
    feature_info = {
        &#34;ordinal_features&#34;: [],
        &#34;numeric_features&#34;: [],
        &#34;categorical_features&#34;: [],
        &#34;ignore_features&#34;: ignore_features,
    }

    for col in df.columns:
        if col in ignore_features:
            continue

        if pd.api.types.is_numeric_dtype(df[col]):
            if pd.api.types.is_integer_dtype(df[col]) or pd.api.types.is_float_dtype(df[col]):
                feature_info[&#34;numeric_features&#34;].append(col)
            elif pd.api.types.is_bool_dtype(df[col]):
                feature_info[&#34;ordinal_features&#34;].append(col)  # Assuming bool can be ordinal
        elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
            feature_info[&#34;categorical_features&#34;].append(col)
        else:
            print(f&#34;Unknown type for feature {col}&#34;)

    if yaml_string:
        return yaml.dump(feature_info, default_flow_style=False)

    return feature_info</code></pre>
</details>
<div class="desc"><p>Generate a YAML string containing information about ordinal, numeric, and categorical features
based on the given DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The DataFrame containing the data.</dd>
<dt><strong><code>ignore_features</code></strong> :&ensp;<code>List[<code>str</code>]</code></dt>
<dd>A list of features to ignore.</dd>
<dt><strong><code>yaml_string</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>True</code>, return the result as a YAML formatted string. Otherwise, return it as a dictionary. Default is <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>feature_info</code></strong> :&ensp;<code>Dict` | `str</code></dt>
<dd>A dictionary with four keys ('ordinal_features', 'numeric_features', 'categorical_features', 'ignore_features')
mapping to lists of feature names. Or a YAML formatted string if <code>yaml_string</code> is <code>True</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.generate_series"><code class="name flex">
<span>def <span class="ident">generate_series</span></span>(<span>n: int, n_steps: int, incline: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_series(n: int, n_steps: int, incline: bool = True):
    &#34;&#34;&#34;Function that generates `n` series of length `n_steps`&#34;&#34;&#34;
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, n, 1)

    if incline:
        slope = np.random.rand(n, 1)
    else:
        slope = 0.0
        offsets2 = 1

    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # + wave 2
    series += 0.7 * (np.random.rand(n, n_steps) - 0.5)  # + noise
    series += 5 * slope * time + 2 * (offsets2 - offsets1) * time ** (1 - offsets2)
    series = series
    return series.astype(np.float32)</code></pre>
</details>
<div class="desc"><p>Function that generates <code>n</code> series of length <code>n_steps</code></p></div>
</dd>
<dt id="likelihood.tools.tools.get_period"><code class="name flex">
<span>def <span class="ident">get_period</span></span>(<span>dataset: numpy.ndarray) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_period(dataset: np.ndarray) -&gt; float:
    &#34;&#34;&#34;Calculates the periodicity of a `dataset`.

    Parameters
    ----------
    dataset : `ndarray`
        the `dataset` describing the function over which the period is calculated.

    Returns
    -------
    period : `float`
        period of the function described by the `dataset`.
    &#34;&#34;&#34;
    n = dataset.size

    if n &lt; 2:
        raise ValueError(&#34;Dataset must contain at least two points.&#34;)

    fhat = np.fft.rfft(dataset)
    freqs = np.fft.rfftfreq(n)

    PSD = np.abs(fhat) ** 2 / n

    PSD[0] = 0

    max_psd_index = np.argmax(PSD)

    dominant_freq = freqs[max_psd_index]
    if dominant_freq == 0:
        raise ValueError(&#34;No significant periodic component found in the dataset.&#34;)

    period = 1 / dominant_freq

    return period</code></pre>
</details>
<div class="desc"><p>Calculates the periodicity of a <code>dataset</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the <code>dataset</code> describing the function over which the period is calculated.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>period</code></strong> :&ensp;<code>float</code></dt>
<dd>period of the function described by the <code>dataset</code>.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.mean_square_error"><code class="name flex">
<span>def <span class="ident">mean_square_error</span></span>(<span>y_true: numpy.ndarray, y_pred: numpy.ndarray, print_error: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_square_error(y_true: np.ndarray, y_pred: np.ndarray, print_error: bool = False):
    &#34;&#34;&#34;Calculates the Root Mean Squared Error

    Parameters
    ----------
    y_true : `np.array`
        An array containing the true values.
    y_pred : `np.array`
        An array containing the predicted values.

    Returns
    -------
    RMSE : `float`
        The Root Mean Squared Error.
    &#34;&#34;&#34;
    if print_error:
        print(f&#34;The RMSE is {np.sqrt(np.mean((y_true - y_pred)**2))}&#34;)

    return np.sqrt(np.mean((y_true - y_pred) ** 2))</code></pre>
</details>
<div class="desc"><p>Calculates the Root Mean Squared Error</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_true</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the true values.</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the predicted values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>RMSE</code></strong> :&ensp;<code>float</code></dt>
<dd>The Root Mean Squared Error.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.minibatches"><code class="name flex">
<span>def <span class="ident">minibatches</span></span>(<span>dataset: List, batch_size: int, shuffle: bool = True) ‑> List</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minibatches(dataset: List, batch_size: int, shuffle: bool = True) -&gt; List:
    &#34;&#34;&#34;Generates &#39;batch_size&#39;-sized minibatches from the dataset

    Parameters
    ----------
    dataset : `List`
        The data to be divided into mini-batch.
    batch_size : `int`
        Specifies the size of each mini-batch.
    shuffle : `bool`
        If set `True`, the data will be shuffled before dividing it into mini-batches.

    Returns
    -------
    `List[List]`
        A list of lists containing the mini-batches. Each sublist is a separate mini-batch with length `batch_size`.
    &#34;&#34;&#34;

    # start indexes 0, batch_size, 2 * batch_size, ...
    batch_starts = [start for start in range(0, len(dataset), batch_size)]

    if shuffle:
        np.random.shuffle(batch_starts)  # shuffle the batches

    for start in batch_starts:
        end = start + batch_size
        yield dataset[start:end]</code></pre>
</details>
<div class="desc"><p>Generates 'batch_size'-sized minibatches from the dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>List</code></dt>
<dd>The data to be divided into mini-batch.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Specifies the size of each mini-batch.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code></dt>
<dd>If set <code>True</code>, the data will be shuffled before dividing it into mini-batches.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>List[List]</code>
A list of lists containing the mini-batches. Each sublist is a separate mini-batch with length <code>batch_size</code>.</p></div>
</dd>
<dt id="likelihood.tools.tools.partial_difference_quotient"><code class="name flex">
<span>def <span class="ident">partial_difference_quotient</span></span>(<span>f: Callable, v: numpy.ndarray, i: int, h: float) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_difference_quotient(f: Callable, v: np.ndarray, i: int, h: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculates the partial difference quotient of `f`

    Parameters
    ----------
    `f(x0,...,xi-th)` : `Callable` function
        Function to differentiate.
    v : `Vector` | `np.array`
        1D array representing vector `v=(x0,...,xi)`.
    h : `float`
        Step size.

    Returns
    -------
    `(f(w) - f(v)) / h` : `np.array`
        the `i-th` partial difference quotient of `f` at `v`

    &#34;&#34;&#34;

    w = [
        v_j + (h if j == i else 0) for j, v_j in enumerate(v)  # add h to just the ith element of v
    ]
    return (f(w) - f(v)) / h</code></pre>
</details>
<div class="desc"><p>Calculates the partial difference quotient of <code>f</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><code>f(x0,...,xi-th)</code> : <code>Callable</code> function</dt>
<dt>Function to differentiate.</dt>
<dt><strong><code>v</code></strong> :&ensp;<code>Vector` | `np.array</code></dt>
<dd>1D array representing vector <code>v=(x0,...,xi)</code>.</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code></dt>
<dd>Step size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>(f(w) - f(v)) / h</code> : <code>np.array</code>
the <code>i-th</code> partial difference quotient of <code>f</code> at <code>v</code></p></div>
</dd>
<dt id="likelihood.tools.tools.sigmoide"><code class="name flex">
<span>def <span class="ident">sigmoide</span></span>(<span>x: float) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoide(x: float) -&gt; float:
    &#34;&#34;&#34;The sigmoid function&#34;&#34;&#34;
    return 1 / (1 + math.exp(-x))</code></pre>
</details>
<div class="desc"><p>The sigmoid function</p></div>
</dd>
<dt id="likelihood.tools.tools.sigmoide_inv"><code class="name flex">
<span>def <span class="ident">sigmoide_inv</span></span>(<span>y: float) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoide_inv(y: float) -&gt; float:
    &#34;&#34;&#34;Calculates the inverse of the sigmoid function

    Parameters
    ----------
    y : `float`
        the number to evaluate the function.

    Returns
    -------
    `float`
        value of evaluated function.
    &#34;&#34;&#34;

    return math.log(y / (1 - y))</code></pre>
</details>
<div class="desc"><p>Calculates the inverse of the sigmoid function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>float</code></dt>
<dd>the number to evaluate the function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>float</code>
value of evaluated function.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="likelihood.tools.tools.AutoCorrelation"><code class="flex name class">
<span>class <span class="ident">AutoCorrelation</span></span>
<span>(</span><span>x: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoCorrelation(CorrelationBase):
    &#34;&#34;&#34;Calculates the autocorrelation of a dataset.

    Parameters
    ----------
    x : `np.ndarray`
        An array containing the data.

    Returns
    -------
    z : `np.ndarray`
        An array containing the autocorrelation of the data.
    &#34;&#34;&#34;

    def __init__(self, x: np.ndarray):
        super().__init__(x)</code></pre>
</details>
<div class="desc"><p>Calculates the autocorrelation of a dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the autocorrelation of the data.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="likelihood.tools.tools.CorrelationBase" href="#likelihood.tools.tools.CorrelationBase">CorrelationBase</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="likelihood.tools.tools.CorrelationBase" href="#likelihood.tools.tools.CorrelationBase">CorrelationBase</a></b></code>:
<ul class="hlist">
<li><code><a title="likelihood.tools.tools.CorrelationBase.plot" href="#likelihood.tools.tools.CorrelationBase.plot">plot</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="likelihood.tools.tools.Correlation"><code class="flex name class">
<span>class <span class="ident">Correlation</span></span>
<span>(</span><span>x: numpy.ndarray, y: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Correlation(CorrelationBase):
    &#34;&#34;&#34;Calculates the cross-correlation of two datasets.

    Parameters
    ----------
    x : `np.ndarray`
        An array containing the first dataset.
    y : `np.ndarray`
        An array containing the second dataset.

    Returns
    -------
    z : `np.ndarray`
        An array containing the correlation of `x` and `y`.

    &#34;&#34;&#34;

    def __init__(self, x: np.ndarray, y: np.ndarray):
        super().__init__(x, y)</code></pre>
</details>
<div class="desc"><p>Calculates the cross-correlation of two datasets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the first dataset.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the second dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array containing the correlation of <code>x</code> and <code>y</code>.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="likelihood.tools.tools.CorrelationBase" href="#likelihood.tools.tools.CorrelationBase">CorrelationBase</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="likelihood.tools.tools.CorrelationBase" href="#likelihood.tools.tools.CorrelationBase">CorrelationBase</a></b></code>:
<ul class="hlist">
<li><code><a title="likelihood.tools.tools.CorrelationBase.plot" href="#likelihood.tools.tools.CorrelationBase.plot">plot</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="likelihood.tools.tools.CorrelationBase"><code class="flex name class">
<span>class <span class="ident">CorrelationBase</span></span>
<span>(</span><span>x: numpy.ndarray, y: numpy.ndarray | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CorrelationBase:
    &#34;&#34;&#34;Base class for correlation calculations.&#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: np.ndarray, y: Union[np.ndarray, None] = None):
        self.x = x
        self.y = y if y is not None else x
        self._compute_correlation()
        self.z = self.result[self.result.size // 2 :]
        self.z /= np.abs(self.z).max()

    def _compute_correlation(self):
        &#34;&#34;&#34;Compute the correlation between x and y (or x with itself for autocorrelation).&#34;&#34;&#34;
        self.result = np.correlate(self.x, self.y, mode=&#34;full&#34;)

    def plot(self):
        &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
        plt.plot(range(len(self.z)), self.z, label=self._get_label())
        plt.legend()
        plt.show()

    def _get_label(self) -&gt; str:
        return &#34;Autocorrelation&#34; if np.array_equal(self.x, self.y) else &#34;Correlation&#34;

    def __call__(self):
        &#34;&#34;&#34;Return the computed correlation or autocorrelation.&#34;&#34;&#34;
        return self.z</code></pre>
</details>
<div class="desc"><p>Base class for correlation calculations.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="likelihood.tools.tools.AutoCorrelation" href="#likelihood.tools.tools.AutoCorrelation">AutoCorrelation</a></li>
<li><a title="likelihood.tools.tools.Correlation" href="#likelihood.tools.tools.Correlation">Correlation</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.CorrelationBase.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CorrelationBase:
    &#34;&#34;&#34;Base class for correlation calculations.&#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: np.ndarray, y: Union[np.ndarray, None] = None):
        self.x = x
        self.y = y if y is not None else x
        self._compute_correlation()
        self.z = self.result[self.result.size // 2 :]
        self.z /= np.abs(self.z).max()

    def _compute_correlation(self):
        &#34;&#34;&#34;Compute the correlation between x and y (or x with itself for autocorrelation).&#34;&#34;&#34;
        self.result = np.correlate(self.x, self.y, mode=&#34;full&#34;)

    def plot(self):
        &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
        plt.plot(range(len(self.z)), self.z, label=self._get_label())
        plt.legend()
        plt.show()

    def _get_label(self) -&gt; str:
        return &#34;Autocorrelation&#34; if np.array_equal(self.x, self.y) else &#34;Correlation&#34;

    def __call__(self):
        &#34;&#34;&#34;Return the computed correlation or autocorrelation.&#34;&#34;&#34;
        return self.z</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.CorrelationBase.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CorrelationBase:
    &#34;&#34;&#34;Base class for correlation calculations.&#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: np.ndarray, y: Union[np.ndarray, None] = None):
        self.x = x
        self.y = y if y is not None else x
        self._compute_correlation()
        self.z = self.result[self.result.size // 2 :]
        self.z /= np.abs(self.z).max()

    def _compute_correlation(self):
        &#34;&#34;&#34;Compute the correlation between x and y (or x with itself for autocorrelation).&#34;&#34;&#34;
        self.result = np.correlate(self.x, self.y, mode=&#34;full&#34;)

    def plot(self):
        &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
        plt.plot(range(len(self.z)), self.z, label=self._get_label())
        plt.legend()
        plt.show()

    def _get_label(self) -&gt; str:
        return &#34;Autocorrelation&#34; if np.array_equal(self.x, self.y) else &#34;Correlation&#34;

    def __call__(self):
        &#34;&#34;&#34;Return the computed correlation or autocorrelation.&#34;&#34;&#34;
        return self.z</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.CorrelationBase.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CorrelationBase:
    &#34;&#34;&#34;Base class for correlation calculations.&#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: np.ndarray, y: Union[np.ndarray, None] = None):
        self.x = x
        self.y = y if y is not None else x
        self._compute_correlation()
        self.z = self.result[self.result.size // 2 :]
        self.z /= np.abs(self.z).max()

    def _compute_correlation(self):
        &#34;&#34;&#34;Compute the correlation between x and y (or x with itself for autocorrelation).&#34;&#34;&#34;
        self.result = np.correlate(self.x, self.y, mode=&#34;full&#34;)

    def plot(self):
        &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
        plt.plot(range(len(self.z)), self.z, label=self._get_label())
        plt.legend()
        plt.show()

    def _get_label(self) -&gt; str:
        return &#34;Autocorrelation&#34; if np.array_equal(self.x, self.y) else &#34;Correlation&#34;

    def __call__(self):
        &#34;&#34;&#34;Return the computed correlation or autocorrelation.&#34;&#34;&#34;
        return self.z</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.CorrelationBase.z"><code class="name">var <span class="ident">z</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CorrelationBase:
    &#34;&#34;&#34;Base class for correlation calculations.&#34;&#34;&#34;

    __slots__ = [&#34;x&#34;, &#34;y&#34;, &#34;result&#34;, &#34;z&#34;]

    def __init__(self, x: np.ndarray, y: Union[np.ndarray, None] = None):
        self.x = x
        self.y = y if y is not None else x
        self._compute_correlation()
        self.z = self.result[self.result.size // 2 :]
        self.z /= np.abs(self.z).max()

    def _compute_correlation(self):
        &#34;&#34;&#34;Compute the correlation between x and y (or x with itself for autocorrelation).&#34;&#34;&#34;
        self.result = np.correlate(self.x, self.y, mode=&#34;full&#34;)

    def plot(self):
        &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
        plt.plot(range(len(self.z)), self.z, label=self._get_label())
        plt.legend()
        plt.show()

    def _get_label(self) -&gt; str:
        return &#34;Autocorrelation&#34; if np.array_equal(self.x, self.y) else &#34;Correlation&#34;

    def __call__(self):
        &#34;&#34;&#34;Return the computed correlation or autocorrelation.&#34;&#34;&#34;
        return self.z</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.CorrelationBase.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
    &#34;&#34;&#34;Plot the correlation or autocorrelation.&#34;&#34;&#34;
    plt.plot(range(len(self.z)), self.z, label=self._get_label())
    plt.legend()
    plt.show()</code></pre>
</details>
<div class="desc"><p>Plot the correlation or autocorrelation.</p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder"><code class="flex name class">
<span>class <span class="ident">DataFrameEncoder</span></span>
<span>(</span><span>data: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [
        &#34;_df&#34;,
        &#34;_names&#34;,
        &#34;_encode_columns&#34;,
        &#34;encoding_list&#34;,
        &#34;decoding_list&#34;,
        &#34;median_list&#34;,
    ]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []
        self.median_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        self.median_list = labelencoder[3]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if len(self._df[i].unique()) &gt; 1:
                    median_value = len(self._df[i].unique()) // 2
                else:
                    median_value = 1.0
                if norm_method == &#34;median&#34;:
                    self._df[i] = self._df[i].astype(&#34;float64&#34;)
                    self._df[i] = self._df[i] / median_value
                    self.median_list.append(median_value)
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            if len(self.median_list) == len(self._encode_columns):
                median_mode = True
            else:
                median_mode = False
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
                    if median_mode:
                        self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                        self._df[colname] = self._df[colname] / self.median_list[num]
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                    if median_mode:
                        df_decoded[i] = df_decoded[i] * self.median_list[j]
                        df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump(
                [self.encoding_list, self.decoding_list, self._encode_columns, self.median_list], f
            )

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<div class="desc"><p>Allows encoding and decoding Dataframes</p>
<p>Sets the columns of the <code>DataFrame</code></p></div>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decoding_list"><code class="name">var <span class="ident">decoding_list</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [
        &#34;_df&#34;,
        &#34;_names&#34;,
        &#34;_encode_columns&#34;,
        &#34;encoding_list&#34;,
        &#34;decoding_list&#34;,
        &#34;median_list&#34;,
    ]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []
        self.median_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        self.median_list = labelencoder[3]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if len(self._df[i].unique()) &gt; 1:
                    median_value = len(self._df[i].unique()) // 2
                else:
                    median_value = 1.0
                if norm_method == &#34;median&#34;:
                    self._df[i] = self._df[i].astype(&#34;float64&#34;)
                    self._df[i] = self._df[i] / median_value
                    self.median_list.append(median_value)
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            if len(self.median_list) == len(self._encode_columns):
                median_mode = True
            else:
                median_mode = False
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
                    if median_mode:
                        self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                        self._df[colname] = self._df[colname] / self.median_list[num]
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                    if median_mode:
                        df_decoded[i] = df_decoded[i] * self.median_list[j]
                        df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump(
                [self.encoding_list, self.decoding_list, self._encode_columns, self.median_list], f
            )

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encoding_list"><code class="name">var <span class="ident">encoding_list</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [
        &#34;_df&#34;,
        &#34;_names&#34;,
        &#34;_encode_columns&#34;,
        &#34;encoding_list&#34;,
        &#34;decoding_list&#34;,
        &#34;median_list&#34;,
    ]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []
        self.median_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        self.median_list = labelencoder[3]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if len(self._df[i].unique()) &gt; 1:
                    median_value = len(self._df[i].unique()) // 2
                else:
                    median_value = 1.0
                if norm_method == &#34;median&#34;:
                    self._df[i] = self._df[i].astype(&#34;float64&#34;)
                    self._df[i] = self._df[i] / median_value
                    self.median_list.append(median_value)
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            if len(self.median_list) == len(self._encode_columns):
                median_mode = True
            else:
                median_mode = False
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
                    if median_mode:
                        self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                        self._df[colname] = self._df[colname] / self.median_list[num]
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                    if median_mode:
                        df_decoded[i] = df_decoded[i] * self.median_list[j]
                        df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump(
                [self.encoding_list, self.decoding_list, self._encode_columns, self.median_list], f
            )

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.median_list"><code class="name">var <span class="ident">median_list</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFrameEncoder:
    &#34;&#34;&#34;Allows encoding and decoding Dataframes&#34;&#34;&#34;

    __slots__ = [
        &#34;_df&#34;,
        &#34;_names&#34;,
        &#34;_encode_columns&#34;,
        &#34;encoding_list&#34;,
        &#34;decoding_list&#34;,
        &#34;median_list&#34;,
    ]

    def __init__(self, data: DataFrame) -&gt; None:
        &#34;&#34;&#34;Sets the columns of the `DataFrame`&#34;&#34;&#34;
        self._df = data.copy()
        self._names = data.columns
        self._encode_columns = []
        self.encoding_list = []
        self.decoding_list = []
        self.median_list = []

    def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
        &#34;&#34;&#34;Loads dictionaries from a given directory

        Keyword Arguments:
        ----------
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        &#34;&#34;&#34;
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
            labelencoder = pickle.load(file)
        self.encoding_list = labelencoder[0]
        self.decoding_list = labelencoder[1]
        self._encode_columns = labelencoder[2]
        self.median_list = labelencoder[3]
        print(&#34;Configuration successfully uploaded&#34;)

    def train(self, path_to_save: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
        save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
        dictionary_name = (
            kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
        )
        norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
        for i in self._names:
            if self._df[i].dtype == &#34;object&#34;:
                self._encode_columns.append(i)
                column_index = range(len(self._df[i].unique()))
                column_keys = self._df[i].unique()
                encode_dict = dict(zip(column_keys, column_index))
                decode_dict = dict(zip(column_index, column_keys))
                self._df[i] = self._df[i].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if len(self._df[i].unique()) &gt; 1:
                    median_value = len(self._df[i].unique()) // 2
                else:
                    median_value = 1.0
                if norm_method == &#34;median&#34;:
                    self._df[i] = self._df[i].astype(&#34;float64&#34;)
                    self._df[i] = self._df[i] / median_value
                    self.median_list.append(median_value)
                self.encoding_list.append(encode_dict)
                self.decoding_list.append(decode_dict)
        if save_mode:
            self._save_encoder(path_to_save, dictionary_name)

    def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
        &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

        Keyword Arguments:
        ----------
        - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
        - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
        - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
        &#34;&#34;&#34;
        if len(self.encoding_list) == 0:
            self.train(path_to_save, **kwargs)
            return self._df

        else:
            print(&#34;Configuration detected&#34;)
            if len(self.median_list) == len(self._encode_columns):
                median_mode = True
            else:
                median_mode = False
            for num, colname in enumerate(self._encode_columns):
                if self._df[colname].dtype == &#34;object&#34;:
                    encode_dict = self.encoding_list[num]
                    self._df[colname] = self._df[colname].apply(
                        self._code_transformation_to, dictionary_list=encode_dict
                    )
                    if median_mode:
                        self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                        self._df[colname] = self._df[colname] / self.median_list[num]
            return self._df

    def decode(self) -&gt; DataFrame:
        &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
        j = 0
        df_decoded = self._df.copy()
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        try:
            number_of_columns = len(self.decoding_list[j])
            for i in self._encode_columns:
                if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                    if median_mode:
                        df_decoded[i] = df_decoded[i] * self.median_list[j]
                        df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                    df_decoded[i] = df_decoded[i].apply(
                        self._code_transformation_to, dictionary_list=self.decoding_list[j]
                    )
                    j += 1
            return df_decoded
        except AttributeError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
        &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
        try:
            return self.encoding_list, self.decoding_list
        except ValueError as e:
            warning_type = &#34;UserWarning&#34;
            msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
            msg += &#34;Error: {%s}&#34; % e
            print(f&#34;{warning_type}: {msg}&#34;)

    def _save_encoder(self, path_to_save: str, dictionary_name: str) -&gt; None:
        &#34;&#34;&#34;Method to serialize the `encoding_list`, `decoding_list` and `_encode_columns` list&#34;&#34;&#34;
        with open(path_to_save + dictionary_name + &#34;.pkl&#34;, &#34;wb&#34;) as f:
            pickle.dump(
                [self.encoding_list, self.decoding_list, self._encode_columns, self.median_list], f
            )

    def _code_transformation_to(self, character: str, dictionary_list: List[dict]) -&gt; int:
        &#34;&#34;&#34;Auxiliary function to perform data transformation using a dictionary

        Parameters
        ----------
        character : `str`
            A character data type.
        dictionary_list : List[`dict`]
            An object of dictionary type.

        Returns
        -------
        dict_type[`character`] or `np.nan` if dict_type[`character`] doesn&#39;t exist.
        &#34;&#34;&#34;
        try:
            return dictionary_list[character]
        except:
            return np.nan</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataFrameEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self) -&gt; DataFrame:
    &#34;&#34;&#34;Decodes the `int` type columns of the `DataFrame`&#34;&#34;&#34;
    j = 0
    df_decoded = self._df.copy()
    if len(self.median_list) == len(self._encode_columns):
        median_mode = True
    else:
        median_mode = False
    try:
        number_of_columns = len(self.decoding_list[j])
        for i in self._encode_columns:
            if df_decoded[i].dtype == &#34;int64&#34; or df_decoded[i].dtype == &#34;float64&#34;:
                if median_mode:
                    df_decoded[i] = df_decoded[i] * self.median_list[j]
                    df_decoded[i] = df_decoded[i].astype(&#34;int64&#34;)
                df_decoded[i] = df_decoded[i].apply(
                    self._code_transformation_to, dictionary_list=self.decoding_list[j]
                )
                j += 1
        return df_decoded
    except AttributeError as e:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;It is not possible to decode the dataframe, since it has not been encoded&#34;
        msg += &#34;Error: {%s}&#34; % e
        print(f&#34;{warning_type}: {msg}&#34;)</code></pre>
</details>
<div class="desc"><p>Decodes the <code>int</code> type columns of the <code>DataFrame</code></p></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, path_to_save: str = './', **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, path_to_save: str = &#34;./&#34;, **kwargs) -&gt; DataFrame:
    &#34;&#34;&#34;Encodes the `object` type columns of the dataframe

    Keyword Arguments:
    ----------
    - save_mode (`bool`): An optional integer parameter. By default it is set to `True`
    - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
    - norm_method (`str`): An optional string parameter to perform normalization. By default it is set to `None`
    &#34;&#34;&#34;
    if len(self.encoding_list) == 0:
        self.train(path_to_save, **kwargs)
        return self._df

    else:
        print(&#34;Configuration detected&#34;)
        if len(self.median_list) == len(self._encode_columns):
            median_mode = True
        else:
            median_mode = False
        for num, colname in enumerate(self._encode_columns):
            if self._df[colname].dtype == &#34;object&#34;:
                encode_dict = self.encoding_list[num]
                self._df[colname] = self._df[colname].apply(
                    self._code_transformation_to, dictionary_list=encode_dict
                )
                if median_mode:
                    self._df[colname] = self._df[colname].astype(&#34;float64&#34;)
                    self._df[colname] = self._df[colname] / self.median_list[num]
        return self._df</code></pre>
</details>
<div class="desc"><p>Encodes the <code>object</code> type columns of the dataframe</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>save_mode (<code>bool</code>): An optional integer parameter. By default it is set to <code>True</code></li>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
<li>norm_method (<code>str</code>): An optional string parameter to perform normalization. By default it is set to <code>None</code></li>
</ul></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.get_dictionaries"><code class="name flex">
<span>def <span class="ident">get_dictionaries</span></span>(<span>self) ‑> Tuple[List[dict], List[dict]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dictionaries(self) -&gt; Tuple[List[dict], List[dict]]:
    &#34;&#34;&#34;Allows to return the `list` of dictionaries for `encoding` and `decoding`&#34;&#34;&#34;
    try:
        return self.encoding_list, self.decoding_list
    except ValueError as e:
        warning_type = &#34;UserWarning&#34;
        msg = &#34;It is not possible to return the list of dictionaries as they have not been created.&#34;
        msg += &#34;Error: {%s}&#34; % e
        print(f&#34;{warning_type}: {msg}&#34;)</code></pre>
</details>
<div class="desc"><p>Allows to return the <code>list</code> of dictionaries for <code>encoding</code> and <code>decoding</code></p></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.load_config"><code class="name flex">
<span>def <span class="ident">load_config</span></span>(<span>self, path_to_dictionaries: str = './', **kwargs) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_config(self, path_to_dictionaries: str = &#34;./&#34;, **kwargs) -&gt; None:
    &#34;&#34;&#34;Loads dictionaries from a given directory

    Keyword Arguments:
    ----------
    - dictionary_name (`str`): An optional string parameter. By default it is set to `labelencoder_dictionary`
    &#34;&#34;&#34;
    dictionary_name = (
        kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
    )
    with open(os.path.join(path_to_dictionaries, dictionary_name + &#34;.pkl&#34;), &#34;rb&#34;) as file:
        labelencoder = pickle.load(file)
    self.encoding_list = labelencoder[0]
    self.decoding_list = labelencoder[1]
    self._encode_columns = labelencoder[2]
    self.median_list = labelencoder[3]
    print(&#34;Configuration successfully uploaded&#34;)</code></pre>
</details>
<div class="desc"><p>Loads dictionaries from a given directory</p>
<h2 id="keyword-arguments">Keyword Arguments:</h2>
<ul>
<li>dictionary_name (<code>str</code>): An optional string parameter. By default it is set to <code>labelencoder_dictionary</code></li>
</ul></div>
</dd>
<dt id="likelihood.tools.tools.DataFrameEncoder.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, path_to_save: str, **kwargs) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, path_to_save: str, **kwargs) -&gt; None:
    &#34;&#34;&#34;Trains the encoders and decoders using the `DataFrame`&#34;&#34;&#34;
    save_mode = kwargs[&#34;save_mode&#34;] if &#34;save_mode&#34; in kwargs else True
    dictionary_name = (
        kwargs[&#34;dictionary_name&#34;] if &#34;dictionary_name&#34; in kwargs else &#34;labelencoder_dictionary&#34;
    )
    norm_method = kwargs[&#34;norm_method&#34;] if &#34;norm_method&#34; in kwargs else &#34;None&#34;
    for i in self._names:
        if self._df[i].dtype == &#34;object&#34;:
            self._encode_columns.append(i)
            column_index = range(len(self._df[i].unique()))
            column_keys = self._df[i].unique()
            encode_dict = dict(zip(column_keys, column_index))
            decode_dict = dict(zip(column_index, column_keys))
            self._df[i] = self._df[i].apply(
                self._code_transformation_to, dictionary_list=encode_dict
            )
            if len(self._df[i].unique()) &gt; 1:
                median_value = len(self._df[i].unique()) // 2
            else:
                median_value = 1.0
            if norm_method == &#34;median&#34;:
                self._df[i] = self._df[i].astype(&#34;float64&#34;)
                self._df[i] = self._df[i] / median_value
                self.median_list.append(median_value)
            self.encoding_list.append(encode_dict)
            self.decoding_list.append(decode_dict)
    if save_mode:
        self._save_encoder(path_to_save, dictionary_name)</code></pre>
</details>
<div class="desc"><p>Trains the encoders and decoders using the <code>DataFrame</code></p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.DataScaler"><code class="flex name class">
<span>class <span class="ident">DataScaler</span></span>
<span>(</span><span>dataset: numpy.ndarray, n: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"><p>numpy array <code>scaler</code> and <code>rescaler</code></p>
<p>Initializes the parameters required for scaling the data</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.data_scaled"><code class="name">var <span class="ident">data_scaled</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.dataset_"><code class="name">var <span class="ident">dataset_</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.inv_fitting"><code class="name">var <span class="ident">inv_fitting</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.transpose"><code class="name">var <span class="ident">transpose</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.values"><code class="name">var <span class="ident">values</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataScaler:
    &#34;&#34;&#34;numpy array `scaler` and `rescaler`&#34;&#34;&#34;

    __slots__ = [&#34;dataset_&#34;, &#34;_n&#34;, &#34;data_scaled&#34;, &#34;values&#34;, &#34;transpose&#34;, &#34;inv_fitting&#34;]

    def __init__(self, dataset: np.ndarray, n: int = 1) -&gt; None:
        &#34;&#34;&#34;Initializes the parameters required for scaling the data&#34;&#34;&#34;
        self.dataset_ = dataset.copy()
        self._n = n

    def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
        &#34;&#34;&#34;Perform a standard rescaling of the data

        Returns
        -------
        data_scaled : `np.array`
            An array containing the scaled data.
        &#34;&#34;&#34;
        if isinstance(dataset_, np.ndarray):
            data_scaled = np.copy(dataset_)
            mu = self.values[0]
            sigma = self.values[1]
            f = self.values[2]
            data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
            for i in range(self.dataset_.shape[0]):
                if self._n != None:
                    poly = f[i](self.inv_fitting[i](data_scaled[i]))
                    data_scaled[i] += -poly
                data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
            return data_scaled
        else:
            self.data_scaled = np.copy(self.dataset_.copy())

        mu = []
        sigma = []
        fitting = []
        self.inv_fitting = []

        try:
            xaxis = range(self.dataset_.shape[1])
        except:
            error_type = &#34;IndexError&#34;
            msg = &#34;Trying to access an item at an invalid index.&#34;
            print(f&#34;{error_type}: {msg}&#34;)
            return None
        if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
            self.dataset_ = self.dataset_.T
            self.transpose = True
        else:
            self.transpose = False
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
                inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
                f = np.poly1d(fit)
                poly = f(xaxis)
                fitting.append(f)
                self.inv_fitting.append(inv_fit)
                self.data_scaled[i, :] += -poly
            else:
                fitting.append(0.0)
                self.inv_fitting.append(0.0)
            mu.append(np.min(self.data_scaled[i, :]))
            if np.max(self.data_scaled[i, :]) != 0:
                sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
            else:
                sigma.append(1)

            self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

        self.values = [mu, sigma, fitting]

        return self.data_scaled

    def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Performs the inverse operation to the rescale function

        Parameters
        ----------
        dataset_ : `np.array`
            An array containing the scaled values.

        Returns
        -------
        dataset_ : `np.array`
            An array containing the rescaled data.
        &#34;&#34;&#34;
        if self.transpose:
            dataset_ = dataset_.T
        for i in range(dataset_.shape[0]):
            dataset_[i, :] += 1
            dataset_[i, :] /= 2
            dataset_[i, :] = dataset_[i, :] * self.values[1][i]
            dataset_[i, :] += self.values[0][i]
            dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

        return dataset_</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.DataScaler.rescale"><code class="name flex">
<span>def <span class="ident">rescale</span></span>(<span>self, dataset_: numpy.ndarray | None = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rescale(self, dataset_: np.ndarray | None = None) -&gt; np.ndarray:
    &#34;&#34;&#34;Perform a standard rescaling of the data

    Returns
    -------
    data_scaled : `np.array`
        An array containing the scaled data.
    &#34;&#34;&#34;
    if isinstance(dataset_, np.ndarray):
        data_scaled = np.copy(dataset_)
        mu = self.values[0]
        sigma = self.values[1]
        f = self.values[2]
        data_scaled = data_scaled.reshape((self.dataset_.shape[0], -1))
        for i in range(self.dataset_.shape[0]):
            if self._n != None:
                poly = f[i](self.inv_fitting[i](data_scaled[i]))
                data_scaled[i] += -poly
            data_scaled[i] = 2 * ((data_scaled[i] - mu[i]) / sigma[i]) - 1
        return data_scaled
    else:
        self.data_scaled = np.copy(self.dataset_.copy())

    mu = []
    sigma = []
    fitting = []
    self.inv_fitting = []

    try:
        xaxis = range(self.dataset_.shape[1])
    except:
        error_type = &#34;IndexError&#34;
        msg = &#34;Trying to access an item at an invalid index.&#34;
        print(f&#34;{error_type}: {msg}&#34;)
        return None
    if self.dataset_.shape[0] &gt; self.dataset_.shape[1]:
        self.dataset_ = self.dataset_.T
        self.transpose = True
    else:
        self.transpose = False
    for i in range(self.dataset_.shape[0]):
        if self._n != None:
            fit = np.polyfit(xaxis, self.dataset_[i, :], self._n)
            inv_fit = np.polyfit(self.dataset_[i, :], xaxis, self._n)
            f = np.poly1d(fit)
            poly = f(xaxis)
            fitting.append(f)
            self.inv_fitting.append(inv_fit)
            self.data_scaled[i, :] += -poly
        else:
            fitting.append(0.0)
            self.inv_fitting.append(0.0)
        mu.append(np.min(self.data_scaled[i, :]))
        if np.max(self.data_scaled[i, :]) != 0:
            sigma.append(np.max(self.data_scaled[i, :]) - mu[i])
        else:
            sigma.append(1)

        self.data_scaled[i, :] = 2 * ((self.data_scaled[i, :] - mu[i]) / sigma[i]) - 1

    self.values = [mu, sigma, fitting]

    return self.data_scaled</code></pre>
</details>
<div class="desc"><p>Perform a standard rescaling of the data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_scaled</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.DataScaler.scale"><code class="name flex">
<span>def <span class="ident">scale</span></span>(<span>self, dataset_: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale(self, dataset_: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Performs the inverse operation to the rescale function

    Parameters
    ----------
    dataset_ : `np.array`
        An array containing the scaled values.

    Returns
    -------
    dataset_ : `np.array`
        An array containing the rescaled data.
    &#34;&#34;&#34;
    if self.transpose:
        dataset_ = dataset_.T
    for i in range(dataset_.shape[0]):
        dataset_[i, :] += 1
        dataset_[i, :] /= 2
        dataset_[i, :] = dataset_[i, :] * self.values[1][i]
        dataset_[i, :] += self.values[0][i]
        dataset_[i, :] += self.values[2][i](range(dataset_.shape[1]))

    return dataset_</code></pre>
</details>
<div class="desc"><p>Performs the inverse operation to the rescale function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset_</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the rescaled data.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection"><code class="flex name class">
<span>class <span class="ident">FeatureSelection</span></span>
<span>(</span><span>not_features: list[str] = [])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"><p>Generate the data graph using a variation of the feature selection algorithm.</p>
<ul>
<li>The method <code>get_digraph</code> returns the network based on the feature selection method.</li>
</ul>
<p>The initializer of the class. The initial parameter is a list of strings with variables to discard.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="likelihood.graph.graph.DynamicGraph" href="../graph/graph.html#likelihood.graph.graph.DynamicGraph">DynamicGraph</a></li>
<li><a title="likelihood.models.simulation.SimulationEngine" href="../models/simulation.html#likelihood.models.simulation.SimulationEngine">SimulationEngine</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.all_features_imp_graph"><code class="name">var <span class="ident">all_features_imp_graph</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.not_features"><code class="name">var <span class="ident">not_features</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.scaler"><code class="name">var <span class="ident">scaler</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.FeatureSelection.w_dict"><code class="name">var <span class="ident">w_dict</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection:
    &#34;&#34;&#34;
    Generate the data graph using a variation of the feature selection algorithm.

    - The method `get_digraph` returns the network based on the feature selection method.
    &#34;&#34;&#34;

    __slots__ = [&#34;not_features&#34;, &#34;X&#34;, &#34;all_features_imp_graph&#34;, &#34;w_dict&#34;, &#34;scaler&#34;]

    def __init__(self, not_features: list[str] = []) -&gt; None:
        &#34;&#34;&#34;The initializer of the class. The initial parameter is a list of strings with variables to discard.&#34;&#34;&#34;
        self.not_features: List[str] = not_features
        self.all_features_imp_graph: List[Tuple] = []
        self.w_dict = dict()

    def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
        &#34;&#34;&#34;
        Get directed graph showing importance of features.

        Parameters
        ----------
        dataset : `DataFrame`
            Dataset to be used for generating the graph.
        n_importances : `int`
            Number of top importances to show in the graph.

        Returns
        -------
        `str`
            A string representation of the directed graph.
        &#34;&#34;&#34;
        self._load_data(dataset)

        curr_dataset = self.X
        columns = list(curr_dataset.columns)

        feature_string = &#34; digraph { &#34;
        for column in columns:
            feature_string += column + &#34;; &#34;

        numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
        if use_scaler:
            self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
            numeric_scaled = self.scaler.rescale()
            numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
            curr_dataset[numeric_df.columns] = numeric_df

        numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

        for index_column, column in enumerate(columns):
            Y = curr_dataset[column]
            column_type = Y.dtype
            if column_type != &#34;object&#34;:
                Model = LinearRegression()
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
                importance = Model.get_importances()
                w = Model.w
            else:
                Model = LogisticRegression()
                num_unique_entries = curr_dataset[column].nunique()
                quick_encoder = DataFrameEncoder(Y.to_frame())
                encoded_Y = quick_encoder.encode(save_mode=False)
                one_hot = OneHotEncoder()
                train_y = one_hot.encode(encoded_Y[column])
                for i in range(len(train_y)):
                    for j in range(num_unique_entries):
                        if train_y[i][j] == 1.0:
                            train_y[i][j] = 0.73105
                        else:
                            train_y[i][j] = 0.5
                X_aux = curr_dataset.drop([column], axis=1)
                dfe = DataFrameEncoder(X_aux)
                encoded_df = dfe.encode(save_mode=False)
                Model.fit(encoded_df.to_numpy().T, train_y)
                importance = Model.get_importances()
                w = Model.w
            top_n_indexes = sorted(
                range(len(importance)), key=lambda i: importance[i], reverse=True
            )[:n_importances]

            names_cols = list(X_aux.columns)
            features_imp_node = [
                (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
                for i in range(n_importances)
            ]

            if column_type != &#34;object&#34;:
                self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
            else:
                self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
            self.all_features_imp_graph.append((column, features_imp_node))
            for i in top_n_indexes:
                feature_string += names_cols[i] + &#34; -&gt; &#34;

            feature_string += column + &#34;; &#34;

        return feature_string + &#34;} &#34;

    def _load_data(self, dataset: DataFrame):

        if len(self.not_features) &gt; 0:
            self.X = dataset.drop(columns=self.not_features)

        else:
            self.X = dataset

        self.X.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.X.replace(&#34; &#34;, np.nan, inplace=True)
        self.X.dropna(inplace=True)
        self.X = self.X.reset_index()
        self.X = self.X.drop(columns=[&#34;index&#34;])</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.FeatureSelection.get_digraph"><code class="name flex">
<span>def <span class="ident">get_digraph</span></span>(<span>self,<br>dataset: pandas.core.frame.DataFrame,<br>n_importances: int,<br>use_scaler: bool = False) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_digraph(self, dataset: DataFrame, n_importances: int, use_scaler: bool = False) -&gt; str:
    &#34;&#34;&#34;
    Get directed graph showing importance of features.

    Parameters
    ----------
    dataset : `DataFrame`
        Dataset to be used for generating the graph.
    n_importances : `int`
        Number of top importances to show in the graph.

    Returns
    -------
    `str`
        A string representation of the directed graph.
    &#34;&#34;&#34;
    self._load_data(dataset)

    curr_dataset = self.X
    columns = list(curr_dataset.columns)

    feature_string = &#34; digraph { &#34;
    for column in columns:
        feature_string += column + &#34;; &#34;

    numeric_df = curr_dataset.select_dtypes(include=&#34;number&#34;)
    if use_scaler:
        self.scaler = DataScaler(numeric_df.copy().to_numpy().T, n=None)
        numeric_scaled = self.scaler.rescale()
        numeric_df = pd.DataFrame(numeric_scaled.T, columns=numeric_df.columns)
        curr_dataset[numeric_df.columns] = numeric_df

    numeric_dict = dict(zip(list(numeric_df.columns), range(len(list(numeric_df.columns)))))

    for index_column, column in enumerate(columns):
        Y = curr_dataset[column]
        column_type = Y.dtype
        if column_type != &#34;object&#34;:
            Model = LinearRegression()
            X_aux = curr_dataset.drop([column], axis=1)
            dfe = DataFrameEncoder(X_aux)
            encoded_df = dfe.encode(save_mode=False)
            Model.fit(encoded_df.to_numpy().T, Y.to_numpy().T)
            importance = Model.get_importances()
            w = Model.w
        else:
            Model = LogisticRegression()
            num_unique_entries = curr_dataset[column].nunique()
            quick_encoder = DataFrameEncoder(Y.to_frame())
            encoded_Y = quick_encoder.encode(save_mode=False)
            one_hot = OneHotEncoder()
            train_y = one_hot.encode(encoded_Y[column])
            for i in range(len(train_y)):
                for j in range(num_unique_entries):
                    if train_y[i][j] == 1.0:
                        train_y[i][j] = 0.73105
                    else:
                        train_y[i][j] = 0.5
            X_aux = curr_dataset.drop([column], axis=1)
            dfe = DataFrameEncoder(X_aux)
            encoded_df = dfe.encode(save_mode=False)
            Model.fit(encoded_df.to_numpy().T, train_y)
            importance = Model.get_importances()
            w = Model.w
        top_n_indexes = sorted(
            range(len(importance)), key=lambda i: importance[i], reverse=True
        )[:n_importances]

        names_cols = list(X_aux.columns)
        features_imp_node = [
            (names_cols[top_n_indexes[i]], importance[top_n_indexes[i]])
            for i in range(n_importances)
        ]

        if column_type != &#34;object&#34;:
            self.w_dict[column] = (w, None, names_cols, dfe, numeric_dict)
        else:
            self.w_dict[column] = (w, quick_encoder, names_cols, dfe, numeric_dict)
        self.all_features_imp_graph.append((column, features_imp_node))
        for i in top_n_indexes:
            feature_string += names_cols[i] + &#34; -&gt; &#34;

        feature_string += column + &#34;; &#34;

    return feature_string + &#34;} &#34;</code></pre>
</details>
<div class="desc"><p>Get directed graph showing importance of features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Dataset to be used for generating the graph.</dd>
<dt><strong><code>n_importances</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top importances to show in the graph.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>str</code>
A string representation of the directed graph.</p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LinearRegression"><code class="flex name class">
<span>class <span class="ident">LinearRegression</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        `None` : The function doesn&#39;t return anything.
        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"><p>class implementing multiple linear regression</p>
<p>The class initializer</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        `None` : The function doesn&#39;t return anything.
        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        `None` : The function doesn&#39;t return anything.
        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        `None` : The function doesn&#39;t return anything.
        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRegression:
    &#34;&#34;&#34;class implementing multiple linear regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        `None` : The function doesn&#39;t return anything.
        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)

        if verbose:
            print(&#34;\nSummary:&#34;)
            print(&#34;--------&#34;)
            print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
            print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        &#34;&#34;&#34;
        return np.array(self.importance) @ datapoints

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LinearRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray, verbose: bool = False) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, dataset: np.ndarray, values: np.ndarray, verbose: bool = False) -&gt; None:
    &#34;&#34;&#34;Performs linear multiple model training

    Parameters
    ----------
    dataset : `np.array`
        An array containing the scaled data.
    values : `np.ndarray`
        A set of values returned by the linear function.

    Returns
    -------
    `None` : The function doesn&#39;t return anything.
    &#34;&#34;&#34;

    self.X = dataset
    self.y = values

    U, S, VT = np.linalg.svd(self.X, full_matrices=False)
    self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ self.y

    for i in range(self.X.shape[0]):
        a = np.around(self.w[i], decimals=8)
        self.importance.append(a)

    if verbose:
        print(&#34;\nSummary:&#34;)
        print(&#34;--------&#34;)
        print(&#34;\nParameters:&#34;, np.array(self.importance).shape)
        print(&#34;RMSE: {:.4f}&#34;.format(mean_square_error(self.y, self.predict(self.X))))</code></pre>
</details>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>None</code> : The function doesn't return anything.</p></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Returns the important features

    Parameters
    ----------
    print_important_features : `bool`
        determines whether or not are printed on the screen. By default it is set to `False`.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.
    &#34;&#34;&#34;
    if print_important_features:
        for i, a in enumerate(self.importance):
            print(f&#34;The importance of the {i+1} feature is {a}&#34;)
    return np.array(self.importance)</code></pre>
</details>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LinearRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Performs predictions for a set of points

    Parameters
    ----------
    datapoints : `np.array`
        An array containing the values of the independent variable.

    &#34;&#34;&#34;
    return np.array(self.importance) @ datapoints</code></pre>
</details>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression"><code class="flex name class">
<span>class <span class="ident">LogisticRegression</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        Returns
        -------
        `np.array`

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"><p>class implementing multiple logistic regression</p>
<p>The class initializer</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.X"><code class="name">var <span class="ident">X</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        Returns
        -------
        `np.array`

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.importance"><code class="name">var <span class="ident">importance</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        Returns
        -------
        `np.array`

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.w"><code class="name">var <span class="ident">w</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        Returns
        -------
        `np.array`

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.y"><code class="name">var <span class="ident">y</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticRegression:
    &#34;&#34;&#34;class implementing multiple logistic regression&#34;&#34;&#34;

    __slots__ = [&#34;importance&#34;, &#34;X&#34;, &#34;y&#34;, &#34;w&#34;]

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;The class initializer&#34;&#34;&#34;

        self.importance = []

    def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Performs linear multiple model training

        Parameters
        ----------
        dataset : `np.array`
            An array containing the scaled data.
        values : `np.ndarray`
            A set of values returned by the linear function.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.

        &#34;&#34;&#34;

        self.X = dataset
        self.y = values

        U, S, VT = np.linalg.svd(self.X, full_matrices=False)

        inverse_sig = np.vectorize(sigmoide_inv)
        self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

        if self.y.shape[1] &gt; 1:
            for row in self.w:
                self.importance.append(np.around(np.max(row), decimals=8))
        else:
            for i in range(self.X.shape[0]):
                a = np.around(self.w[i], decimals=8)
                self.importance.append(a)

    def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Performs predictions for a set of points

        Parameters
        ----------
        datapoints : `np.array`
            An array containing the values of the independent variable.

        Returns
        -------
        `np.array`

        &#34;&#34;&#34;
        sig = np.vectorize(sigmoide)

        return sig(np.array(self.importance) @ datapoints)

    def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns the important features

        Parameters
        ----------
        print_important_features : `bool`
            determines whether or not are printed on the screen. By default it is set to `False`.

        Returns
        -------
        importance : `np.array`
            An array containing the importance of each feature.
        &#34;&#34;&#34;
        if print_important_features:
            for i, a in enumerate(self.importance):
                print(f&#34;The importance of the {i+1} feature is {a}&#34;)
        return np.array(self.importance)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.LogisticRegression.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset: numpy.ndarray, values: numpy.ndarray) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, dataset: np.ndarray, values: np.ndarray) -&gt; None:
    &#34;&#34;&#34;Performs linear multiple model training

    Parameters
    ----------
    dataset : `np.array`
        An array containing the scaled data.
    values : `np.ndarray`
        A set of values returned by the linear function.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.

    &#34;&#34;&#34;

    self.X = dataset
    self.y = values

    U, S, VT = np.linalg.svd(self.X, full_matrices=False)

    inverse_sig = np.vectorize(sigmoide_inv)
    self.w = (VT.T @ np.linalg.inv(np.diag(S)) @ U.T).T @ inverse_sig(self.y)

    if self.y.shape[1] &gt; 1:
        for row in self.w:
            self.importance.append(np.around(np.max(row), decimals=8))
    else:
        for i in range(self.X.shape[0]):
            a = np.around(self.w[i], decimals=8)
            self.importance.append(a)</code></pre>
</details>
<div class="desc"><p>Performs linear multiple model training</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the scaled data.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A set of values returned by the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.get_importances"><code class="name flex">
<span>def <span class="ident">get_importances</span></span>(<span>self, print_important_features: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_importances(self, print_important_features: bool = False) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Returns the important features

    Parameters
    ----------
    print_important_features : `bool`
        determines whether or not are printed on the screen. By default it is set to `False`.

    Returns
    -------
    importance : `np.array`
        An array containing the importance of each feature.
    &#34;&#34;&#34;
    if print_important_features:
        for i, a in enumerate(self.importance):
            print(f&#34;The importance of the {i+1} feature is {a}&#34;)
    return np.array(self.importance)</code></pre>
</details>
<div class="desc"><p>Returns the important features</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>print_important_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether or not are printed on the screen. By default it is set to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>importance</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the importance of each feature.</dd>
</dl></div>
</dd>
<dt id="likelihood.tools.tools.LogisticRegression.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, datapoints: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, datapoints: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Performs predictions for a set of points

    Parameters
    ----------
    datapoints : `np.array`
        An array containing the values of the independent variable.

    Returns
    -------
    `np.array`

    &#34;&#34;&#34;
    sig = np.vectorize(sigmoide)

    return sig(np.array(self.importance) @ datapoints)</code></pre>
</details>
<div class="desc"><p>Performs predictions for a set of points</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapoints</code></strong> :&ensp;<code>np.array</code></dt>
<dd>An array containing the values of the independent variable.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>np.array</code></p></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder"><code class="flex name class">
<span>class <span class="ident">OneHotEncoder</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OneHotEncoder:
    &#34;&#34;&#34;
    Class used to encode categorical variables.
    It receives an array of integers and returns a binary array using the one-hot encoding method.
    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;]

    def __init__(self) -&gt; None:
        pass

    def encode(self, x: np.ndarray | list):
        self.x = x

        if not isinstance(self.x, np.ndarray):
            self.x = np.array(self.x)

        y = np.zeros((self.x.size, self.x.max() + 1))

        y[np.arange(self.x.size), self.x] = 1

        return y

    def decode(self, x: np.ndarray | list) -&gt; np.ndarray:
        if not isinstance(x, np.ndarray):
            x = np.array(x)

        y = np.argmax(x, axis=1)

        return y</code></pre>
</details>
<div class="desc"><p>Class used to encode categorical variables.
It receives an array of integers and returns a binary array using the one-hot encoding method.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OneHotEncoder:
    &#34;&#34;&#34;
    Class used to encode categorical variables.
    It receives an array of integers and returns a binary array using the one-hot encoding method.
    &#34;&#34;&#34;

    __slots__ = [&#34;x&#34;]

    def __init__(self) -&gt; None:
        pass

    def encode(self, x: np.ndarray | list):
        self.x = x

        if not isinstance(self.x, np.ndarray):
            self.x = np.array(self.x)

        y = np.zeros((self.x.size, self.x.max() + 1))

        y[np.arange(self.x.size), self.x] = 1

        return y

    def decode(self, x: np.ndarray | list) -&gt; np.ndarray:
        if not isinstance(x, np.ndarray):
            x = np.array(x)

        y = np.argmax(x, axis=1)

        return y</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.OneHotEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, x: numpy.ndarray | list) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, x: np.ndarray | list) -&gt; np.ndarray:
    if not isinstance(x, np.ndarray):
        x = np.array(x)

    y = np.argmax(x, axis=1)

    return y</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.OneHotEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, x: numpy.ndarray | list)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, x: np.ndarray | list):
    self.x = x

    if not isinstance(self.x, np.ndarray):
        self.x = np.array(self.x)

    y = np.zeros((self.x.size, self.x.max() + 1))

    y[np.arange(self.x.size), self.x] = 1

    return y</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures"><code class="flex name class">
<span>class <span class="ident">PerformanceMeasures</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerformanceMeasures:
    &#34;&#34;&#34;Class with methods to measure performance&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        pass

    def f_mean(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; float:
        F_vec = self._f1_score(y_true, y_pred, labels)
        mean_f_measure = np.mean(F_vec)
        mean_f_measure = np.around(mean_f_measure, decimals=4)

        for label, f_measure in zip(labels, F_vec):
            print(f&#34;F-measure of label {label} -&gt; {f_measure}&#34;)

        print(f&#34;Mean of F-measure -&gt; {mean_f_measure}&#34;)

        return mean_f_measure

    def resp(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; float:
        T_C = len(y_true)
        sum1, sum2 = 0.0, 0.0
        F_vec = self._f1_score(y_true, y_pred, labels)

        for label_idx, label in enumerate(labels):
            class_instances = np.sum(y_true == label) / T_C
            sum1 += (1 - class_instances) * F_vec[label_idx]
            sum2 += 1 - class_instances

        res_p = sum1 / sum2 if sum2 != 0 else 0.0
        print(f&#34;Metric Res_p -&gt; {res_p}&#34;)

        return res_p

    def _summary_pred(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; None:
        count_mat = self._confu_mat(y_true, y_pred, labels)
        print(&#34; &#34; * 6, &#34; | &#34;.join(f&#34;--{label}--&#34; for label in labels))
        for i, label_i in enumerate(labels):
            row = [f&#34; {int(count_mat[i, j]):5d} &#34; for j in range(len(labels))]
            print(f&#34;--{label_i}--|&#34;, &#34; | &#34;.join(row))

    def _f1_score(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; np.ndarray:
        count_mat = self._confu_mat(y_true, y_pred, labels)
        sum_cols = np.sum(count_mat, axis=0)
        sum_rows = np.sum(count_mat, axis=1)

        precision = np.divide(
            count_mat.diagonal(), sum_cols, out=np.zeros_like(sum_cols), where=sum_cols != 0
        )
        recall = np.divide(
            count_mat.diagonal(), sum_rows, out=np.zeros_like(sum_rows), where=sum_rows != 0
        )
        f1_vec = 2 * ((precision * recall) / (precision + recall))
        f1_vec = np.around(f1_vec, decimals=4)

        return f1_vec

    def _confu_mat(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; np.ndarray:
        num_classes = len(labels)
        label_mapping = {label: idx for idx, label in enumerate(labels)}
        count_mat = np.zeros((num_classes, num_classes))

        for pred_label, true_label in zip(y_pred, y_true):
            if pred_label in label_mapping and true_label in label_mapping:
                count_mat[label_mapping[pred_label], label_mapping[true_label]] += 1

        return count_mat</code></pre>
</details>
<div class="desc"><p>Class with methods to measure performance</p></div>
<h3>Methods</h3>
<dl>
<dt id="likelihood.tools.tools.PerformanceMeasures.f_mean"><code class="name flex">
<span>def <span class="ident">f_mean</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: List[int]) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f_mean(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; float:
    F_vec = self._f1_score(y_true, y_pred, labels)
    mean_f_measure = np.mean(F_vec)
    mean_f_measure = np.around(mean_f_measure, decimals=4)

    for label, f_measure in zip(labels, F_vec):
        print(f&#34;F-measure of label {label} -&gt; {f_measure}&#34;)

    print(f&#34;Mean of F-measure -&gt; {mean_f_measure}&#34;)

    return mean_f_measure</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="likelihood.tools.tools.PerformanceMeasures.resp"><code class="name flex">
<span>def <span class="ident">resp</span></span>(<span>self, y_true: numpy.ndarray, y_pred: numpy.ndarray, labels: List[int]) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resp(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]) -&gt; float:
    T_C = len(y_true)
    sum1, sum2 = 0.0, 0.0
    F_vec = self._f1_score(y_true, y_pred, labels)

    for label_idx, label in enumerate(labels):
        class_instances = np.sum(y_true == label) / T_C
        sum1 += (1 - class_instances) * F_vec[label_idx]
        sum2 += 1 - class_instances

    res_p = sum1 / sum2 if sum2 != 0 else 0.0
    print(f&#34;Metric Res_p -&gt; {res_p}&#34;)

    return res_p</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="likelihood.tools" href="index.html">likelihood.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="likelihood.tools.tools.cal_average" href="#likelihood.tools.tools.cal_average">cal_average</a></code></li>
<li><code><a title="likelihood.tools.tools.cal_missing_values" href="#likelihood.tools.tools.cal_missing_values">cal_missing_values</a></code></li>
<li><code><a title="likelihood.tools.tools.calculate_probability" href="#likelihood.tools.tools.calculate_probability">calculate_probability</a></code></li>
<li><code><a title="likelihood.tools.tools.cdf" href="#likelihood.tools.tools.cdf">cdf</a></code></li>
<li><code><a title="likelihood.tools.tools.check_nan_inf" href="#likelihood.tools.tools.check_nan_inf">check_nan_inf</a></code></li>
<li><code><a title="likelihood.tools.tools.difference_quotient" href="#likelihood.tools.tools.difference_quotient">difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.estimate_gradient" href="#likelihood.tools.tools.estimate_gradient">estimate_gradient</a></code></li>
<li><code><a title="likelihood.tools.tools.fft_denoise" href="#likelihood.tools.tools.fft_denoise">fft_denoise</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_feature_yaml" href="#likelihood.tools.tools.generate_feature_yaml">generate_feature_yaml</a></code></li>
<li><code><a title="likelihood.tools.tools.generate_series" href="#likelihood.tools.tools.generate_series">generate_series</a></code></li>
<li><code><a title="likelihood.tools.tools.get_period" href="#likelihood.tools.tools.get_period">get_period</a></code></li>
<li><code><a title="likelihood.tools.tools.mean_square_error" href="#likelihood.tools.tools.mean_square_error">mean_square_error</a></code></li>
<li><code><a title="likelihood.tools.tools.minibatches" href="#likelihood.tools.tools.minibatches">minibatches</a></code></li>
<li><code><a title="likelihood.tools.tools.partial_difference_quotient" href="#likelihood.tools.tools.partial_difference_quotient">partial_difference_quotient</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide" href="#likelihood.tools.tools.sigmoide">sigmoide</a></code></li>
<li><code><a title="likelihood.tools.tools.sigmoide_inv" href="#likelihood.tools.tools.sigmoide_inv">sigmoide_inv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="likelihood.tools.tools.AutoCorrelation" href="#likelihood.tools.tools.AutoCorrelation">AutoCorrelation</a></code></h4>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.Correlation" href="#likelihood.tools.tools.Correlation">Correlation</a></code></h4>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.CorrelationBase" href="#likelihood.tools.tools.CorrelationBase">CorrelationBase</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.CorrelationBase.plot" href="#likelihood.tools.tools.CorrelationBase.plot">plot</a></code></li>
<li><code><a title="likelihood.tools.tools.CorrelationBase.result" href="#likelihood.tools.tools.CorrelationBase.result">result</a></code></li>
<li><code><a title="likelihood.tools.tools.CorrelationBase.x" href="#likelihood.tools.tools.CorrelationBase.x">x</a></code></li>
<li><code><a title="likelihood.tools.tools.CorrelationBase.y" href="#likelihood.tools.tools.CorrelationBase.y">y</a></code></li>
<li><code><a title="likelihood.tools.tools.CorrelationBase.z" href="#likelihood.tools.tools.CorrelationBase.z">z</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.DataFrameEncoder" href="#likelihood.tools.tools.DataFrameEncoder">DataFrameEncoder</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decode" href="#likelihood.tools.tools.DataFrameEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.decoding_list" href="#likelihood.tools.tools.DataFrameEncoder.decoding_list">decoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encode" href="#likelihood.tools.tools.DataFrameEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.encoding_list" href="#likelihood.tools.tools.DataFrameEncoder.encoding_list">encoding_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.get_dictionaries" href="#likelihood.tools.tools.DataFrameEncoder.get_dictionaries">get_dictionaries</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.load_config" href="#likelihood.tools.tools.DataFrameEncoder.load_config">load_config</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.median_list" href="#likelihood.tools.tools.DataFrameEncoder.median_list">median_list</a></code></li>
<li><code><a title="likelihood.tools.tools.DataFrameEncoder.train" href="#likelihood.tools.tools.DataFrameEncoder.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.DataScaler" href="#likelihood.tools.tools.DataScaler">DataScaler</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.DataScaler.data_scaled" href="#likelihood.tools.tools.DataScaler.data_scaled">data_scaled</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.dataset_" href="#likelihood.tools.tools.DataScaler.dataset_">dataset_</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.inv_fitting" href="#likelihood.tools.tools.DataScaler.inv_fitting">inv_fitting</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.rescale" href="#likelihood.tools.tools.DataScaler.rescale">rescale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.scale" href="#likelihood.tools.tools.DataScaler.scale">scale</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.transpose" href="#likelihood.tools.tools.DataScaler.transpose">transpose</a></code></li>
<li><code><a title="likelihood.tools.tools.DataScaler.values" href="#likelihood.tools.tools.DataScaler.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.FeatureSelection" href="#likelihood.tools.tools.FeatureSelection">FeatureSelection</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.FeatureSelection.X" href="#likelihood.tools.tools.FeatureSelection.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.all_features_imp_graph" href="#likelihood.tools.tools.FeatureSelection.all_features_imp_graph">all_features_imp_graph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.get_digraph" href="#likelihood.tools.tools.FeatureSelection.get_digraph">get_digraph</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.not_features" href="#likelihood.tools.tools.FeatureSelection.not_features">not_features</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.scaler" href="#likelihood.tools.tools.FeatureSelection.scaler">scaler</a></code></li>
<li><code><a title="likelihood.tools.tools.FeatureSelection.w_dict" href="#likelihood.tools.tools.FeatureSelection.w_dict">w_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LinearRegression" href="#likelihood.tools.tools.LinearRegression">LinearRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LinearRegression.X" href="#likelihood.tools.tools.LinearRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.fit" href="#likelihood.tools.tools.LinearRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.get_importances" href="#likelihood.tools.tools.LinearRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.importance" href="#likelihood.tools.tools.LinearRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.predict" href="#likelihood.tools.tools.LinearRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.w" href="#likelihood.tools.tools.LinearRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LinearRegression.y" href="#likelihood.tools.tools.LinearRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.LogisticRegression" href="#likelihood.tools.tools.LogisticRegression">LogisticRegression</a></code></h4>
<ul class="two-column">
<li><code><a title="likelihood.tools.tools.LogisticRegression.X" href="#likelihood.tools.tools.LogisticRegression.X">X</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.fit" href="#likelihood.tools.tools.LogisticRegression.fit">fit</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.get_importances" href="#likelihood.tools.tools.LogisticRegression.get_importances">get_importances</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.importance" href="#likelihood.tools.tools.LogisticRegression.importance">importance</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.predict" href="#likelihood.tools.tools.LogisticRegression.predict">predict</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.w" href="#likelihood.tools.tools.LogisticRegression.w">w</a></code></li>
<li><code><a title="likelihood.tools.tools.LogisticRegression.y" href="#likelihood.tools.tools.LogisticRegression.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.OneHotEncoder" href="#likelihood.tools.tools.OneHotEncoder">OneHotEncoder</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.OneHotEncoder.decode" href="#likelihood.tools.tools.OneHotEncoder.decode">decode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.encode" href="#likelihood.tools.tools.OneHotEncoder.encode">encode</a></code></li>
<li><code><a title="likelihood.tools.tools.OneHotEncoder.x" href="#likelihood.tools.tools.OneHotEncoder.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="likelihood.tools.tools.PerformanceMeasures" href="#likelihood.tools.tools.PerformanceMeasures">PerformanceMeasures</a></code></h4>
<ul class="">
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.f_mean" href="#likelihood.tools.tools.PerformanceMeasures.f_mean">f_mean</a></code></li>
<li><code><a title="likelihood.tools.tools.PerformanceMeasures.resp" href="#likelihood.tools.tools.PerformanceMeasures.resp">resp</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
